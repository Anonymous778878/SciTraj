[
  {
    "paperId": "2f5102ec3f70d0dea98c957cc2cab4d15d83a2da",
    "title": "The Stanford CoreNLP Natural Language Processing Toolkit",
    "sections": {
      "Introduction": [
        "This paper describe the design and development of Stanford CoreNLP, a Java (or at least JVM-based) annotation pipeline framework, which provides most of the common core natural language processing (NLP) steps, from tokenization through to coreference resolution. We describe the original design of the system and its strengths (section 2), simple usage patterns (section 3), the set of provided annotators and how properties control them (section 4), and how to add additional annotators (section 5), before concluding with some higherlevel remarks and additional appendices. While there are several good natural language analysis toolkits, Stanford CoreNLP is one of the most used, and a central theme is trying to identify the attributes that contributed to its success."
      ],
      "Conclusion": [
        "In this paper, we have presented the design and usage of the Stanford CoreNLP system, an annotation-based NLP processing pipeline. We have in particular tried to emphasize the properties that we feel have made it successful. Rather than trying to provide the largest and most engineered kitchen sink, the goal has been to make it as easy as possible for users to get started using the framework, and to keep the framework small, so it is easily comprehensible, and can easily be used as a component within the much larger system that a user may be developing. The broad usage of this system, and of other systems such as NLTK  (Bird et al., 2009) , which emphasize accessibility to beginning users, suggests the merits of this approach.",
        "MorphaAnnotator: 0.0 sec. ParserAnnotator: 0.4 sec. SentimentAnnotator: 0.1 sec. TOTAL: 0.6 sec. for 16 tokens at 27.4 tokens/sec. Pipeline setup: 3.0 sec. Total time for StanfordCoreNLP pipeline: 4.2 sec. $ grep sentiment sentiment.txt.xml <sentence id=\"1\" sentimentValue=\"3\" sentiment=\"Positive\"> <sentence id=\"2\" sentimentValue=\"4\" sentiment=\"Verypositive\"> <sentence id=\"3\" sentimentValue=\"1\" sentiment=\"Negative\">"
      ]
    }
  },
  {
    "paperId": "a2f38d03fd363e920494ad65a5f0ad8bd18cd60b",
    "title": "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing",
    "sections": {
      "Introduction": [
        "In natural language processing (NLP), pretraining large neural language models on unlabeled text has proven to be a successful strategy for transfer learning. A prime example is Bidirectional Encoder Representations from Transformers (BERT)  [16] , which has become a standard building block for training task-specific NLP models. Existing pretraining work typically focuses on the newswire and Web domains. For example, the original BERT model was trained on Wikipedia 1 and BookCorpus  [62] , and subsequent efforts have focused on crawling additional text from the Web to power even larger-scale pretraining  [39, 50] ."
      ],
      "Methods": [
        "In this section, we provide a brief overview of neural language model pretraining, using BERT  [16]  as a running example.",
        "2.1.1 Vocabulary. We assume that the input consists of text spans, such as sentences separated by special tokens  [SEP] . To address the problem of out-of-vocabulary words, neural language models generate a vocabulary from subword units, using Byte-Pair Encoding (BPE)  [51]  or variants such as WordPiece  [32] . Essentially, the BPE algorithm tries to greedily identify a small set of subwords that can compactly form all words in the given corpus. It does this by first shattering all words in the corpus and initializing the vocabulary with characters and delimiters. It then iteratively augments the vocabulary with a new subword that is most frequent in the corpus and can be formed by concatenating two existing subwords, until the vocabulary reaches the pre-specified size (e.g.,  30 ,000 in standard BERT models or 50,000 in RoBERTa  [39] ). In this paper, we use the WordPiece algorithm which is a BPE variant that uses likelihood based on the unigram language model rather than frequency in choosing which subwords to concatenate. The text corpus and vocabulary may preserve the case (cased) or convert all characters to lower case (uncased).",
        "In the above studies on pretraining methods, we fix the fine-tuning methods to the standard methods described in subsection 2.4. Next, we will study the effect of modeling choices in task-specific fine-tuning, by fixing the underlying pretrained language model to our standard PubMedBERT (WWM, PubMed vocabulary, pretrained using PubMed abstracts).",
        "Prior to the current success of pretraining neural language models, standard NLP approaches were often dominated by sequential labeling methods, such as conditional random fields (CRF) and more recently recurrent With the advent of BERT models and the self-attention mechanism, the utility of explicit sequential modeling becomes questionable. The top layer in the BERT model already captures many non-linear dependencies across the entire text span. Therefore, it's conceivable that even a linear layer on top can perform competitively. We find that this is indeed the case for NER and relation extraction, as shown in Table  12 . The use of a bidirectional LSTM (Bi-LSTM) does not lead to any substantial gain compared to linear layer.",
        "We also investigate the tagging scheme used in NER. The standard tagging scheme distinguishes words by their positions within an entity. For sequential tagging methods such as CRF and LSTM, distinguishing the position within an entity is potentially advantageous compared to the minimal IO scheme that only distinguishes between inside and outside of entities. But for BERT models, once again, the utility of more complex tagging schemes is diminished. We thus conducted a head-to-head comparison of the tagging schemes using three biomedical NER tasks in BLURB. As we can see in Table  13 , the difference is minuscule, suggesting that with self-attention, the sequential nature of the tags is less essential in NER modeling.  The use of neural methods also has subtle, but significant, implications for relation extraction. Previously, relation extraction was generally framed as a classification problem with manually-crafted feature templates. To prevent overfitting and enhance generalization, the feature templates would typically avoid using the entities in question. Neural methods do not need hand-crafted features, but rather use the neural encoding of the given text span, including the entities themselves. This introduces a potential risk that the neural network may simply memorize the entity combination. This problem is particularly pronounced in self-supervision settings, such as distant supervision, because the positive instances are derived from entity tuples with known relations. As a result, it is a common practice to \"dummify\" entities (i.e., replace an entity with a generic tag such as $DRUG or $GENE)  [24, 58] .",
        "This risk remains in the standard supervised setting, such as in the tasks that comprise BLURB. We thus conducted a systematic evaluation of entity dummification and relation encoding, using two relation extraction tasks in BLURB.",
        "For entity marking, we consider three variants: dummify the entities in question; use the original text; add start and end tags to entities in question. For relation encoding, we consider three schemes. In the [CLS] encoding introduced by the original BERT paper, the special token [CLS] is prepended to the beginning of the text span, and its contextual representation at the top layer is used as the input in the final classification. Another standard approach concatenates the BERT encoding of the given entity mentions, each obtained by applying max pooling to the corresponding token representations. Finally, following prior work, we also consider simply concatenating the top contextual representation of the entity start tag, if the entity markers are in use  [7] .",
        "Table  14  shows the results. Simply using the original text indeed exposes the neural methods to significant overfitting risk. Using [CLS] with the original text is the worst choice, as the relation encoding has a hard time to distinguish which entities in the text span are in question. Dummification remains the most reliable method, which works for either relation encoding method. Interestingly, using entity markers leads to slightly better results in both datasets, as it appears to prevent overfitting while preserving useful entity information. We leave it to future work to study whether this would generalize to all relation extraction tasks."
      ],
      "Results": [
        "In this section, we conduct a thorough evaluation to assess the impact of domain-specific pretraining in biomedical NLP applications. First, we fix the standard task-specific model for each task in BLURB, and conduct a head-tohead comparison of domain-specific pretraining and mixed-domain pretraining. Next, we evaluate the impact of various pretraining options such as vocabulary, whole-word masking (WWM), and adversarial pretraining. Finally, we fix a pretrained BERT model and compare various modeling choices for task-specific fine-tuning. Table  6 . Comparison of pretrained language models on the BLURB biomedical NLP benchmark. The standard task-specific models are used in the same fine-tuning process for all BERT models. The BLURB score is the macro average of average test results for each of the six tasks (NER, PICO, relation extraction, sentence similarity, document classification, question answering). See Table  3  for the evaluation metric used in each task."
      ],
      "Discussion": [
        "Standard supervised learning requires labeled examples, which are expensive and time-consuming to annotate. Self-supervision using unlabeled text is thus a long-standing direction for alleviating the annotation bottleneck using transfer learning. Early methods focused on clustering related words using distributed similarity, such as Brown Clusters  [12, 36] . With the revival of neural approaches, neural embedding has become the new staple for transfer learning from unlabeled text. This starts with simple stand-alone word embeddings  [41, 46] , and evolves into more sophisticated pretrained language models, from LSTM in ULMFiT  [23]  and ELMo  [47]  to transformer-based models in GPT  [48, 49]  and BERT  [16, 39] . Their success is fueled by access to large text corpora, advanced hardware such as GPUs, and a culmination of advances in optimization methods, such as Adam  [30]  and slanted triangular learning rate  [23] . Here, transfer learning goes from the pretrained language models to fine-tuning task-specific models for downstream applications.",
        "As the community ventures beyond the standard newswire and Web domains, and begins to explore highvalue verticals such as biomedicine, a different kind of transfer learning is brought into play by combining text from various domains in pretraining language models. The prevailing assumption is that such mixed-domain pretraining is advantageous. In this paper, we show that this type of transfer learning may not be applicable when there is a sufficient amount of in-domain text, as is the case in biomedicine. In fact, our experiments comparing clinical BERTs with PubMedBERT on biomedical NLP tasks show that even related text such as clinical notes may not be helpful, since we already have abundant biomedical text from PubMed. Our results show that we should distinguish different types of transfer learning and separately assess their utility in various situations.",
        "There are a plethora of biomedical NLP datasets, especially from various shared tasks such as BioCreative  [3, 29, 40, 53] , BioNLP  [15, 28] , SemEval  [2, 9, 10, 17] , and BioASQ  [42] . The focus has evolved from simple tasks, such as named entity recognition, to more sophisticated tasks, such as relation extraction and question answering, and new tasks have been proposed for emerging application scenarios such as evidence-based medical information extraction  [44] . However, while comprehensive benchmarks and leaderboards are available for the general domains (e.g., GLUE  [57]  and SuperGLUE  [56] ), they are still a rarity in biomedical NLP. In this paper, inspired by prior effort towards this direction  [45] , we create the first leaderboard for biomedical NLP, BLURB -a comprehensive benchmark containing thirteen datasets for six tasks."
      ],
      "Conclusion": [
        "In this paper, we challenge a prevailing assumption in pretraining neural language models and show that domainspecific pretraining from scratch can significantly outperform mixed-domain pretraining such as continual pretraining from a general-domain language model, leading to new state-of-the-art results for a wide range of biomedical NLP applications. To facilitate this study, we create BLURB, a comprehensive benchmark for biomedical NLP featuring a diverse set of tasks such as named entity recognition, relation extraction, document classification, and question answering. To accelerate research in biomedical NLP, we release our state-of-the-art biomedical BERT models and setup a leaderboard based on BLURB.",
        "Future directions include: further exploration of domain-specific pretraining strategies; incorporating more tasks in biomedical NLP; extension of the BLURB benchmark to clinical and other high-value domains."
      ]
    }
  },
  {
    "paperId": "873a581320d928249609d3c07229d5af182a379c",
    "title": "Is ChatGPT a General-Purpose Natural Language Processing Task Solver?",
    "sections": {
      "Introduction": [
        "Large language models (LLMs) have been shown to be able to solve a variety of natural language processing (NLP) tasks zero shot-i.e., without relying on any training data for a given downstream task-by conditioning the model on appropriate prompts  (Brown et al., 2020; Chowdhery et al., 2022a) . The ability to perform new tasks based on instructions can be seen as an important step towards artificial general intelligence  (Goertzel, 2014) . Despite achieving reasonable performance in some cases, current LLMs are still prone to various mistakes in zero-shot learning. In addition, the format of the prompt can have a substantial impact-for example, simply adding \"Let's think step by step\"  (Kojima et al., 2022)  has been shown to significantly improve the performance of  In-structGPT (Ouyang et al., 2022)  on reasoning tasks. These limitations illustrate that current LLMs are not truly general-purpose language systems.",
        "Recently, the ChatGPT LLM released by Ope-nAI has attracted a great deal of attention from the NLP community. ChatGPT was created by training a GPT-3.5 series model through reinforcement learning from human feedback (RLHF)  (Christiano et al., 2017)  (similarly to InstructGPT). RLHF mainly includes three steps: training a language model with supervised learning, collecting comparison data based on human preferences and training a reward model, and optimizing the language model against the reward model using reinforcement learning  (Ouyang et al., 2022) . Through RLHF training, ChatGPT has been observed to have impressive capabilities in various aspects, including generating high-quality responses to human input, rejecting inappropriate questions, and self-correcting previous errors based on subsequent conversations  (Guo et al., 2023) .",
        "While ChatGPT shows strong dialogic capabilities, it still remains unclear to the NLP community whether ChatGPT attains better zero-shot generalization compared with existing LLMs. To fill in this research gap, we systematically study the zeroshot learning capability of ChatGPT by evaluating it on a large collection of NLP datasets covering 7 representative task categories, including reasoning 2  , natural language inference, question answering (reading comprehension), dialogue, summarization, named entity recognition, and sentiment",
        "Figure  1 : Performance of ChatGPT, GPT-3.5, and models fine-tuned with task-specific data for 20 different datasets.",
        "For each reasoning dataset, the better result between zero-shot and zero-shot chain-of-thought is shown. Measures of SAMsum, CoNLL03, and the rest are ROUGE-1/2/L average, F1, accuracy, respectively.",
        "analysis. With extensive experiments, we aim to answer the following research questions:",
        "• Is ChatGPT a general-purpose NLP task solver? On what types of tasks does ChatGPT perform well? • If ChatGPT fell behind other models on certain tasks, why?",
        "To answer these questions, we empirically compare the performance of ChatGPT (gpt-3.5-turbo) and the previous . In addition, we report zero-shot, fine-tuned, or few-shot fine-tuned results from recent work such as FLAN  (Wei et al., 2021) , T0  (Sanh et al., 2021b) , and PaLM  (Chowdhery et al., 2022b) ."
      ],
      "Related Work": [
        "This work mainly explores the zero-shot learning capability of ChatGPT on a diverse collection of datasets including reasoning and classic NLP tasks.",
        "In light of this, we review three lines of research that form the basis of this work: large language models, zero-shot learning, and chain-of-thought prompting for reasoning."
      ],
      "Methods": [
        "As mentioned in Section 1, we mainly compare the zero-shot learning performance of ChatGPT (gpt-3.5-turbo) and GPT-3.5 (text-davinci-003) on different tasks. Given a task instruction P and a test problem X that are concatenated as the input, the model f is expected to generate a target text Y = f (P, X) to address the test problem. The instructions and input formats of different tasks are shown in Figure  2  and 3 . For example, when the model performs sentiment analysis tasks, the task instruction P is \"For each snippet of text, label the sentiment of the text as positive or negative. The answer should be exact 'positive' or 'negative'.\". After reading the instruction P and the input X \"it 's a stunning lyrical work of considerable force and truth.\", the model is expected to generate the output Y \"positive\".",
        "Different from this single-stage prompting method, we use the same two-stage prompting as  Kojima et al. (2022)  for zero-shot-CoT. In the first stage, we adopt \"Let's think step by step.\" as the instruction P 1 to induce the model to generate the rationale R. In the second stage, we use the selfgenerated rationale R along with the original input X and the instruction P 1 as the new input to guide the model to generate the final answer. A new instruction P 2 , e.g., \"Therefore, among A through E, the answer is\", serves as the trigger sentence for extracting the answer. All task instructions are taken from or inspired by Brown et al. (  2020"
      ],
      "Results": [
        "In this section, we first describe the tasks and datasets, and then present the experimental results.",
        "We now present and analyze the empirical results of different categories of tasks."
      ],
      "Discussion": [
        "Table  12  compares the accuracy of different models on the sentiment analysis dataset: SST2. ChatGPT achieves much better performance than GPT-3.5.",
        "To look into why ChatGPT outperforms GPT-3.5, we calculate the per-class accuracy of both models. We can observe that the performance of Chat-GPT on different classes is unbalanced. It outperforms GPT-3.5 by a large margin on negative samples while the performance on positively-labeled data comes close to that of GPT-3.5. We hypothesize that this difference is caused by the different training data of ChatGPT and GPT-3.5. In addition, although we explicitly specified that the answer should be exact \"positive\" or \"negative\" in task instructions (Figure  2 ), ChatGPT and GPT-3.5 still output some other answers, e.g., \"neutral\" and \"mixed\", which partly explains why they perform much worse than FLAN."
      ],
      "Conclusion": [
        "We Table  12 : Accuracy (%) of different models on sentiment analysis (SST2). We compare zero-shot Chat-GPT with recent models including GPT-3.5 (zeroshot)  (Brown et al., 2020) , FLAN (zero-shot)  (Wei et al., 2021) , and T5-11B (fine-tuned)  (Raffel et al., 2019) .",
        "demonstrated the effectiveness and current limitations of ChatGPT in different types of NLP tasks.",
        "For example, as a powerful generalist model, on one hand, ChatGPT is good at reasoning and dialogue tasks; on the other hand, ChatGPT still faces challenges when solving specific tasks, such as sequence tagging. We hope that this study can inspire future works, such as leveraging the reasoning and dialogue capabilities of ChatGPT in NLP tasks and addressing limitations of generalist models in tasks where they currently struggle with.",
        "This work is an empirical study on the zero-shot learning ability of ChatGPT 3 , and it has several limitations. First, due to the cost of ChatGPT, this work excludes larger-scale datasets and more task categories, which might prevent further insights. Besides, we report the best result in the corresponding paper for models that are not publicly available (e.g., PaLM) and report the result based on the best prompt found for public models, which is consistent with the previous work  (Wei et al., 2022; Kojima et al., 2022; Tay et al., 2022) . A further im-  Q: Bianca had 45 coloring books. If she gave away 6 of them, but then bought 20 more, how many would she have total?",
        "A: The answer (arabic numerals) is 59.",
        "Q: Janet was playing a video game and had 47 lives. In a hard part of the game she lost 23 lives. If she got 46 more lives in the next level, how many lives would she have?",
        "A: The answer (arabic numerals) is 70 lives. You can find this by starting with the number of lives she had at the beginning (47) and subtracting the number of lives she lost (23). Then add the number of lives she gained (46) to that number: 47 -23 + 46 = 70.",
        "Q: A toy store had 17 giant stuffed bears in stock when they got another shipment with 10 bears in it.",
        "The put the bears onto shelves with 9 on each shelf. How many shelves did they use?",
        "A: The answer (arabic numerals) is 3. Explanation: The toy store started with 17 bears, and got another 10, for a total of 17+10 = 27 bears. To put these bears on shelves, they need 27 bears / 9 bears per shelf = 3 shelves.",
        "Q: A group of 3 friends went into a restaurant. The chef already had 6 chicken wings cooked but cooked 3 more for the group. If they each got the same amount how many would each person get?",
        "A: The answer (arabic numerals) is 3 chicken wings per person.",
        "Q: George had 28 socks. If he threw away 4 old ones that didn't fit and bought 36 new ones, how many socks would he have?",
        "A: The answer (arabic numerals) is 60. George would have 60 socks. (28 + 36 = 60) Q: At the town carnival Billy rode the ferris wheel 7 times and the bumper cars 3 times. If each ride cost 5 tickets, how many tickets did he use?",
        "A: The answer (arabic numerals) is 40 tickets.",
        "Q: Kaleb had saved up 21 dollars. If he received another 15 dollars for his allowance, how many 6 dollar toys could he buy?",
        "A: The answer (arabic numerals) is 5. He would have a total of 21 + 15 = 36 dollars, and could buy 36 / 6 = 6 toys.",
        "Q: Tom was putting his spare change into piles. He had 2 piles of quarters and 4 piles of dimes. If each pile had 5 coins in it, how many coins did he have total?",
        "A: The answer (arabic numerals) is 50 coins. (2 piles of quarters * 5 coins per pile = 10 quarters) + (4 piles of dimes * 5 coins per pile = 20 dimes) = 50 coins.",
        "Q: Isabel uploaded 25 pictures to Facebook. She put 10 pics into one album and put the rest into 5 different albums. How many pictures were in each album?",
        "A: The answer (arabic numerals) is 10 in one album and 5 in each of the other 5 albums.",
        "Q: For homework Faye had 13 math problems and 2 spelling problems. If she can finish 3 problems in an hour how long will it take her to finish all the problems?",
        "A: The answer (arabic numerals) is 8 hours. You can get this by adding the number of math problems (13) and spelling problems (2) to get a total of 15 problems. Then divide the total number of problems (  15 ) by the number of problems Faye can finish in an hour (3) to get 5. Finally, divide 5 by 1 hour to get the answer of 5 hours. A: Let's think step by step. Tom had 2 piles of quarters, which means he had 2 * 5 = 10 quarters. He also had 4 piles of dimes, so he had 4 * 5 = 20 dimes. The total number of coins he had is 10 + 20 = 30 coins. Therefore, the answer (arabic numerals) is 30 Q: Isabel uploaded 25 pictures to Facebook. She put 10 pics into one album and put the rest into 5 different albums. How many pictures were in each album? A: Let's think step by step. Isabel put 10 pics into one album, so each of the 5 other albums had 25-10=15 pics. Answer: 15 pics/album. Therefore, the answer (arabic numerals) is 15.",
        "Q: For homework Faye had 13 math problems and 2 spelling problems. If she can finish 3 problems in an hour how long will it take her to finish all the problems? A: Let's think step by step. 13 math problems + 2 spelling problems = 15 problems. 15 problems / 3 problems per hour = 5 hours. Therefore, the answer (arabic numerals) is 5 hours. Q: A store offers sandwiches in 3 different package sizes: the first package with 2 sandwiches for $3, the second with 4 sandwiches for $6, and a third with 8 sandwiches, but the price is not indicated. Knowing that the prices are proportional, how much will the package with 8 sandwiches cost?",
        "A: The answer (arabic numerals) is 12 dollars."
      ]
    }
  },
  {
    "paperId": "641a9749fe546a02bbab9a86bfc91492db1c3bc5",
    "title": "Stanza: A Python Natural Language Processing Toolkit for Many Human Languages",
    "sections": {
      "Introduction": [
        "The growing availability of open-source natural language processing (NLP) toolkits has made it easier for users to build tools with sophisticated linguistic processing. While existing NLP toolkits such as CoreNLP  (Manning et al., 2014) , FLAIR  (Akbik et al., 2019) , spaCy 1 , and UDPipe  (Straka, 2018)  have had wide usage, they also suffer from several limitations. First, existing toolkits often support only a few major languages. This has significantly limited the community's ability to process multilingual text. Second, widely used tools are sometimes under-optimized for accuracy either due to a focus on efficiency (e.g., spaCy) or use of less powerful models (e.g., CoreNLP), potentially mislead- Sta n z a takes multilingual text as input, and produces annotations accessible as native Python objects. Besides this neural pipeline, Sta n z a also features a Python client interface to the Java CoreNLP software."
      ],
      "Results": [
        "To establish benchmark results and compare with other popular toolkits, we trained and evaluated Sta n z a on a total of 112 datasets. All pretrained models are publicly downloadable. Table  2 : Neural pipeline performance comparisons on the Universal Dependencies (v2.5) test treebanks. For our system we show macro-averaged results over all 100 treebanks. We also compare our system against UDPipe and spaCy on treebanks of five major languages where the corresponding pretrained models are publicly available. All results are F 1 scores produced by the 2018 UD Shared Task official evaluation script."
      ],
      "Conclusion": [
        "We introduced Sta n z a , a Python natural language processing toolkit supporting many human languages. We have showed that Sta n z a 's neural pipeline not only has wide coverage of human languages, but also is accurate on all tasks, thanks to its language-agnostic, fully neural architectural design. Simultaneously, Sta n z a 's CoreNLP client extends its functionality with additional NLP tools.  For future work, we consider the following areas of improvement in the near term:",
        "• Models downloadable in Sta n z a are largely trained on a single dataset. To make models robust to many different genres of text, we would like to investigate the possibility of pooling various sources of compatible data to train \"default\" models for each language;",
        "• The amount of computation and resources available to us is limited. We would therefore like to build an open \"model zoo\" for Sta n z a , so that researchers from outside our group can also contribute their models and benefit from models released by others;",
        "• Sta n z a was designed to optimize for accuracy of its predictions, but this sometimes comes at the cost of computational efficiency and limits the toolkit's use. We would like to further investigate reducing model sizes and speeding up computation in the toolkit, while still maintaining the same level of accuracy.",
        "• We would also like to expand Sta n z a 's functionality by adding other processors such as neural coreference resolution or relation extraction for richer text analytics."
      ]
    }
  },
  {
    "paperId": "23ffaa0fe06eae05817f527a47ac3291077f9e58",
    "title": "Rethinking the Inception Architecture for Computer Vision",
    "sections": {
      "Introduction": [
        "Since the 2012 ImageNet competition  [16]  winning entry by Krizhevsky et al  [9] , their network \"AlexNet\" has been successfully applied to a larger variety of computer vision tasks, for example to object-detection  [5] , segmentation  [12] , human pose estimation  [22] , video classification  [8] , object tracking  [23] , and superresolution  [3] .",
        "These successes spurred a new line of research that focused on finding higher performing convolutional neural networks. Starting in 2014, the quality of network architectures significantly improved by utilizing deeper and wider networks. VGGNet  [18]  and GoogLeNet  [20]  yielded simi-larly high performance in the 2014 ILSVRC  [16]  classification challenge. One interesting observation was that gains in the classification performance tend to transfer to significant quality gains in a wide variety of application domains. This means that architectural improvements in deep convolutional architecture can be utilized for improving performance for most other computer vision tasks that are increasingly reliant on high quality, learned visual features. Also, improvements in the network quality resulted in new application domains for convolutional networks in cases where AlexNet features could not compete with hand engineered, crafted solutions, e.g. proposal generation in detection  [4] .",
        "Although VGGNet  [18]  has the compelling feature of architectural simplicity, this comes at a high cost: evaluating the network requires a lot of computation. On the other hand, the Inception architecture of GoogLeNet  [20]  was also designed to perform well even under strict constraints on memory and computational budget. For example, GoogleNet employed only 5 million parameters, which represented a 12× reduction with respect to its predecessor AlexNet, which used 60 million parameters. Furthermore, VGGNet employed about 3x more parameters than AlexNet.",
        "The computational cost of Inception is also much lower than VGGNet or its higher performing successors  [6] . This has made it feasible to utilize Inception networks in big-data scenarios  [17] ,  [13] , where huge amount of data needed to be processed at reasonable cost or scenarios where memory or computational capacity is inherently limited, for example in mobile vision settings. It is certainly possible to mitigate parts of these issues by applying specialized solutions to target memory use  [2] ,  [15]  or by optimizing the execution of certain operations via computational tricks  [10] . However, these methods add extra complexity. Furthermore, these methods could be applied to optimize the Inception architecture as well, widening the efficiency gap again.",
        "Still, the complexity of the Inception architecture makes it more difficult to make changes to the network. If the architecture is scaled up naively, large parts of the computational gains can be immediately lost. Also,  [20]  does not provide a clear description about the contributing factors that lead to the various design decisions of the GoogLeNet architecture. This makes it much harder to adapt it to new use-cases while maintaining its efficiency. For example, if it is deemed necessary to increase the capacity of some Inception-style model, the simple transformation of just doubling the number of all filter bank sizes will lead to a 4x increase in both computational cost and number of parameters. This might prove prohibitive or unreasonable in a lot of practical scenarios, especially if the associated gains are modest. In this paper, we start with describing a few general principles and optimization ideas that that proved to be useful for scaling up convolution networks in efficient ways. Although our principles are not limited to Inceptiontype networks, they are easier to observe in that context as the generic structure of the Inception style building blocks is flexible enough to incorporate those constraints naturally. This is enabled by the generous use of dimensional reduction and parallel structures of the Inception modules which allows for mitigating the impact of structural changes on nearby components. Still, one needs to be cautious about doing so, as some guiding principles should be observed to maintain high quality of the models."
      ],
      "Methods": [
        "We have trained our networks with stochastic gradient utilizing the TensorFlow [1] distributed machine learning system using 50 replicas running each on a NVidia Kepler GPU with batch size 32 for 100 epochs. Our earlier experiments used momentum  [19]  with a decay of 0.9, while our best models were achieved using RMSProp  [21]  with decay of 0.9 and = 1.0. We used a learning rate of 0.045, decayed every two epoch using an exponential rate of 0.94. In addition, gradient clipping  [14]  with threshold 2.0 was found to be useful to stabilize the training. Model evaluations are performed using a running average of the parameters computed over time."
      ],
      "Conclusion": [
        "We have provided several design principles to scale up convolutional networks and studied them in the context of the Inception architecture. This guidance can lead to high performance vision networks that have a relatively modest computation cost compared to simpler, more monolithic architectures. Our highest quality version of Inception-v3 reaches 21.2%, top-1 and 5.6% top-5 error for single crop evaluation on the ILSVR 2012 classification, setting a new state of the art. This is achieved with relatively modest (2.5×) increase in computational cost compared to the network described in Ioffe et al  [7] . Still our solution uses much less computation than the best published results based on denser networks: our model outperforms the results of He et al  [6]  -cutting the top-5 (top-1) error by 25% (14%) relative, respectively -while being six times cheaper computationally and using at least five times less parameters (estimated). Our ensemble of four Inception-v3 models reaches 3.5% with multi-crop evaluation reaches 3.5% top-5 error which represents an over 25% reduction to the best published results and is almost half of the error of ILSVRC 2014 winining GoogLeNet ensemble.",
        "We have also demonstrated that high quality results can be reached with receptive field resolution as low as 79 × 79. This might prove to be helpful in systems for detecting relatively small objects. We have studied how factorizing convolutions and aggressive dimension reductions inside neural network can result in networks with relatively low computational cost while maintaining high quality. The combination of lower parameter count and additional regularization with batch-normalized auxiliary classifiers and label-smoothing allows for training high quality networks on relatively modest sized training sets."
      ]
    }
  },
  {
    "paperId": "ad06c8a5fd292af518f878c7ced132b61739cdd8",
    "title": "A review of convolutional neural networks in computer vision",
    "sections": {
      "Introduction": [
        "Computer vision is gaining popularity as a buzzword in the field of image processing. Human activity recognition (HAR), an established trend with numerous real-life applications including elderly care monitoring, rehabilitation activity tracking, posture correction analysis, and intrusion detection in security, is a prominent area of research in the field of computer vision  (Singh and Vishwakarma 2019) . Over the years, deep learning advances in computer vision have attracted the attention of many scholars in the field of human action recognition  (Vishwakarma and Singh 2019; Singh and Vishwakarma 2021; Dhiman and Vishwakarma 2020) . The convolutional neural network (CNN) is Xia Zhao and Limin Wang have contributed equally to this work.",
        "Extended author information available on the last page of the article used to construct the majority of computer vision algorithms. A convolutional neural network  (Li et al. 2021) , known for local connectivity of neurons, weight sharing, and down-sampling, is a deep feed-forward multilayered hierarchical network inspired by the receptive field mechanism in biology. As one of the deep learning models, a CNN can also achieve \"end-to-end\" learning. Through multiple layers of feature transformation, the underlying feature representation of the original data is gradually transformed into a higher-level feature representation, and the processed data is fed into a prediction function to settle the final classification or other tasks. The representation learned by the machine itself can generate good features, avoiding \"feature engineering\".",
        "In 2006,  Hinton et al.  proposed several perspectives in their article, which was published in Science  (Hinton and Salakhutdinov 2006) , including (1) that artificial neural networks with multiple hidden layers have a robust feature learning capability and (2) that the difficulty of training deep neural networks can be greatly reduced by the \"layerby-layer initialization\" method. Since then, deep learning has become a hot topic in both academia and industry, and it has made a splash in computer vision, speech recognition, machine translation, and other fields. Meanwhile, another learning boom in artificial neural networks  (Yu et al. 2013 ) has kicked off. As a typical neural network model of deep learning, a CNN has also gained wide attention from all walks of life. One of the most widely concerned is AlexNet  (Alom et al. 2018) , which won the ImageNet Large Scale Visual Recognition Competition (ILSVRC) in 2012 due to its excellent performance. With the improvement of AlexNet's accuracy on computer vision tasks such as image classification, researchers started to remedy the defects of the network models based on AlexNet in the expectation of further enhancing their performance. Significant advances have been made in model optimization, and some of the most representative neural network models are Visual Geometry Group (VGG)  (Sengupta et al. 2019) , GoogLeNet  (Khan et al. 2019) , Residual Network (ResNet)  (Wightman et al. 2021) , Squeeze and Excitation Network (SENet)  (Jin et al. 2022) , and MobileNet  (Chen et al. 2022) . With the development of these network architectures, neural network models tend to be deeper, wider, and more complex. Although this evolution can facilitate the networks to capture better feature representations, there is no guarantee that it can operate efficiently in all cases. Models still suffer from disadvantages such as the fact that the networks are more likely to fall into overfitting, and instead of decreasing, the error rate of the training set increases as the networks become deeper and more complex. To remedy the shortcomings of these models, many scholars have come up with various techniques to optimize the structure of CNN, e.g., network pruning  (Yang et al. 2023) , knowledge distilling  (Guo et al. 2023) , and tensor decomposition  (Fernandes et al. 2021) .",
        "Despite the significant achievements of CNN in computer vision applications such as image classification  (Chandra and Bedi 2021) , object detection  (Ma et al. 2023) , speech recognition  (Li et al. 2022) , sentiment analysis  (Chan et al. 2023) , and video recognition  (Yan et al. 2022) , the field continues to face various challenges and opportunities. As computer vision tasks become increasingly complex, there is a pressing need for CNN models and algorithms that offer higher performance and efficiency. Moreover, current research focuses on addressing key issues such as knowledge sharing across different tasks, domain adaptation, and interpretability. Given these things into account, this paper aims to comprehensively summarize and analyze the applications of CNN in computer vision, with a particular emphasis on the latest advancements in tasks including image classification, object detection, and video prediction. The contributions of this survey paper are summarized below:",
        "• A holistic literature review of CNN in computer vision, including image classification, object detection, and video prediction, is presented in this paper. • A theoretical understanding of the CNN design principles and techniques, such as convolution, filter size, stride, down sampling, optimizer, etc., is explained in detail. • The image classification and object detection performance obtained using the existing algorithms on the dataset of the domain to which they belong are compared, respectively. • Classical architectures for deep learning and CNN-based visual models are highlighted. • The current challenges involved and future research directions for CNN are identified and presented.",
        "The remaining part of the paper proceeds as follows (shown in Fig.  1 ): Section 2 gives a basic introduction to the elementary components of CNN and their corresponding functions. Sections 3, 4, and 5 summarize the relevant research models and methods in three application directions, namely, image classification, object detection, and video prediction, respectively. In Sects. 6 and 7, through synthesizing the current research status, the issues of CNN are analyzed and summarized. In addition, an outlook on future research trends is provided."
      ],
      "Conclusion": [
        "CNN has made impressive strides, particularly in image processing and video-related tasks, which has rekindled interest in deep learning among academics. Several studies have been done in this context to enhance CNN's performance, including activation, optimization, regularization, and innovations in architecture. This paper reviews the research progress of CNN architecture in computer vision, especially in image classification, target detection, and video prediction. In addition, this paper also covers the fundamental elements of CNN, its applications, challenges, and future directions. We have shown that CNN outperforms classical methods when it comes to classification, detection, and prediction. Through exploiting depth and other structural modifications, CNN's learning performance has dramatically improved over time. According to recent literature, the increase in CNN performance is primarily attributable to the replacement of the conventional layer structure with blocks. The function of an auxiliary learner can be performed by a block in a network. These additional learners leverage spatial or feature-map information or even enhance input channels to increase performance. Additionally, modular learning is supported by CNN's block-based design, which makes the structure easier to grasp. As a review, this paper will inevitably suffer from the following shortcomings: First, it is limited by the scope of literature and time, resulting in the failure to comprehensively cover all relevant research work. Research on certain emerging areas or specific application scenarios may fail to be covered, and there are certain research blind spots. Second, considering the influence of subjectivity, we realize that the review may be influenced by the subjective judgment of the authors, which may have a certain impact on the objectivity of the research area. As a result, in future studies, we will need to sift through the relevant literature more thoroughly and deal with the subjective factors more cautiously in order to comprehend and investigate the application of CNN in computer vision in a more comprehensive and in-depth manner."
      ]
    }
  },
  {
    "paperId": "45f686be3b96302ede327645227134e1c304dbab",
    "title": "Attention mechanisms in computer vision: A survey",
    "sections": {
      "Methods": [
        "In this section, we first sum up a general form for the attention mechanism based on the recognition process of human visual system in Section 3.1. Then we review various categories of attention models given in Fig.  2 , with a subsection dedicated to each category.",
        "In each, we tabularize representative works for that category. We also introduce that category of attention strategy more deeply, considering its development in terms of motivation, formulation, and function."
      ],
      "Conclusion": [
        "Attention mechanisms have become an indispensable technique in the field of computer vision in the era of deep learning. This survey has systematically reviewed and summarized attention mechanisms for deep neural networks in computer vision. We have grouped different attention methods according to their domain of operation, rather than by application task, and show that attention models can be regarded as an independent topic in their own right. We have concluded with some potential directions for future research. We hope that this work will encourage a variety of potential application developers to put attention mechanisms to use to improve their deep learning results. We also hope that this survey will give researchers a deeper understanding of various attention mechanisms and the relationships between them, as a springboard for future research."
      ]
    }
  },
  {
    "paperId": "c8b25fab5608c3e033d34b4483ec47e68ba109b7",
    "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows",
    "sections": {
      "Introduction": [
        "Modeling in computer vision has long been dominated by convolutional neural networks (CNNs). Beginning with AlexNet  [39]  and its revolutionary performance on the ImageNet image classification challenge, CNN architectures have evolved to become increasingly powerful through * Equal contribution. † Interns at MSRA. ‡ Contact person. greater scale  [30, 76] , more extensive connections  [34] , and more sophisticated forms of convolution  [70, 18, 84] . With CNNs serving as backbone networks for a variety of vision tasks, these architectural advances have led to performance improvements that have broadly lifted the entire field.",
        "On the other hand, the evolution of network architectures in natural language processing (NLP) has taken a different path, where the prevalent architecture today is instead the Transformer  [64] . Designed for sequence modeling and transduction tasks, the Transformer is notable for its use of attention to model long-range dependencies in the data. Its tremendous success in the language domain has led researchers to investigate its adaptation to computer vision, where it has recently demonstrated promising results on certain tasks, specifically image classification  [20]  and joint vision-language modeling  [47] .",
        "In this paper, we seek to expand the applicability of Transformer such that it can serve as a general-purpose backbone for computer vision, as it does for NLP and as CNNs do in vision. We observe that significant challenges in transferring its high performance in the language domain to the visual domain can be explained by differences between the two modalities. One of these differences involves scale. Unlike the word tokens that serve as the basic elements of processing in language Transformers, visual elements can vary substantially in scale, a problem that receives attention in tasks such as object detection  [42, 53, 54] . In existing Transformer-based models  [64, 20] , tokens are all of a fixed scale, a property unsuitable for these vision applications. Another difference is the much higher resolution of pixels in images compared to words in passages of text. There exist many vision tasks such as semantic segmentation that require dense prediction at the pixel level, and this would be intractable for Transformer on high-resolution images, as the computational complexity of its self-attention is quadratic to image size. To overcome these issues, we propose a generalpurpose Transformer backbone, called Swin Transformer, which constructs hierarchical feature maps and has linear computational complexity to image size. As illustrated in Figure  1 (a), Swin Transformer constructs a hierarchical representation by starting from small-sized patches (outlined in gray) and gradually merging neighboring patches in deeper Transformer layers. With these hierarchical feature maps, the Swin Transformer model can conveniently leverage advanced techniques for dense prediction such as feature pyramid networks (FPN)  [42]  or U-Net  [51] . The linear computational complexity is achieved by computing self-attention locally within non-overlapping windows that partition an image (outlined in red). The number of patches in each window is fixed, and thus the complexity becomes linear to image size. These merits make Swin Transformer suitable as a general-purpose backbone for various vision tasks, in contrast to previous Transformer based architectures  [20]  which produce feature maps of a single resolution and have quadratic complexity.",
        "A key design element of Swin Transformer is its shift of the window partition between consecutive self-attention layers, as illustrated in Figure  2 . The shifted windows bridge the windows of the preceding layer, providing connections among them that significantly enhance modeling power (see Table  4 ). This strategy is also efficient in regards to real-world latency: all query patches within a window share the same key set 1  , which facilitates memory access in hardware. In contrast, earlier sliding window based self-attention approaches  [33, 50]  suffer from low latency on general hardware due to different key sets for different query pixels 2  . Our experiments show that the proposed Figure  2 . An illustration of the shifted window approach for computing self-attention in the proposed Swin Transformer architecture. In layer l (left), a regular window partitioning scheme is adopted, and self-attention is computed within each window. In the next layer l + 1 (right), the window partitioning is shifted, resulting in new windows. The self-attention computation in the new windows crosses the boundaries of the previous windows in layer l, providing connections among them.",
        "shifted window approach has much lower latency than the sliding window method, yet is similar in modeling power (see Tables  5  and 6 ). The shifted window approach also proves beneficial for all-MLP architectures  [61] .",
        "The proposed Swin Transformer achieves strong performance on the recognition tasks of image classification, object detection and semantic segmentation. It outperforms the ViT / DeiT  [20, 63]  and ResNe(X)t models  [30, 70]  significantly with similar latency on the three tasks. Its 58.7 box AP and 51.1 mask AP on the COCO test-dev set surpass the previous state-of-the-art results by +2.7 box AP (Copy-paste  [26]  without external data) and +2.6 mask AP (DetectoRS  [46] ). On ADE20K semantic segmentation, it obtains 53.5 mIoU on the val set, an improvement of +3.2 mIoU over the previous state-of-the-art (SETR  [81] ). It also achieves a top-1 accuracy of 87.3% on ImageNet-1K image classification.",
        "It is our belief that a unified architecture across computer vision and natural language processing could benefit both fields, since it would facilitate joint modeling of visual and textual signals and the modeling knowledge from both domains can be more deeply shared. We hope that Swin Transformer's strong performance on various vision problems can drive this belief deeper in the community and encourage unified modeling of vision and language signals."
      ],
      "Related Work": [
        "CNN and variants CNNs serve as the standard network model throughout computer vision. While the CNN has existed for several decades  [40] , it was not until the introduction of AlexNet  [39]  that the CNN took off and became mainstream. Since then, deeper and more effective convolutional neural architectures have been proposed to further propel the deep learning wave in computer vision, e.g., VGG  [52] , GoogleNet  [57] , ResNet  [30] , DenseNet  [34] ,",
        "weights across a feature map, it is difficult for a sliding-window based self-attention layer to have efficient memory access in practice.",
        "HRNet  [65] , and EfficientNet  [58] . In addition to these architectural advances, there has also been much work on improving individual convolution layers, such as depthwise convolution  [70]  and deformable convolution  [18, 84] . While the CNN and its variants are still the primary backbone architectures for computer vision applications, we highlight the strong potential of Transformer-like architectures for unified modeling between vision and language. Our work achieves strong performance on several basic visual recognition tasks, and we hope it will contribute to a modeling shift.",
        "Self-attention based backbone architectures Also inspired by the success of self-attention layers and Transformer architectures in the NLP field, some works employ self-attention layers to replace some or all of the spatial convolution layers in the popular ResNet  [33, 50, 80] . In these works, the self-attention is computed within a local window of each pixel to expedite optimization  [33] , and they achieve slightly better accuracy/FLOPs trade-offs than the counterpart ResNet architecture. However, their costly memory access causes their actual latency to be significantly larger than that of the convolutional networks  [33] . Instead of using sliding windows, we propose to shift windows between consecutive layers, which allows for a more efficient implementation in general hardware.",
        "Self-attention/Transformers to complement CNNs Another line of work is to augment a standard CNN architecture with self-attention layers or Transformers. The selfattention layers can complement backbones  [67, 7, 3, 71, 23, 74, 55]  or head networks  [32, 27]  by providing the capability to encode distant dependencies or heterogeneous interactions. More recently, the encoder-decoder design in Transformer has been applied for the object detection and instance segmentation tasks  [8, 13, 85, 56] . Our work explores the adaptation of Transformers for basic visual feature extraction and is complementary to these works.",
        "Transformer based vision backbones Most related to our work is the Vision Transformer (ViT)  [20]  and its follow-ups  [63, 72, 15, 28, 66] . The pioneering work of ViT directly applies a Transformer architecture on nonoverlapping medium-sized image patches for image classification. It achieves an impressive speed-accuracy tradeoff on image classification compared to convolutional networks. While ViT requires large-scale training datasets (i.e., JFT-300M) to perform well, DeiT  [63]  introduces several training strategies that allow ViT to also be effective using the smaller ImageNet-1K dataset. The results of ViT on image classification are encouraging, but its architecture is unsuitable for use as a general-purpose backbone network on dense vision tasks or when the input image resolution is high, due to its low-resolution feature maps and the quadratic increase in complexity with image size. There are a few works applying ViT models to the dense vision tasks of object detection and semantic segmentation by direct upsampling or deconvolution but with relatively lower performance  [2, 81] . Concurrent to our work are some that modify the ViT architecture  [72, 15, 28]  for better image classification. Empirically, we find our Swin Transformer architecture to achieve the best speedaccuracy trade-off among these methods on image classification, even though our work focuses on general-purpose performance rather than specifically on classification. Another concurrent work  [66]  explores a similar line of thinking to build multi-resolution feature maps on Transformers. Its complexity is still quadratic to image size, while ours is linear and also operates locally which has proven beneficial in modeling the high correlation in visual signals  [36, 25, 41] . Our approach is both efficient and effective, achieving state-of-the-art accuracy on both COCO object detection and ADE20K semantic segmentation."
      ],
      "Results": [
        "We conduct experiments on ImageNet-1K image classification  [19] , COCO object detection  [43] , and ADE20K semantic segmentation  [83] . In the following, we first compare the proposed Swin Transformer architecture with the previous state-of-the-arts on the three tasks. Then, we ablate the important design elements of Swin Transformer."
      ],
      "Conclusion": [
        "As a key element of Swin Transformer, the shifted window based self-attention is shown to be effective and efficient on vision problems, and we look forward to investigating its use in natural language processing as well."
      ]
    }
  },
  {
    "paperId": "6351ebb4a3287f5f3e1273464b3b91e5df5a16d7",
    "title": "Masked Autoencoders Are Scalable Vision Learners",
    "sections": {
      "Introduction": [
        "Deep learning has witnessed an explosion of architectures of continuously growing capability and capacity  [33, 25, 57] . Aided by the rapid gains in hardware, models today can easily overfit one million images  [13]  and begin to demand hundreds of millions of-often publicly inaccessible-labeled images  [16] .",
        "This appetite for data has been successfully addressed in natural language processing (NLP) by self-supervised pretraining. The solutions, based on autoregressive language modeling in GPT  [47, 48, 4]  and masked autoencoding in BERT  [14] , are conceptually simple: they remove a portion of the data and learn to predict the removed content. These methods now enable training of generalizable NLP models containing over one hundred billion parameters  [4] .",
        "The idea of masked autoencoders, a form of more general denoising autoencoders  [58] , is natural and applicable in computer vision as well. Indeed, closely related research ) is masked out. The encoder is applied to the small subset of visible patches. Mask tokens are introduced after the encoder, and the full set of encoded patches and mask tokens is processed by a small decoder that reconstructs the original image in pixels. After pre-training, the decoder is discarded and the encoder is applied to uncorrupted images (full sets of patches) for recognition tasks.",
        "in vision  [59, 46]  preceded BERT. However, despite significant interest in this idea following the success of BERT, progress of autoencoding methods in vision lags behind NLP. We ask: what makes masked autoencoding different between vision and language?",
        "We attempt to answer this question from the following perspectives: (i) Until recently, architectures were different. In vision, convolutional networks  [34]  were dominant over the last decade  [33] . Convolutions typically operate on regular grids and it is not straightforward to integrate 'indicators' such as mask tokens  [14]  or positional embeddings  [57]  into convolutional networks. This architectural gap, however, has been addressed with the introduction of Vision Transformers (ViT)  [16]  and should no longer present an obstacle.",
        "(ii) Information density is different between language and vision. Languages are human-generated signals that are highly semantic and information-dense. When training a model to predict only a few missing words per sentence, this task appears to induce sophisticated language understanding. Images, on the contrary, are natural signals with heavy spatial redundancy-e.g., a missing patch can be recovered from neighboring patches with little high-level un-Figure  2 . Example results on ImageNet validation images. For each triplet, we show the masked image (left), our MAE reconstruction † (middle), and the ground-truth (right). The masking ratio is 80%, leaving only 39 out of 196 patches. More examples are in the appendix. † As no loss is computed on visible patches, the model output on visible patches is qualitatively worse. One can simply overlay the output with the visible patches to improve visual quality. We intentionally opt not to do this, so we can more comprehensively demonstrate the method's behavior. derstanding of parts, objects, and scenes. To overcome this difference and encourage learning useful features, we show that a simple strategy works well in computer vision: masking a very high portion of random patches. This strategy largely reduces redundancy and creates a challenging selfsupervisory task that requires holistic understanding beyond low-level image statistics. To get a qualitative sense of our reconstruction task, see Figures  2 3 4 .",
        "(iii) The autoencoder's decoder, which maps the latent representation back to the input, plays a different role between reconstructing text and images. In vision, the decoder reconstructs pixels, hence its output is of a lower semantic level than common recognition tasks. This is in contrast to language, where the decoder predicts missing words that contain rich semantic information. While in BERT the decoder can be trivial (an MLP)  [14] , we found that for images, the decoder design plays a key role in determining the semantic level of the learned latent representations.",
        "Driven by this analysis, we present a simple, effective, and scalable form of a masked autoencoder (MAE) for visual representation learning. Our MAE masks random patches from the input image and reconstructs the missing patches in the pixel space. It has an asymmetric encoderdecoder design. Our encoder operates only on the visible subset of patches (without mask tokens), and our decoder is lightweight and reconstructs the input from the latent representation along with mask tokens (Figure  1 ). Shifting the mask tokens to the small decoder in our asymmetric encoder-decoder results in a large reduction in computation. Under this design, a very high masking ratio (e.g., 75%) can achieve a win-win scenario: it optimizes accuracy while allowing the encoder to process only a small portion (e.g., 25%) of patches. This can reduce overall pre-training time by 3× or more and likewise reduce memory consumption, enabling us to easily scale our MAE to large models.",
        "Our MAE learns very high-capacity models that generalize well. With MAE pre-training, we can train datahungry models like ViT-Large/-Huge  [16]  on ImageNet-1K with improved generalization performance. With a vanilla ViT-Huge model, we achieve 87.8% accuracy when finetuned on ImageNet-1K. This outperforms all previous results that use only ImageNet-1K data. We also evaluate transfer learning on object detection, instance segmentation, and semantic segmentation. In these tasks, our pre-training achieves better results than its supervised pre-training counterparts, and more importantly, we observe significant gains by scaling up models. These observations are aligned with those witnessed in self-supervised pre-training in NLP  [14, 47, 48, 4]  and we hope that they will enable our field to explore a similar trajectory. original mask 75% mask 85% mask 95%",
        "Figure  4 . Reconstructions of ImageNet validation images using an MAE pre-trained with a masking ratio of 75% but applied on inputs with higher masking ratios. The predictions differ plausibly from the original images, showing that the method can generalize."
      ],
      "Related Work": [
        "Masked language modeling and its autoregressive counterparts, e.g., BERT  [14]  and GPT  [47, 48, 4] , are highly successful methods for pre-training in NLP. These methods hold out a portion of the input sequence and train models to predict the missing content. These methods have been shown to scale excellently  [4]  and a large abundance of evidence indicates that these pre-trained representations generalize well to various downstream tasks.",
        "Autoencoding is a classical method for learning representations. It has an encoder that maps an input to a latent representation and a decoder that reconstructs the input. For example, PCA and k-means are autoencoders  [29] . Denoising autoencoders (DAE)  [58]  are a class of autoencoders that corrupt an input signal and learn to reconstruct the original, uncorrupted signal. A series of methods can be thought of as a generalized DAE under different corruptions, e.g., masking pixels  [59, 46, 6]  or removing color channels  [70] .",
        "Our MAE is a form of denoising autoencoding, but different from the classical DAE in numerous ways.",
        "Masked image encoding methods learn representations from images corrupted by masking. The pioneering work of  [59]  presents masking as a noise type in DAE. Context Encoder  [46]  inpaints large missing regions using convolutional networks. Motivated by the success in NLP, related recent methods  [6, 16, 2]  are based on Transformers  [57] .",
        "iGPT  [6]  operates on sequences of pixels and predicts unknown pixels. The ViT paper  [16]  studies masked patch prediction for self-supervised learning. Most recently, BEiT  [2]  proposes to predict discrete tokens  [44, 50] .",
        "Self-supervised learning approaches have seen significant interest in computer vision, often focusing on different pretext tasks for pre-training  [15, 61, 42, 70, 45, 17] . Recently, contrastive learning  [3, 22]  has been popular, e.g.,  [62, 43, 23, 7] , which models image similarity and dissimilarity (or only similarity  [21, 8] ) between two or more views. Contrastive and related methods strongly depend on data augmentation  [7, 21, 8] . Autoencoding pursues a conceptually different direction, and it exhibits different behaviors as we will present."
      ],
      "Results": [
        "We do self-supervised pre-training on the ImageNet-1K (IN1K)  [13]  training set. Then we do supervised training to evaluate the representations with (i) end-to-end fine-tuning or (ii) linear probing. We report top-1 validation accuracy of a single 224×224 crop. Details are in Appendix A.1.",
        "Baseline: ViT-Large. We use ViT-Large (ViT-L/16)  [16]  as the backbone in our ablation study. ViT-L is very big (an order of magnitude bigger than ResNet-50  [25] ) and tends to overfit. The following is a comparison between ViT-L trained from scratch vs. fine-tuned from our baseline MAE: scratch, original  [16]  scratch, our impl. baseline MAE 76.5 82.5 84.9",
        "We note that it is nontrivial to train supervised ViT-L from scratch and a good recipe with strong regularization is needed (82.5%, see Appendix A.2). Even so, our MAE pretraining contributes a big improvement. Here fine-tuning is only for 50 epochs (vs. 200 from scratch), implying that the fine-tuning accuracy heavily depends on pre-training.",
        "Comparisons with self-supervised methods. In Table  3  we compare the fine-tuning results of self-supervised ViT models. For ViT-B, all methods perform closely. For ViT-L, the gaps among methods are bigger, suggesting that a challenge for bigger models is to reduce overfitting.",
        "Our MAE can scale up easily and has shown steady improvement from bigger models. We obtain 86.9% accuracy using ViT-H (224 size). By fine-tuning with a 448 size, we achieve 87.8% accuracy, using only IN1K data. The previous best accuracy, among all methods using only IN1K data, is 87.1% (512 size)  [67] , based on advanced networks. We improve over the state-of-the-art by a nontrivial margin in the highly competitive benchmark of IN1K (no external data). Our result is based on vanilla ViT, and we expect advanced networks will perform better.",
        "Comparing with BEiT  [2] , our MAE is more accurate while being simpler and faster. Our method reconstructs pixels, in contrast to BEiT that predicts tokens: BEiT reported a 1.8% degradation  [2]  when reconstructing pixels with ViT-B.  2  We do not need dVAE pre-training. Moreover, our MAE is considerably faster (3.5× per epoch) than BEiT, for the reason as studied in Table  1c .  # blocks fine-tuned Figure  9 . Partial fine-tuning results of ViT-L w.r.t. the number of fine-tuned Transformer blocks under the default settings from Table  1 . Tuning 0 blocks is linear probing; 24 is full fine-tuning.",
        "Our MAE representations are less linearly separable, but are consistently better than MoCo v3 if one or more blocks are tuned.",
        "The MAE models in Table  3  are pre-trained for 1600 epochs for better accuracy (Figure  7 ). Even so, our total pre-training time is less than the other methods when trained on the same hardware. For example, training ViT-L on 128 TPU-v3 cores, our MAE's training time is 31 hours for 1600 epochs and MoCo v3's is 36 hours for 300 epochs  [9] .",
        "Comparisons with supervised pre-training. In the original ViT paper  [16] , ViT-L degrades when trained in IN1K. Our implementation of supervised training (see A.2) works better, but accuracy saturates. See Figure  8 .",
        "Our MAE pre-training, using only IN1K, can generalize better: the gain over training from scratch is bigger for higher-capacity models. It follows a trend similar to the JFT-300M supervised pre-training in  [16] . This comparison shows that our MAE can help scale up model sizes.",
        "We evaluate transfer learning in downstream tasks using the pre-trained models in Table  3 .",
        "Object detection and segmentation. We fine-tune Mask R-CNN  [24]  end-to-end on COCO  [37] . The ViT backbone is adapted for use with FPN  [36]  (see A.3). We apply this approach for all entries in Table  4 . We report box AP for object detection and mask AP for instance segmentation.",
        "Compared to supervised pre-training, our MAE performs better under all configurations (Table  4 ). With the smaller ViT-B, our MAE is 2.4 points higher than supervised pretraining (50.3 vs. 47.9, AP box ). More significantly, with the larger ViT-L, our MAE pre-training outperforms supervised pre-training by 4.0 points  (53.3 vs. 49.3) .",
        "The pixel-based MAE is better than or on par with the token-based BEiT, while MAE is much simpler and faster. Both MAE and BEiT are better than MoCo v3 and MoCo v3 is on par with supervised pre-training.",
        "Semantic segmentation. We experiment on ADE20K  [72]  using UperNet  [63]  (see A.4). Table  5  shows that our pretraining significantly improves results over supervised pretraining, e.g., by 3.7 points for ViT-L. Our pixel-based MAE also outperforms the token-based BEiT. These observations are consistent with those in COCO. Classification tasks. Table  6  studies transfer learning on the iNaturalists  [56]  and Places  [71]  tasks (see A.5). On iNat, our method shows strong scaling behavior: accuracy improves considerably with bigger models. Our results surpass the previous best results by large margins. On Places, our MAE outperforms the previous best results  [19, 40] , which were obtained via pre-training on billions of images. Pixels vs. tokens. Table  7  compares pixels vs. tokens as the MAE reconstruction target. While using dVAE tokens is better than using unnormalized pixels, it is statistically similar to using normalized pixels across all cases we tested. It again shows that tokenization is not necessary for our MAE.   7 . Pixels vs. tokens as the MAE reconstruction target. is the difference between using dVAE tokens and using normalized pixels. The difference is statistically insignificant.",
        "ViT architecture. We follow the standard ViT architecture  [16] . It has a stack of Transformer blocks  [57] , and each block consists of a multi-head self-attention block and an MLP block, both having LayerNorm (LN)  [1] . The encoder ends with LN. As the MAE encoder and decoder have different width, we adopt a linear projection layer after the encoder to match it. Our MAE adds positional embeddings  [57]  (the sine-cosine version) to both the encoder and decoder inputs. Our MAE does not use relative position or layer scaling (which are used in the code of  [2] ).",
        "We extract features from the encoder output for finetuning and linear probing. As ViT has a class token  [16] , to adapt to this design, in our MAE pre-training we append an auxiliary dummy token to the encoder input. This token will be treated as the class token for training the classifier in linear probing and fine-tuning. Our MAE works similarly well without this token (with average pooling).",
        "Pre-training. The default setting is in Table  8 . We do not use color jittering, drop path, or gradient clip. We use xavier uniform  [18]  to initialize all Transformer blocks, following ViT's official code  [16] . We use the linear lr scaling rule  [20] : lr = base lr×batchsize / 256.",
        "End-to-end fine-tuning. Our fine-tuning follows common practice of supervised ViT training. The default setting is in Table  9 . We use layer-wise lr decay  [10]  following  [2] .",
        "Linear probing. Our linear classifier training follows  [9] . See Table  10 . We observe that linear probing requires a very different recipe than end-to-end fine-tuning. In particular, regularization is in general harmful for linear probing. Following  [9] , we disable many common regularization strategies: we do not use mixup  [69] , cutmix  [68] , drop path  [30] , or color jittering, and we set weight decay as zero.",
        "It is a common practice to normalize the classifier input when training a classical linear classifier (e.g., SVM  [11] ). Similarly, it is beneficial to normalize the pre-trained features when training the linear probing classifier. Following  [15] , we adopt an extra BatchNorm layer  [31]  without affine transformation (affine=False). This layer is applied on the pre-trained features produced by the encoder, and is before the linear classifier. We note that the layer does not break the linear property, and it can be absorbed into the linear classifier after training: it is essentially a reparameterized linear classifier.  3  Introducing this layer helps calibrate the feature magnitudes across different variants in our ablations, so that they can use the same setting without further lr search.   [52]  0.1 mixup  [69]  0.8 cutmix  [68]  1.0 drop path  [30]  0.1 (B/L) 0.2 (H) Partial fine-tuning. Our MAE partial fine-tuning ( §4.3) follows the setting in Table  9 , except that we adjust the number of fine-tuning epochs. We observe that tuning fewer blocks requires a longer schedule. We set the numbers of fine-tuning epochs as {50, 100, 200} and use the optimal one for each number of blocks tuned."
      ],
      "Discussion": [
        "Simple algorithms that scale well are the core of deep learning. In NLP, simple self-supervised learning methods (e.g.,  [47, 14, 48, 4] ) enable benefits from exponentially scaling models. In computer vision, practical pre-training paradigms are dominantly supervised (e.g.  [33, 51, 25, 16] ) despite progress in self-supervised learning. In this study, we observe on ImageNet and in transfer learning that an autoencoder-a simple self-supervised method similar to techniques in NLP-provides scalable benefits. Selfsupervised learning in vision may now be embarking on a similar trajectory as in NLP.",
        "On the other hand, we note that images and languages are signals of a different nature and this difference must be addressed carefully. Images are merely recorded light without a semantic decomposition into the visual analogue of words. Instead of attempting to remove objects, we remove random patches that most likely do not form a semantic segment. Likewise, our MAE reconstructs pixels, which are not semantic entities. Nevertheless, we observe (e.g., Figure  4 ) that our MAE infers complex, holistic reconstructions, suggesting it has learned numerous visual concepts, i.e., semantics. We hypothesize that this behavior occurs by way of a rich hidden representation inside the MAE. We hope this perspective will inspire future work.",
        "Broader impacts. The proposed method predicts content based on learned statistics of the training dataset and as such will reflect biases in those data, including ones with negative societal impacts. The model may generate inexistent content. These issues warrant further research and consideration when building upon this work to generate images."
      ]
    }
  },
  {
    "paperId": "0090023afc66cd2741568599057f4e82b566137c",
    "title": "A Survey on Bias and Fairness in Machine Learning",
    "sections": {
      "Introduction": [
        "Machine learning algorithms have penetrated every aspect of our lives. Algorithms make movie recommendations, suggest products to buy, and who to date. They are increasingly used in high-stakes scenarios such as loans  [113]  and hiring decisions  [19, 39] . There are clear benefits to algorithmic decision-making; unlike people, machines do not become tired or bored  [45, 119] , and can take into account orders of magnitude more factors than people can. However, like people, algorithms are vulnerable to biases that render their decisions \"unfair\"  [6, 121] . In the context of decision-making, fairness is the absence of any prejudice or favoritism toward an individual or group based on their inherent or acquired characteristics. Thus, an unfair algorithm is one whose decisions are skewed toward a particular group of people. A canonical example comes from a tool used by courts in the United States to make pretrial detention and release decisions. The software, Correctional Offender Management Profiling for Alternative Sanctions (COMPAS), measures the risk of a person to recommit another crime. Judges use COMPAS to decide whether to release an offender, or to keep him or her in prison. An investigation into the software found a bias against African-Americans: 1  COMPAS is more likely to have higher false positive rates for African-American offenders than Caucasian offenders in falsely predicting them to be at a higher risk of recommitting a crime or recidivism. Similar findings have been made in other areas, such as an AI system that judges beauty pageant winners but was biased against darker-skinned contestants,  2  or facial recognition software in digital cameras that overpredicts Asians as blinking.  3  These biased predictions stem from the hidden or neglected biases in data or algorithms.",
        "In this survey we identify two potential sources of unfairness in machine learning outcomesthose that arise from biases in the data and those that arise from the algorithms. We review research investigating how biases in data skew what is learned by machine learning algorithms, and nuances in the way the algorithms themselves work to prevent them from making fair decisions-even when the data is unbiased. Furthermore, we observe that biased algorithmic outcomes might impact user experience, thus generating a feedback loop between data, algorithms and users that can perpetuate and even amplify existing sources of bias.",
        "We begin the review with several highly visible real-world cases of where unfair machine learning algorithms have led to suboptimal and discriminatory outcomes in Section 2. In Section 3, we describe the different types and sources of biases that occur within the data-algorithms-users loop mentioned above. Next, in Section 4, we present the different ways that the concept of fairness has been operationalized and studied in the literature. We discuss the ways in which these two concepts are coupled. Last, we will focus on different families of machine learning approaches, how fairness manifests differently in each one, and the current state-of-the-art for tackling them in Section 5, followed by potential areas of future work in each of the domains in Section 6."
      ],
      "Methods": [
        "There have been numerous attempts to address bias in artificial intelligence in order to achieve fairness; these stem from domains of AI. In this section we will enumerate different domains of AI, and the work that has been produced by each community to combat bias and unfairness in their methods. Table  2  provides an overview of the different areas that we focus upon in this survey. While this section is largely domain-specific, it can be useful to take a cross-domain view. Generally, methods that target biases in the algorithms fall under three categories:",
        "(1) Pre-processing. Pre-processing techniques try to transform the data so that the underlying discrimination is removed  [43] . If the algorithm is allowed to modify the training data, then pre-processing can be used  [11] . (2) In-processing. In-processing techniques try to modify and change state-of-the-art learning algorithms in order to remove discrimination during the model training process  [43] . If it is allowed to change the learning procedure for a machine learning model, then in-processing can be used during the training of a model-either by incorporating changes into the objective function or imposing a constraint  [11, 14] . (3) Post-processing. Post-processing is performed after training by accessing a holdout set which was not involved during the training of the model  [43] . If the algorithm can only treat the learned model as a black box without any ability to modify the training data or learning algorithm, then only post-processing can be used in which the labels assigned by the black-box model initially get reassigned based on a function during the post-processing phase  [11, 14] .",
        "Examples of some existing work and their categorization into these types is shown in Table  4 . These methods are not just limited to general machine learning techniques, but because of AI's popularity, they have expanded to different domains such as natural language processing and deep learning. From learning fair representations  [42, 97, 112]  to learning fair word embeddings  [20, 58, 169] , debiasing methods have been proposed in different AI applications and domains. Most of these methods try to avoid unethical interference of sensitive or protected attributes into the decision-making process, while others target exclusion bias by trying to include users from sensitive groups. In addition, some works try to satisfy one or more of the fairness notions in their methods, such as disparate learning processes (DLPs) which try to satisfy notions of treatment disparity and impact disparity by allowing the protected attributes during the training phase but avoiding them during prediction time  [94] . A list of protected or sensitive attributes is provided in Table  3 . They point out what attributes should not affect the outcome of the decision in housing loan or credit card decision-making  [30]  according to the law. Some of the existing work tries to treat sensitive attributes as noise to disregard their effect on decision-making, while some causal methods use causal graphs, and disregard some paths in the causal graph that result in sensitive attributes affecting the outcome of the decision. Different bias-mitigating methods and techniques are discussed below for different domains-each targeting a different problem in different areas of machine learning in detail. This can expand the horizon of the reader on where and how bias can affect the system and try to help researchers carefully look at various new problems concerning potential places where discrimination and bias can affect the outcome of a system."
      ],
      "Conclusion": [
        "In this survey we introduced problems that can adversely affect AI systems in terms of bias and unfairness. The issues were viewed primarily from two dimensions: data and algorithms. We illustrated problems that demonstrate why fairness is an important issue. We further showed examples of the potential real-world harm that unfairness can have on society-such as applications in judicial systems, face recognition, and promoting algorithms. We then went over the definitions of fairness and bias that have been proposed by researchers. To further stimulate the interest of readers, we provided some of the work done in different areas in terms of addressing the biases that may affect AI systems and different methods and domains in AI, such as general machine learning, deep learning and natural language processing. We then further subdivided the fields into a more fine-grained analysis of each subdomain and the work being done to address fairness constraints in each. The hope is to expand the horizons of the readers to think deeply while working on a system or a method to ensure that it has a low likelihood of causing potential harm or bias toward a particular group.",
        "With the expansion of AI use in our world, it is important that researchers take this issue seriously and expand their knowledge in this field. In this survey we categorized and created a taxonomy of what has been done so far to address different issues in different domains regarding the fairness issue.",
        "Other possible future work and directions can be taken to address the existing problems and biases in AI that we discussed in the previous sections."
      ]
    }
  },
  {
    "paperId": "f0dcc9aa31dc9b31b836bcac1b140c8c94a2982d",
    "title": "Membership Inference Attacks Against Machine Learning Models",
    "sections": {
      "Introduction": [
        "Machine learning is the foundation of popular Internet services such as image and speech recognition and natural language translation. Many companies also use machine learning internally, to improve marketing and advertising, recommend products and services to users, or better understand the data generated by their operations. In all of these scenarios, activities of individual users-their purchases and preferences, health data, online and offline transactions, photos they take, commands they speak into their mobile phones, locations they travel to-are used as the training data.",
        "Internet giants such as Google and Amazon are already offering \"machine learning as a service.\" Any customer in possession of a dataset and a data classification task can upload this dataset to the service and pay it to construct a model. The service then makes the model available to the customer, typically as a black-box API. For example, a mobile-app maker can use such a service to analyze users' activities and query the resulting model inside the app to promote in-app purchases to users when they are most likely to respond. Some machinelearning services also let data owners expose their models to external users for querying or even sell them.",
        "Our contributions. We focus on the fundamental question known as membership inference: given a machine learning model and a record, determine whether this record was used as * This research was performed while the author was at Cornell Tech. part of the model's training dataset or not. We investigate this question in the most difficult setting, where the adversary's access to the model is limited to black-box queries that return the model's output on a given input. In summary, we quantify membership information leakage through the prediction outputs of machine learning models.",
        "To answer the membership inference question, we turn machine learning against itself and train an attack model whose purpose is to distinguish the target model's behavior on the training inputs from its behavior on the inputs that it did not encounter during training. In other words, we turn the membership inference problem into a classification problem.",
        "Attacking black-box models such as those built by commercial \"machine learning as a service\" providers requires more sophistication than attacking white-box models whose structure and parameters are known to the adversary. To construct our attack models, we invented a shadow training technique. First, we create multiple \"shadow models\" that imitate the behavior of the target model, but for which we know the training datasets and thus the ground truth about membership in these datasets. We then train the attack model on the labeled inputs and outputs of the shadow models.",
        "We developed several effective methods to generate training data for the shadow models. The first method uses black-box access to the target model to synthesize this data. The second method uses statistics about the population from which the target's training dataset was drawn. The third method assumes that the adversary has access to a potentially noisy version of the target's training dataset. The first method does not assume any prior knowledge about the distribution of the target model's training data, while the second and third methods allow the attacker to query the target model only once before inferring whether a given record was in its training dataset.",
        "Our inference techniques are generic and not based on any particular dataset or model type. We evaluate them against neural networks, as well as black-box models trained using Amazon ML and Google Prediction API. All of our experiments on Amazon's and Google's platforms were done without knowing the learning algorithms used by these services, nor the architecture of the resulting models, since Amazon and Google don't reveal this information to the customers. For our evaluation, we use realistic classification tasks and standard model-training procedures on concrete datasets of images, retail purchases, location traces, and hospital inpatient stays. In addition to demonstrating that membership inference attacks are successful, we quantify how their success relates to the classification tasks and the standard metrics of overfitting.",
        "Inferring information about the model's training dataset should not be confused with techniques such as model inversion that use a model's output on a hidden input to infer something about this input  [17]  or to extract features that characterize one of the model's classes  [16] . As explained in  [27]  and Section IX, model inversion does not produce an actual member of the model's training dataset, nor, given a record, does it infer whether this record was in the training dataset. By contrast, the membership inference problem we study in this paper is essentially the same as the well-known problem of identifying the presence of an individual's data in a mixed pool given some statistics about the pool  [3] ,  [15] ,  [21] ,  [29] . In our case, however, the goal is to infer membership given a black-box API to a model of unknown structure, as opposed to explicit statistics.",
        "Our experimental results show that models created using machine-learning-as-a-service platforms can leak a lot of information about their training datasets. For multi-class classification models trained on 10,000-record retail transaction datasets using Google's and Amazon's services in default configurations, our membership inference achieves median accuracy of 94% and 74%, respectively. Even if we make no prior assumptions about the distribution of the target model's training data and use fully synthetic data for our shadow models, the accuracy of membership inference against Google-trained models is 90%. Our results for the Texas hospital discharge dataset (over 70% accuracy) indicate that membership inference can present a risk to health-care datasets if these datasets are used to train machine learning models and access to the resulting models is open to the public. Membership in such datasets is highly sensitive.",
        "We discuss the root causes that make these attacks possible and quantitatively compare mitigation strategies such as limiting the model's predictions to top k classes, decreasing the precision of the prediction vector, increasing its entropy, or using regularization while training the model.",
        "In summary, this paper demonstrates and quantifies the problem of machine learning models leaking information about their training datasets. To create our attack models, we developed a new shadow learning technique that works with minimal knowledge about the target model and its training dataset. Finally, we quantify how the leakage of membership information is related to model overfitting.",
        "Machine learning algorithms help us better understand and analyze complex data. When the model is created using unsupervised training, the objective is to extract useful features from the unlabeled data and build a model that explains its hidden structure. When the model is created using supervised training, which is the focus of this paper, the training records (as inputs of the model) are assigned labels or scores (as outputs of the model). The goal is to learn the relationship between the data and the labels and construct a model that can generalize to data records beyond the training set  [19] . Modeltraining algorithms aim to minimize the model's prediction error on the training dataset and thus may overfit to this dataset, producing models that perform better on the training inputs than on the inputs drawn from the same population but not used during the training. Many regularization techniques have been proposed to prevent models from becoming overfitted to their training datasets while minimizing their prediction error  [19] .",
        "Supervised training is often used for classification and other prediction tasks. For example, a retailer may train a model that predicts a customer's shopping style in order to offer her suitable incentives, while a medical researcher may train a model to predict which treatment is most likely to succeed given a patient's clinical symptoms or genetic makeup.",
        "Machine learning as a service. Major Internet companies now offer machine learning as a service on their cloud platforms. Examples include Google Prediction API,  1  Amazon Machine Learning (Amazon ML),  2  Microsoft Azure Machine Learning (Azure ML),  3  and BigML.  4 These platforms provide simple APIs for uploading the data and for training and querying models, thus making machine learning technologies available to any customer. For example, a developer may create an app that gathers data from users, uploads it into the cloud platform to train a model (or update an existing model with new data), and then uses the model's predictions inside the app to improve its features or better interact with the users. Some platforms even envision data holders training a model and then sharing it with others through the platform's API for profit.  5 The details of the models and the training algorithms are hidden from the data owners. The type of the model may be chosen by the service adaptively, depending on the data and perhaps accuracy on validation subsets. Service providers do not warn customers about the consequences of overfitting and provide little or no control over regularization. For example, Google Prediction API hides all details, while Amazon ML provides only a very limited set of pre-defined options (L1-or L2-norm regularization). The models cannot be downloaded and are accessed only through the service's API. Service providers derive revenue mainly by charging customers for queries through this API. Therefore, we treat \"machine learning as a service\" as a black box. All inference attacks we demonstrate in this paper are performed entirely through the services' standard APIs."
      ],
      "Results": [
        "We first describe the datasets that we use for evaluation, followed by the description of the target models and our experimental setup. We then present the results of our membership inference attacks in several settings and study in detail how and why the attacks work against different datasets and machine learning platforms.",
        "A. Data CIFAR. CIFAR-10 and CIFAR-100 are benchmark datasets used to evaluate image recognition algorithms  [24] . CIFAR-10 is composed of 32×32 color images in 10 classes, with 6, 000 images per class. In total, there are 50, 000 training images and 10, 000 test images. CIFAR-100 has the same format as CIFAR-10, but it has 100 classes containing 600 images each. There are 500 training images and 100 testing images per class. We use different fractions of this dataset in our attack experiments to show the effect of the training dataset size on the accuracy of the attack.",
        "Purchases. Our purchase dataset is based on Kaggle's \"acquire valued shoppers\" challenge dataset that contains shopping histories for several thousand individuals.  6  The purpose of the challenge is to design accurate coupon promotion strategies. Each user record contains his or her transactions over a year. The transactions include many fields such as product name, store chain, quantity, and date of purchase.",
        "For our experiments, we derived a simplified purchase dataset (with 197, 324 records), where each record consists of 600 binary features. Each feature corresponds to a product and represents whether the user has purchased it or not. To design our classification tasks, we first cluster the records into multiple classes, each representing a different purchase style. In our experiments, we use 5 different classification tasks with a different number of classes {2, 10, 20, 50, 100}. The classification task is to predict the purchase style of a user given the 600-feature vector. We use 10, 000 randomly selected records from the purchase dataset to train the target model. The rest of the dataset contributes to the test set and (if necessary) the training sets of the shadow models.",
        "Locations. We created a location dataset from the publicly available set of mobile users' location \"check-ins\" in the Foursquare social network, restricted to the Bangkok area and collected from April 2012 to September 2013  [36] .  7  The check-in dataset contains 11, 592 users and 119, 744 locations, for a total of 1, 136, 481 check-ins. We filtered out users with fewer than 25 check-ins and venues with fewer than 100 visits, which left us with 5, 010 user profiles. For each location venue, we have the geographical position as well as its location type (e.g., Indian restaurant, fast food, etc.). The total number of location types is 128. We partition the Bangkok map into areas of size 0.5km × 0.5km, yielding 318 regions for which we have at least one user check-in.",
        "Each record in the resulting dataset has 446 binary features, representing whether the user visited a certain region or location type, i.e., the user's semantic and geographical profile. The classification task is similar to the purchase dataset. We cluster the location dataset into 30 classes, each representing a different geosocial type. The classification task is to predict the user's geosocial type given his or her record. We use 1, 600 randomly selected records to train the target model. The rest of the dataset contributes to the test set and (if necessary) the training sets of the shadow models.",
        "Texas hospital stays. This dataset is based on the Hospital Discharge Data public use files with information about inpatients stays in several health facilities,  8  released by the Texas Department of State Health Services from 2006 to 2009. Each record contains four main groups of attributes: the external causes of injury (e.g., suicide, drug misuse), the diagnosis (e.g., schizophrenia, illegal abortion), the procedures the patient underwent (e.g., surgery) and some generic information such as the gender, age, race, hospital id, and length of stay.",
        "Our classification task is to predict the patient's main procedure based on the attributes other than secondary procedures. We focus on the 100 most frequent procedures. The resulting dataset has 67, 330 records and 6, 170 binary features. We use 10, 000 randomly selected records to train the target model.",
        "Note that our experiments do not involve re-identification of known individuals and fully comply with the data use agreement for the original Public Use Data File.",
        "MNIST. This is a dataset of 70, 000 handwritten digits formatted as 32 × 32 images and normalized so that the digits are located at the center of the image.  9  We use 10, 000 randomly selected images to train the target model.",
        "To evaluate the effectiveness of different mitigation strategies, we implemented all of them in locally trained mod-els over which we have full control. The inference attack, however, still assumes only black-box access to the resulting models. The baseline model for these experiments is a neural network with one hidden layer with 256 units (for the purchase dataset) and 1,000 units (for the Texas hospital-stay dataset). We use Tanh as the activation function.",
        "Table  III  shows the results of our evaluation. It compares different mitigation strategies based on how they degrade the accuracy of our attack relative to the attack on a model that does not use any mitigation. The mitigation strategies that we implemented did not impose any cost on the target model's prediction accuracy, and in the case of regularization, the target model's prediction accuracy increased as expected. Note that more regularization (by increasing λ even further) would potentially result in a significant reduction of the target model's test accuracy, even if it foils membership inference. This is shown in the table for λ = 1e -2 on the purchase dataset, and for λ = 5e -3 on the Texas hospital stay dataset.",
        "Overall, our attack is robust against these mitigation strategies. Filtering out low-probability classes from the prediction vector and limiting the vector to the top 1 or 3 most likely classes does not foil the attack. Even restricting the prediction vector to a single label (most likely class), which is the absolute minimum a model must output to remain useful, is not enough to fully prevent membership inference. Our attack can still exploit the mislabeling behavior of the target model because members and non-members of the training dataset are mislabeled differently (assigned to different wrong classes). If the prediction vector contains probabilities in addition to the labels, the model leaks even more information that can be used for membership inference. Some of the mitigation methods are not suitable for machine-learning-as-service APIs used by general applications and services. Regularization, however, appears to be necessary and useful. As mentioned above, it (1) generalizes the model and improves its predictive power and (2) decreases the model's information leakage about its training dataset. However, regularization needs to be deployed carefully to avoid damaging the model's performance on the test datasets."
      ],
      "Methods": [
        "The training set and the test set of each target and shadow model are randomly selected from the respective datasets, have the same size, and are disjoint. There is no overlap between the datasets of the target model and those of the shadow models, but the datasets used for different shadow models can overlap with each other.",
        "We set the training set size to 10, 000 for the purchase dataset as well as the Texas hospital-stay dataset, Adult dataset and the MNIST dataset. We set it to 1, 200 for the location dataset. We vary the size of the training set for the CIFAR datasets, to measure the difference in the attack accuracy. For the CIFAR-10 dataset, we choose 2, 500; 5, 000; 10, 000; and 15, 000. For the CIFAR-100 dataset, we choose 4, 600; 10, 520; 19, 920; and 29, 540.",
        "The experiments on the CIFAR datasets were run locally, against our own models, so we can vary the model's configuration and measure the impact on the attack accuracy. The experiments on the other datasets (purchases with {2, 10, 20, 50, 100} classes, Texas hospital stays, locations, Adult, and MNIST) were run against models trained using either Google or Amazon services, where we have no visibility into their choice of the model type and structure and little control over the training process (see Section VI-B).",
        "For the purchase dataset, we built target models on all platforms (Google, Amazon, local neural networks) employing the same training dataset, thus enabling us to compare the leakage from different models. We used similar training architectures for the attack models across different platforms: either a fully connected neural network with one hidden layer of size 64 with ReLU (rectifier linear units) activation functions and a SoftMax layer, or a Google-trained black-box model.",
        "We set the number of shadow models to 100 for the CIFAR datasets, 20 for the purchase dataset, 10 for the Texas hospitalstay dataset, 60 for the location dataset, 50 for the MNIST dataset, and 20 for the Adult dataset. Increasing the number of shadow models would increase the accuracy of the attack but also its cost."
      ],
      "Related Work": [
        "Attacks on statistical and machine learning models. In  [2] , knowledge of the parameters of SVM and HMM models is used to infer general statistical information about the training dataset, for example, whether records of a particular race were used during training. By contrast, our inference attacks work in a black-box setting, without any knowledge of the model's parameters, and infer information about specific records in the training dataset, as opposed to general statistics.",
        "Homer et al.  [21]  developed a technique, which was further studied in  [3] ,  [15] , for inferring the presence of a particular genome in a dataset, based on comparing the published statistics about this dataset (in particular, minor allele frequencies) to the distribution of these statistics in the general population. By contrast, our inference attacks target trained machine learning models, not explicit statistics. Other attacks on machine learning include  [7] , where the adversary exploits changes in the outputs of a collaborative recommender system to infer inputs that caused these changes. These attacks exploit temporal behavior specific to the recommender systems based on collaborative filtering.  [16] ,  [17]  uses the output of a model applied to a hidden input to infer certain features of this input. See  [27]  for a detailed analysis of this attack and an explanation of why it does not necessarily entail a privacy breach. For example, in the specific case of pharmacogenetics analyzed in  [17] , the model captures the correlation between the patient's genotype and the dosage of a certain medicine. This correlation is a valid scientific fact that holds for all patients, regardless of whether they were included in the model's training dataset or not. It is not possible to prevent disclosure due to population statistics  [14] ."
      ],
      "Conclusion": [
        "We have designed, implemented, and evaluated the first membership inference attack against machine learning models, notably black-box models trained in the cloud using Google Prediction API and Amazon ML. Our attack is a general, quantitative approach to understanding how machine learning models leak information about their training datasets. When choosing the type of the model to train or a machine learning service to use, our attack can be used as one of the selection metrics.",
        "Our key technical innovation is the shadow training technique that trains an attack model to distinguish the target model's outputs on members versus non-members of its training dataset. We demonstrate that shadow models used in this attack can be effectively created using synthetic or noisy data. In the case of synthetic data generated from the target model itself, the attack does not require any prior knowledge about the distribution of the target model's training data.",
        "Membership in hospital-stay and other health-care datasets is sensitive from the privacy perspective. Therefore, our results have substantial practical privacy implications."
      ]
    }
  },
  {
    "paperId": "6b0b93396792958c5e58da1933b46a4f565e82c6",
    "title": "Data Mining And Knowledge Discovery Handbook",
    "sections": {}
  },
  {
    "paperId": "73feb5cfe491342d52d47e8817d113c072067306",
    "title": "The Information Retrieval Experiment Platform",
    "sections": {
      "Introduction": [
        "Research and development in information retrieval (IR) has been predominantly experimental. In its early days in the 1960s, the IR community saw the need to develop and validate experimental procedures, giving rise to the Cranfield paradigm  [28] , which became the de facto standard for shared tasks hosted at TREC  [92]  and beyond. Organizers of typical shared IR tasks provide a task description, a document corpus, and topics. Participants implement retrieval approaches for the task and run them on each topic to produce document rankings (a so-called \"run\"). The rankings are then usually submitted as files to the organizers who pool all runs, gather (reusable) relevance judgments for the pools, and calculate the evaluation scores  [91] . Finally, the participants describe their methodology and findings in a published \"notebook\" paper. This division of labor allowed the community to scale up collaborative laboratory experiments, especially at a time of limited bandwidths for data exchange, since run files occupy only a few kilobytes. With many research laboratories working independently on the same task, the community draws on the \"wisdom of crowds\" while ensuring rigorous comparative evaluation.",
        "Despite the lasting success, this way of organizing shared tasks also has shortcomings. First, as with many other disciplines in computer science and beyond, the retrieval approach of a run described in a notebook paper might not be reproducible. There are well-documented cases where reproductions failed, despite putting much effort into it, even for approaches with diligently archived code repositories  [1, 65] . Second, run submissions require that participants have access to the test topics, which has severe implications  [45] , such as informing (biasing) the research hypothesis or retrieval approach, unless researchers make a point of not looking at the topics, ever, during development. Third, it cannot be ruled out that current or future large language models have been trained, by mistake or deliberately, on publicly available test data, or that a usage warning stating not to use the data for training would go unnoticed. 1 In any case, the current best practices for shared tasks do not enforce \"blinded experimentation\" 2 with sufficient rigor, compared to other empirical disciplines.",
        "To address all of these shortcomings, we have developed the IR Experiment Platform (TIREx; cf. Figure  1  for an overview). Available as open source, 3 a key feature of TIREx is the full integration 1 Some form of leakage from MS MARCO  [73]  to the Flan-T5 prompting model  [20]  has already been observed: twitter.com/UnderdogGeek/status/1630983277363228672, twitter.com/macavaney/status/1649779164625481733. 2   of tools for working with IR data (ir_datasets  [68] ), for executing retrieval pipelines (PyTerrier  [69] ), and for evaluating IR systems (ir_measures  [66] ) with the TIRA Integrated Research Architecture  [43] , a continuous integration service for reproducible shared tasks and experiments. TIREx is designed to for reproducibility through software submissions while keeping an experimenter's or task organizer's workload comparable to run file submissions.",
        "On our Betaweb and Gammaweb clusters, 4 we have deployed an instance of TIREx that is open for software submissions and experiments. A substantial efficiency boost comes from integrating GPU cores and result caching into the platform to accelerate neural IR approaches. As a proof of concept, we conducted a largescale evaluation of 50 \"standard\" retrieval approaches on 32 shared retrieval tasks (based on 15 corpora with a total of 1.9 billion documents). This experiment consists of 1,600 runs and was started by just clicking a button. It finished unattended in less than a week.",
        "We review ad hoc retrieval experiments in evaluation campaigns, common problems and pitfalls in IR experiments, best practices for leaderboards, existing reproducibility initiatives, and tools to support reproducibility. Insights from all these areas have influenced our implementation decisions for TIREx.",
        "Ad hoc Retrieval Experiments in Evaluation Campaigns. Today's shared task-style experiments for ad hoc retrieval evolved from the Cranfield experiments  [92] . In the 1960s, the Cranfield experiments  [28, 29]  were conducted on a corpus of 1,400 documents with complete relevance judgments for 225 topics. Since corpus sizes grew substantially, complete judgments became infeasible almost immediately thereafter  [92] . The current practice at shared tasks in IR thus is to only assess the relevance of per-topic pools of the submitted systems' top-ranked documents  [92] . Subsequent evaluations on the same corpus usually are based on the assumption that the pools are \"essentially complete\", i.e., unjudged documents 4 webis.de/facilities.html#hardware that were not in the pool are non-relevant  [92] . Although this completeness assumption is reasonable for tasks with a diverse set of submitted runs pooled at high depth  [97] , recent observations suggest that scenarios with many relevant documents per query (e.g., corpora with many duplicates  [94] ) or with topics representing broad information needs  [86]  are rather problematic. Especially for shared tasks that do not attract diverse submissions, TIREx can help to produce a more diverse judgment pool, as a wide range of different baseline retrieval systems is directly available and can be applied to any imported retrieval task.",
        "Common Problems and Pitfalls in IR Experiments. Even though the current discussion about how to conduct IR experiments  [44, 83, 104]  includes some controversial points (e.g., whether MRR should be abandoned  [44]  or not  [72, 83] ), there is still a consensus in the IR community on many characteristics of \"bad\" or \"good\" experiments. For instance, it is rather undisputed that retrieval studies should be internally valid (conclusions must be supported by the data) and externally valid (repeating an experiment on different but similar data should yield similar observations)  [46] . Still, external validity of IR experiments remains an open problem  [45] . TIREx can help to further improve both: the internal validity via archiving all experiments and results on some corpus (e.g., to accurately correct for multiple hypothesis tests), and the external validity via simplifying to run a submitted software on different data.",
        "Thakur et al.  [86]  attempted to address the external validity problem by combining diverse retrieval corpora in the BEIR benchmark for en masse evaluation. However, in practice, running an approach on all corpora in BEIR requires some effort, so that many studies still only report results for a selected subset (e.g.,  [12, 41, 47] )often even without clearly justifying the selection. In contrast, a software in TIREx can rather easily be evaluated against many if not all corpora so that analyzing improvements and limitations of an approach on diverse data is not much effort.",
        "An often criticized practice is that many IR studies compare a new approach against weak or \"wrong\" baselines (i.e., not the best or most reasonable previous approaches). Any improvements claimed in such studies are not really meaningful  [3, 62] . One reason for choosing a wrong baseline could be that neither the researchers nor the reviewers are actually aware of what previous approaches exist for a specific corpus since results are often scattered across multiple publications  [62] . Centralized leaderboards that directly show the effectiveness of diverse approaches for a wide range of tasks would address this problem, but multiple efforts have failed so far  [62] . In TIREx, we include many popular corpora and standard retrieval approaches right from the start so that the TIREx leaderboards can initially gain traction. The more shared tasks (but also researchers) employ TIREx for software submissions, the broader TIREx' coverage will get over time.",
        "Maintaining Ongoing Leaderboards. Inspired by the observation that many IR studies do not compare a new approach against reasonable baselines (e.g., the most effective TREC runs)  [3] , Armstrong et al.  [2]  released EvaluateIR, a public leaderboard accepting run file submissions. Although the concept was highly valuable for the community in helping researchers and reviewers alike to select appropriate baselines, \"EvaluateIR never gained traction, and a number of similar efforts following it have also floundered\"  [62] .",
        "While there is still no centralized general leaderboard for IR, certain task-specific leaderboards are quite popular. For instance, the leaderboard of the recent MIRACL Challenge  [103]  received 25 submissions within one week, and the MS MARCO leaderboard  [63]  has been popular for years. Maintaining such long-running leaderboards comes with some caveats, as they are conceptually turnbased games where every leaderboard submission might leak information from the test set  [63] . Lin et al.  [63]  propose best practices, inspired by previous problems of the Netflix prize.  5  Most importantly, Lin et al. note that, while submissions to a leaderboard are open, the retrieval results should not be public, nor should system descriptions or implementations, as this would potentially leak information from the test set and foster \"uninteresting\" approaches like ensembles of all the top submissions. With TIREx and its blind evaluation, organizers can choose to blind all submissions as long as they need to, with the ability to unblind approaches and submissions as they see fit, so that TIREx supports the best practices recommended by Lin et al.  [63] .",
        "Reproducibility Initiatives in IR. Reproducibility is a major challenge in research. For instance, a survey among 1,576 researchers revealed that more than 50% failed at least once to reproduce their own experiments  [5] . The IR community makes substantial efforts to foster reproducibility. There are, for instance, dedicated reproducibility tracks at conferences  6  and dedicated reproducibility initiatives like OSIRRC  [1, 21]  or CENTRE  [39, 40, 84, 85] . OSIRRC aims to produce archived versions of retrieval systems that are replicable, while CENTRE runs replicability and reproducibility challenges across IR evaluation campaigns. Lin and Zhang  [65]  looked at all the artifacts produced in the OSIRRC 2015 challenge  [1]  to verify which results are still replicable four years after their creation. Out of the seven systems that participated in the challenge, only the results of Terrier  [75]  were fully reproducible out of the box, while two other systems could still be fixed by manual adjustments to the code. The main reasons for failure were that external dependencies could not be loaded anymore, or that platform dependencies changed (i.e., the operating system with its packages). To mitigate the problem of changing platform dependencies, the follow-up iteration of OSIRRC  [21]  focused on Docker images that had to implement a strict specification (enforced by the companion tool \"jig\") that triggered the indexing and subsequent retrieval via Docker hooks. Even though 17 systems have been dockerized to follow the jig specification, the concept has not gained traction. By centering TIREx around shared tasks in the beginning, we hope that we can kick off and maintain the attention of the community. Furthermore, we believe that there are many retrieval scenarios that can not be encapsulated into the two-step index-then-retrieve pipeline that jig imposes (e.g., explicit relevance feedback). We thus minimize the TIREx requirements: just Docker images in which commands are executed without Internet access on read-only mounted data.",
        "Tooling for Reproducibility. Many tools have been developed to support shared tasks by reducing the workload of organizers and participants while increasing the reproducibility  [18, 43, 54, 56, 87, 88, 100] . For instance, as documenting the metadata of experiments improves reproducibility  [61] , ir_metadata  [17]  simplifies the documentation of IR experiments according to the PRIMAD model  [38]  (platform, research goal, implementation, method, actor, data). There are also platforms that support organizing and running shared tasks, among which four are still active: CodaLab, EvalAI, STELLA, and TIRA.  7  They implement the so-called evaluationas-a-service paradigm in the form of cloud-based web services for evaluations  [55] . Of these four systems, STELLA and TIRA are hosted within universities, while CodaLab and EvalAI use Microsoft Azure and Amazon S3, respectively. We use TIRA for TIREx as it supports blinded experimentation and as it is based on (private) git repositories hosted on GitLab or GitHub to versionize shared tasks and to distribute the workloads via runners connected to the corresponding repositories. The computation can thus be done in the cloud but also on private machines. We substantially extend large parts of TIRA as part of TIREx so that it supports the current IR workflows like chaining multiple retrieval stages."
      ],
      "Results": [
        "As illustrated in Figure  1 , TIREx facilitates the entire process of conducting retrieval experiments. It allows shared task organizers and individual experimenters to import data and utilize any pre-existing retrieval software submitted to TIREx as baselines. Following that, submissions of new retrieval approaches for evaluation can be made as software submissions or, if enabled, also as run submissions. Any submission can be accompanied by descriptive annotations and metadata; for instance, run submissions can be grouped to denote that they were generated by the same retrieval approach for multiple retrieval tasks. By providing relevance judgments, organizers or experimenters can directly evaluate all available runs.",
        "To incorporate a new corpus and topics into TIREx, they can be easily added to ir_datasets, utilizing a private branch if the data is sensitive. This data can then be imported by TIRA through a Docker image with a matching ir_datasets installation. Participants submit their software as Docker images as well. TIRA ensures their reproducibility and prevents test data leaks by executing them in a sandbox. Among other things, the sandbox disables Internet connectivity for the running software, which ensures that the software and its dependencies are fully installed and no data is sent to unauthorized third parties. Participants can provide additional data their software needs during execution by uploading it to TIRA. This is particularly useful for non-reproducible elements of a submission, such as manual query reformulations. TIREx also provides a \"starter implementation\" for five commonly used IR research frameworks, which participants can use as a development base. The simplest starter uses BM25 retrieval, which is implemented using a few lines of declarative PyTerrier code in a Jupyter notebook.  9 TIREx allows for software submissions to be executed on demand within a cloud-based execution environment, utilizing GitLab or GitHub CI/CD pipelines. In order to meet varying demand, experiment organizers can incorporate additional runners as necessary. TIREx maintains a comprehensive record of every artifact of a retrieval experiment within a specific git repository (Figure  1 , right), which can be exported and published. This \"archived shared task\" is entirely self-contained, enabling the independent re-execution of approaches with identical or differing data using PyTerrier pipelines. The availability of every software that generated a run as part of the repository makes it a key outcome and asset of an experiment. Consequently, TIREx facilitates \"always-on\" shared tasks for the IR community, along with an extensive variety of ablation studies.",
        "TIRA can automatically evaluate run files (created by retrieval software submissions or uploads) via an ir_measures evaluator. First, the evaluator performs a sanity check to test whether a run file can be parsed and warns of potential errors (e.g., score ties, NaN scores, empty result sets, unknown queries, scores contradicting the ranks, etc.). Then, if relevance judgments have been provided, the evaluator derives all specified measures averaged over all queries and per query (suitable for significance tests).",
        "To demonstrate the scalability of TIREx, we report about an experiment with 50 retrieval approaches on 32 retrieval tasks based on 15 corpora (1.9 billion documents). The resulting leaderboards are public and new submissions can be made at any time.  12  We also describe a repro_eval-based  [16]  case study on system preference reproducibility for different tasks.",
        "Table  3  shows the 15 corpora currently available in TIREx. Each has been used for 1 to 4 shared retrieval tasks, consists of 1,400 to 1 billion documents, and comes with the relevance judgments created during the respective shared tasks. Table  4  overviews the 50 retrieval approaches that we imported into TIREx from 5 retrieval frameworks: BEIR  [86] , ChatNoir  [7] , Pyserini  [64]  (our import was not ready during the experiments), PyGaggle  [64] , PyTerrier  [69]  (including two PyTerrier plugins for duoT5  [79]  and ColBERT  [59] ). From BEIR, we use 17 dense retrieval approaches (e.g., ANCE  [99] , DPR  [58] , and TAS-B  [53] ) by using the different SBERT  [80]  models available in BEIR. ChatNoir is an Elasticsearch-based BM25F search engine hosting all three ClueWeb corpora. It can be accessed from within TIRA to allow retrieval approaches on huge corpora with a REST-API that is kept consistent to ensure reproducibility. From Pyserini, we use the 4 lexical models available trough the SimpleSearcher interface. From PyGaggle, we use 8 variants of monoBERT  [74]  and monoT5  [79]  (including the state-of-the-art monoT5 with 3 billion parameters), and from PyTerrier, we use 20 lexical retrieval models (e.g., BM25, PL2, etc.). From the duoT5 plugin of PyTerrier, we use 3 variants based on different duoT5 models (including the state-of-the-art model with 3 billion parameters). For all retrieval approaches, we keep all parameters at their default values. Almost all approaches use the default_textbased fields that we added to ir_datasets, except for ChatNoir that is a full-rank software for the ClueWeb corpora and uses different fields (title, body, etc.). The lexical approaches in PyTerrier and the dense approaches in BEIR can be configured as full-rank software (i.e., a first component building an index and a second component retrieving from the index) or re-rank software-but are just counted as one approach in Table  4 . All duoT5 and PyGaggle approaches only work as re-rankers. For ColBERT, we only use the re-rank variant, as ColBERT indices become very large.",
        "In TIREx, all of these variants are available. To increase result comparability, however, our analysis fixes the first stage rankers to ChatNoir for the ClueWeb corpora and PyTerrier BM25 on all other corpora. Their respective results are then handed to the total of 50 available re-ranking approaches mentioned above. Altogether, 50 approaches are executed on all 32 tasks listed in Table  3 . We executed the lexical approaches using 1 CPU and 10 GB RAM, while all other approaches had additional access to a GeForce GTX 1080 GPU with 8 GB RAM. Some models fail on this GPU as 8 GB of RAM do not suffice: ColBERT and two SBERT models failed on a few tasks, while the 3 billion parameter monoT5 / duoT5 failed on all tasks. To handle these cases, we added two runners with access to an A100 GPU with 40 GB RAM to TIRA, which was sufficient. TIRA manages metadata about the resources used to produce a run, making hardware difference between evaluations transparent.",
        "Table  5  shows the aggregated evaluation results on 31 tasks (leaving out the ClueWeb22 as there are no judgments yet). We report the effectiveness as nDCG@10 (macro-averaged in case a corpus is associated with multiple tasks) for BM25, ColBERT, TAS-B, all three duoT5 variants, and monoT5 (in its default configuration with its default model) and the best, median, and worst approaches from the groups of 20 lexical, 17 bi-encoder, and 8 PyGaggle approaches. All deep learning models were trained on MS MARCO and thus substantially improve upon the lexical models on MS MARCO. However, on other corpora the deep learning models work in a zero-shot manner so that sometimes a lexical approach achieves the highest effectiveness (Args.me, ClueWeb09, and MEDLINE). Our results further show that BM25 is not always the best lexical ranker (e.g., on Args.me: 0.43 vs. 0.57). The effectiveness gap between the best and the worst model of a group can be substantial on some corpora (e.g., lexical models on Args.me: 0.14 vs. 0.57), while being negligible on others (e.g., lexical models on NFCorpus). The leaderboards of TIREx as aggregated in Table  5  allow to easily select competitive baselines for very different tasks-often much easier than before."
      ],
      "Discussion": [
        "As an example of a post-hoc analysis enabled by TIREx, we use repro_eval to analyze to which degree system preferences from the TREC Deep Learning 2019 task can be reproduced on other tasks. For each preference between approaches on TREC Deep Learning 2019 (e.g., monoT5 with an nDCG@10 of 0.71 compared to BM25's 0.48 induces a clear system preference), we set the approach with the lower effectiveness on TREC Deep Learning 2019 as the \"baseline\" in repro_eval and the other approach as the \"advanced system\". We study the reproducibility of the preferences on two dimensions  [15] : (1) the effect ratio of the reproduction, and (2) the delta relative improvement of the reproduction. The effect ratio measures to which degree the advanced system is still better than the baseline on the different task (1 indicates a perfect reproducibility, values between 0 and 1 indicate reproducibility with diminished improvements on the different task, and 0 indicates failed reproducibility), while the delta relative improvement measures the relative effectiveness difference of the advanced system to the baseline (0 indicates perfect reproducibility, values between -1 and 0 indicate an increased relative improvement of the advanced system, values between 0 and 1 indicate a smaller relative improvement, and 1 indicates failed reproducibility).",
        "Table  6  shows the results of the preference reproducibility analysis. We report the ratio of system preferences with a successful reproduction (i.e., effect ratio > 0) and the 25%, 50%, and 75% quantiles for the effect ratio and the relative delta improvement. We order the tasks by the percentage of successfully reproduced preferences and show the top-5 tasks and every fifth lower ranked task. Not that surprising, the reproducibility on the very similar TREC Deep Learning 2020 is very good (88.1%) but declines fast for other tasks (e.g., only 57.8% for the Web track 2003 on rank 15). Analyzing the quantiles yields similar observations (e.g., 50% of the system preferences have an almost perfect effect ratio of 0.90 or higher for TREC Deep Learning 2020, while the Web track 2003 on rank 15 has a median effect ratio of 0.04).",
        "Potential Impact of TIREx. We believe that TIREx can have a substantial conceptual impact as we see no alternative to blinded retrieval evaluations in the future (given the practice of training LLMs on basically all available ground truth for IR and NLP tasks  [20] ). Additionally, the platform eases the organization of reproducible IR experiments with software submissions. Shared task organizers can simply provide the well-documented open-source baselines from TIRA as starting points for the participants and can also use the baselines to ensure some more diverse judgment pools, especially for tasks that attract few participants. For shared tasks that Table  5 : Effectiveness scores (nDCG@10) on 14 corpora (31 tasks; ClueWeb22B excluded as no judgments yet) for selected approaches and the best, median, and worst of each group (scores macro-averaged for corpora with multiple associated tasks).  run multiple years on different data, the organizers can automatically re-run all approaches submitted to previous editions to track progress. TIREx combines leaderboards with immutable software, promoting provenance of results, and enabling researchers and reviewers to identify and locally reproduce good baselines. The submission platform TIRA proved robust after its complete redevelopment  [43] : two NLP tasks used TIRA at SemEval 2023  [42, 60]  for which 71 of the 171 registered teams created 647 runs with software submissions. Our initial retrieval experiments with TIREx produced another 1,600 runs on standard corpora in less than a week, showing the platform to be robust and to have the potential for scaling up. When adopted by shared tasks and in individual IR experiments, TIREx can become a (federated) hub for IR resources and serve as a reference for reviewers. If a sufficient number of retrieval approaches, corpora, and supplementary data (e.g., manual query reformulations) are available through TIREx, integrating new resources gives direct access to an entire ecosystem, furthering the nascent standardization of IR experiments.",
        "Future Extensions of TIREx. Interesting directions for future development besides including further IR frameworks and libraries are integrations of TIREx with the IR Anthology  [78]  and with Diff-IR  [57] . An integration with the IR Anthology would enable links between entries in the TIREx leaderboards and the corresponding publications in the IR Anthology to provide more detailed information on an approach but also to \"extend\" a publication by adding results on different corpora than originally used and putting an approach in a broader context with other approaches run on the same data. An integration with DiffIR would enable the rendering of runs as search engine result pages to easily contrast the quantitative evaluations already possible via the integrated ir_measures with more qualitative evaluations of ranking differences or even (basic) user studies."
      ],
      "Conclusion": [
        "With TIREx-The IR Experiment Platform-we aim to substantially ease conducting (blinded) IR experiments and organizing \"always-on\" reproducible shared tasks on the basis of software submissions. TIREx integrates ir_datasets, ir_measures, and PyTerrier with TIRA. Retrieval workflows can be executed on-demand via cloud-native orchestration, reducing the effort for reproducing IR experiments since software submitted to TIREx can be reexecuted in post-hoc experiments. The platform has no lock-in effect, as archived experiments are fully self-contained, work standalone, and are easily exported. By keeping test data private, TIREx promotes further standardization and provenance of IR experiments following the example of, e.g., medicine, where blinded experiments are the norm. TIREx is open to the IR community and ready to include more corpora, shared tasks, and retrieval approaches."
      ]
    }
  },
  {
    "paperId": "fd1cf28a2b8caf2fe29af5e7fa9191cecfedf84d",
    "title": "RT-1: Robotics Transformer for Real-World Control at Scale",
    "sections": {
      "Introduction": [
        "End-to-end robotic learning, with either imitation or reinforcement, typically involves collecting task-specific data in either single-task  (Kalashnikov et al., 2018; Zhang et al., 2018)  or multitask  (Kalashnikov et al., 2021b; Jang et al., 2021)  settings that are narrowly tailored to the tasks that the robot should perform. This workflow mirrors the classic approach to supervised learning in other domains, such as computer vision and NLP, where task-specific datasets would be collected, labeled, and deployed to solve individual tasks, with little interplay between the tasks themselves. Recent years have seen a transformation in vision, NLP, and other domains, away from siloed, smallscale datasets and models and towards large, general models pre-trained on broad, large datasets. The keys to the success of such models lie with open-ended task-agnostic training, combined with high-capacity architectures that can absorb all of the knowledge present in large-scale datasets. If a model can \"sponge up\" experience to learn general patterns in language or perception, then it can bring them to bear on individual tasks more efficiently. While removing the need for large taskspecific datasets is appealing generally in supervised learning, it is even more critical in robotics, where datasets might require engineering-heavy autonomous operation or expensive human demonstrations. We therefore ask: can we train a single, capable, large multi-task backbone model on data consisting of a wide variety of robotic tasks? And does such a model enjoy the benefits observed in other domains, exhibiting zero-shot generalization to new tasks, environments, and objects? Building such models in robotics is not easy. Although recent years have seen several large multitask robot policies proposed in the literature  (Reed et al., 2022; Jang et al., 2021) , such models often have limited breadth of real-world tasks, as with Gato  (Reed et al., 2022) , or focus on training tasks rather than generalization to new tasks, as with recent instruction following methods  (Shridhar et al., 2021; 2022) , or attain comparatively lower performance on new tasks  (Jang et al., 2021) . 1 Authors listed in alphabetical order. Contributions in Appendix A. Corresponding emails: {keerthanapg,kanishkarao,karolhausman}@google.com. (a) RT-1 takes images and natural language instructions and outputs discretized base and arm actions. Despite its size (35M parameters), it does this at 3 Hz, due to its efficient yet high-capacity architecture: a FiLM  (Perez et al., 2018)  conditioned EfficientNet  (Tan & Le, 2019) , a TokenLearner  (Ryoo et al., 2021) , and a Transformer  (Vaswani et al., 2017) .",
        "(b) RT-1's large-scale, real-world training (130k demonstrations) and evaluation (3000 real-world trials) show impressive generalization, robustness, and ability to learn from diverse data.",
        "Figure  1 : A high-level overview of RT-1's architecture, dataset, and evaluation.",
        "The two main challenges lie in assembling the right dataset and designing the right model. While data collection and curation is often the \"unsung hero\" of many large-scale machine learning projects  (Radford et al., 2021; Ramesh et al., 2021) , this is especially true in robotics, where datasets are often robot-specific and gathered manually  (Dasari et al., 2019; Ebert et al., 2021) . As we will show in our evaluations, good generalization requires datasets that combine both scale and breadth, covering a variety of tasks and settings. At the same time, the tasks in the dataset should be sufficiently well-connected to enable generalization, such that the model can discover the patterns between structural similar tasks and perform new tasks that combine those patterns in novel ways. We utilize a dataset that we gathered over the course of 17 months with a fleet of 13 robots, containing ∼130k episodes and over 700 tasks, and we ablate various aspects of this dataset in our evaluation.",
        "The second challenge lies in the design of the model itself. Effective robotic multi-task learning requires a high capacity model, and Transformer  (Vaswani et al., 2017)  models excel in this regard, particularly when it is necessary to learn many tasks conditioned, as in our case, on language instructions. However, robotic controllers must also be efficient enough to run in real time, which presents a major challenge for Transformers in particular. We propose a novel architecture that we call RT-1 (Robotics Transformer 1), which by encoding high-dimensional inputs and outputs, including camera images, instructions and motor commands into compact token representations to be used by the Transformer, allows for efficient inference at runtime to make real-time control feasible.",
        "Our contribution is the RT-1 model and experiments with this model on a large and broad dataset of real-world robotic tasks. Our experiments not only demonstrate that RT-1 can exhibit significantly improved generalization and robustness compared to prior techniques, but also evaluate and ablate many design choices in both the model and in the composition of the training set. Our results show that RT-1 can perform over 700 training instructions at 97% success rate, and can generalize to new tasks, distractors, and backgrounds 25%, 36% and 18% better than the next best baseline, respectively. This level of performance allows us to execute very long-horizon tasks in the SayCan  (Ahn et al., 2022)  framework, with as many as 50 stages. We further show that RT-1 can incorporate data from simulation or even other robot types, retaining performance on the original tasks and improving generalization to new scenarios. A short overview of RT-1 capabilities is presented in Fig.  1b   2  ."
      ],
      "Related Work": [
        "A number of recent works have proposed Transformer-based policies for robotic control. As in RT-1, several works use language commands processed with Transformers as a robust framework for specifying and generalizing to new tasks  (Zhang & Chai, 2021; Pashevich et al., 2021; Silva et al., 2021; Jang et al., 2021; Ahn et al., 2022; Nair et al., 2022) . Our work takes the application of Transformers a step further and treats the mapping of language and vision observations to robot actions as a sequence modelling problem, using a Transformer to learn this mapping. This idea is directly inspired by successes in game-playing  (Chen et al., 2021; Lee et al., 2022a)  as well as simulated robot navigation  (Fang et al., 2019 ), locomotion (Janner et al., 2021; Gupta et al., 2022) , and manipulation  (Jiang et al., 2022)  environments. We note that several of these works go beyond only text conditioning and use Transformers to also generalize across robot morphologies (e.g.,  Gupta et al. (2022) ) and other modalities for task specifications (e.g.,  Jang et al. (2021) ;  Jiang et al. (2022) ). These extensions are promising future directions for RT-1.",
        "Beyond Transformer-based policies, the focus of our work is on generalizable and robust real-world robotic manipulation at scale. Existing works on real-world Transformer-based robotic manipulation focus on efficiently learning tasks from a set of demonstrations per task  (Shridhar et al., 2022) .  Behavior Transformer (Shafiullah et al., 2022)  and Gato  (Reed et al., 2022)  advocate for training a single model on large-scale robotic and non-robotic datasets. However, these works are limited in their real-world robotic tasks; e.g., Gato learns effectively a single task (colored block stacking) without evaluating generalization to new tasks or a variety of real-world settings. On the technical side, our work examines how Transformer-based policies can be built so as to combine high capacity and generalization with the computational efficiency necessary for real-time control.",
        "While the use of high-capacity Transformer models to learn robotic control policies is a fairly recent innovation, robotics has a long history of multi-task and language-conditioned learning, and RT-1 builds on these foundations. A significant body of work deals with learning policies and predictive models for robotic grasping  (Saxena et al., 2006; Lenz et al., 2015; Pinto & Gupta, 2016; Gupta et al., 2018; Viereck et al., 2017) , with the aim of generalizing to new objects. Prior works have sought to address robotic language understanding through pipelined approaches that combine language parsing, vision, and robotic control  (MacMahon et al., 2006; Kollar et al., 2010; Tellex et al., 2011)  and with end-to-end approaches  (Mei et al., 2016; Stepputtis et al., 2020; Lynch & Sermanet, 2020; Ahn et al., 2022) . Multi-task robotic learning has also been approached from the perspective of learning to reach goals  (Chung et al., 2015; Raffin et al., 2019; Jurgenson et al., 2020; Huang et al., 2020) , as well as learning policies that can perform tasks in a discrete set or some other parameterized form  (Deisenroth et al., 2014; Devin et al., 2017; Fox et al., 2019; Kalashnikov et al., 2021a) . A number of prior works in robotics have also focused on collecting datasets containing demonstrations or trials that illustrate a variety of different tasks  (Sharma et al., 2018; Dasari et al., 2019; Yu et al., 2020; Singh et al., 2020; James et al., 2020) . Our work adds further evidence in support of the power of multi-task, language-conditioned robotic learning, presenting experimental results at a larger scale and with a greater variety of behaviors, objects, and scenes and proposing new architectures and design choices that enable robotic learning at a significantly larger scale."
      ],
      "Results": [
        "Our experiments seek to answer the following questions:",
        "1. Can an RT-1 learn to perform a large number of instructions, as well as to generalize in zero shot to new tasks, objects and environments? (Section 6.2) 2. Can we push the resulting model even further by incorporating heterogeneous data sources, such as simulated data or data from different robots? (Section 6.3) 3. How do various methods generalize to long-horizon robotic scenarios? (Section 6.4) 4. How do generalization metrics change with varying amounts of data quantity and data diversity? (Section 6.5) 5. What are the important and practical decisions in the design of the model and how do they affect performance and generalization? (Appendix Section D.4)",
        "Throughout this section we will compare to two baseline state of the art architectures, Gato  (Reed et al., 2022)  and BC-Z  (Jang et al., 2021) . Importantly both of these are trained on our data described in detail in Sec. 5.2 (which is an important part of our system) since the original models in these publications would not exhibit generalization properties required for our evaluation tasks. Gato is, similarly to RT-1, based on a Transformer architecture, but varies from RT-1 in multiple aspects. First, it computes image tokens without the notion of language and each image token embedding is computed separately for each image patch, as opposed to early language fusion and global image embedding in our model. Second, it does not use a pre-trained text embedding to encode the language string. It also does not include inference time considerations that are necessary for real robots as discussed in Sec. 5.1 such as TokenLearner and the removal of auto-regressive actions. In order to run Gato on real robots at a high enough frequency, we also limit the size of the model compared to the original publication, which was 1.2B parameters (resulting in on robot inference time of 1.9s), to be of similar size to RT-1 (37M parameters for Gato vs. 35M for RT-1). BC-Z is based on a ResNet architecture, and was used in SayCan  (Ahn et al., 2022) . BC-Z differs from RT-1 in that it is a feedforward model that does not use previous timesteps, and it uses continuous actions rather than discrete action tokens. In addition to the original BC-Z model size, we also compare our method to a larger version of BC-Z that has a similar number of parameters to RT-1 and refer to it as BC-Z XL. We study and analyze how each of these design decisions changes performance in Appendix Sections D.4 and D.5.",
        "We evaluate the success rate in experiments to measure performance on training instructions, generalization to unseen instructions, robustness to backgrounds and distractors, and performance in long-horizon scenarios, as detailed below. Throughout this section, we evaluate our approach and baselines with over 3000 real-world trials, making one of the largest scale evaluation of a robot learning system to-date.",
        "DATA SOURCES SUCH AS SIMULATION OR DATA FROM DIFFERENT ROBOTS?",
        "Next, we explore the limits of RT-1 for utilizing highly heterogeneous data. We demonstrate how RT-1 can incorporate and learn from vastly different data sources and improve from such data without sacrificing its original-tasks performance across the varied tasks inherent in this data. To this end, we conduct two experiments: (1) RT-1 trained and tested on both real data and simulation data and (2)  RT-1 trained across large datasets of different tasks, originally collected by different robots. More information on each is provided in Appendix D.2.",
        "Absorbing simulation data. Table  4  shows the ability of RT-1, and baselines, to absorb both real and simulation data. To test this, we take all of the real demonstration data but we also provide ad-  does not impact the performance on real objects, while significantly improving real performance on objects that were only introduced in simulation (+64%). It also improves real-world generalization on simulated objects used with skills seen only in the real world (+26%), e.g. \"move X to Y\" where X only appeared in simulated \"pick X\" task.",
        "ditional simulation data that includes objects that the robot has never seen in the real world. Specifically, we specify different generalization scenarios: for seen skills with real objects the training data has real data of that instruction (i.e., performance on seen tasks), for seen skills with sim objects the training data has sim data of that instruction (e.g. \"pick up a sim object\", which was present in sim), and for unseen skills with sim objects the training data has sim data of that object but there are no examples of the instruction describing the skill with that object either in sim or in real (e.g., \"move a sim object to apple\", even though the robot has only practiced in picking that sim object and not moving it near other objects). All evaluations are done in the real world but to limit the number of instructions evaluated, we focus on pick and move-to skills.",
        "We find in Table  4  that for RT-1, we do not lose performance adding simulation data compared to the Real Only dataset. We do however, see a significant increase in performance (from 23% to 87%) on objects and tasks seen only in simulation, to approximately the performance of the those in real, demonstrating an impressive degree of domain transfer. We also see a significant increase in performance on unseen instructions from 7% to 33%; impressive given the object in question has never been seen in real and the instruction never seen at all. Overall, we find that RT-1 is able to efficiently absorb new data, even from a very different domain.",
        "Absorbing data from different robots. To push the data absorption limits of RT-1, we conduct an additional set of experiments where we combine two data sources that originate from different robots: Kuka IIWA as well as the Everyday Robots mobile manipulators used in the experiments so far. The Kuka data contains all the successful examples collected in QT-Opt  (Kalashnikov et al., 2018) , which corresponds to 209k episodes, where the robot was indiscriminately grasping objects in a bin (see an example of a Kuka episode in Table . 5). To test whether RT-1 can effectively absorb these two very different datasets, which we refer to as the standard \"Classroom eval\", as well as the performance on the newly constructed tasks that reflect the bin-picking setup present in the Kuka data, which we refer to as the \"Bin-picking eval\" (see Fig.  6 ).",
        "We would like to emphasize the difficulty of this setting by noting the major differences between the datasets. Not only are the robots that collected the data different in appearance and action space, but also the environment they were deployed in has different appearance and dynamics. In addition the QT-Opt data presents a completely different action distribution -it was collected by an RL agent as opposed to human demonstrations present in our dataset.",
        "The results are presented in Table  5 . We observe that the model that mixes the RT-1 data and the Kuka data has only a minimal decrease in the original tasks' performance (i.e. Classroom eval), i.e. 2%. Even more importantly, in the Bin-picking eval, we observe that the model trained on multirobot data performs at 39% compared to the 22% of the model that was trained only on the RT-1 data. This is a 17% performance difference (almost 2x). Additionally, RT-1 trained on Kuka bin-picking data and evaluated on the bin-picking tasks with the Everyday Robots (EDR) robot achieves 0% performance, confirming that it is difficult to transfer a behavior from another robot morphology. However, mixing the data from both robots allows RT-1 to infer the correct actions of the EDR     (Kalashnikov et al., 2018)  in RT-1 minimally impacts the standard classroom evaluation performance and results in almost a 2x improvement in generalization to the Binpicking evaluation (that is similar to the setup in the Kuka data) on the Everyday Robots manipulator. This demonstrates an effective transfer across two different robot morphologies. robot even when faced with the states observed by Kuka robots. This is achieved without explicit demonstrations of bin-picking on EDR robot and by taking advantage of past experiences collected by Kuka robots. These results indicate that RT-1's absorption properties also include the ability to acquire new skills through observing other robots' experiences and present an exciting avenue of future work where we combine many more multi-robot datasets to enhance the robot capabilities.",
        "In Section 6.2, we study the zero-shot generalization capabilities of RT-1 to difficult scenarios not present in the training dataset. To fairly evaluate different ablations of RT-1 as well as baseline policies, we design standardized evaluation procedures that cover a range of incremental difficulty levels.",
        "Seen tasks. We evaluate on 744 tasks present in the training dataset. The breakdown between 12 skills is shown in Table  1 . For all \"Seen\" evaluations, we use the same classroom setting used for data collection as described in Section 5.2. For each policy, we report a single representative metric that takes a skill-weighted average across individual skill evaluations.",
        "Unseen tasks. We evaluate policy performance on 53 tasks that are held out during training. While the unseen instructions' specific combinations of skills and objects are not seen during training, other combinations of the same skills and objects are present in the training set. We evaluate these unseen tasks in the same environment and the same randomization procedure as the Seen tasks. A full list of these unseen tasks is shown in Table  8 .",
        "Distractor robustness. We test three tasks (\"pick coke can\", \"place coke can upright\", \"move coke can near green rice chip bag\") with incrementally more distractor objects added to the scene. The easy setting includes 0, 2, or 5 distractor objects. The medium setting includes 9 distractor objects, but the coke can is never obscured. The hard setting includes 9 distractor objects, but the scene is more crowded and the coke can is partially occluded. Both the medium are hard setting are more difficult than scenarios in the training dataset, which contained between 0 and 4 distractors.",
        "Examples of these difficulty settings and policy evaluation rollouts are shown in Figure  12 .",
        "Background robustness. We test six tasks (\"pick coke can\", \"move blue chip bag near orange\", \"knock redbull can over\", \"pick green jalapeno chip bag\", \"move sponge near brown chip bag\",\"place redbull can upright\") with incrementally more challenging backgrounds and counter textures. In the easy setting, we utilize the same background environments and counter textures as the training dataset. In the medium setting, we utilize the same background environment but add a patterned tablecloth to change the counter texture. In the hard setting, we utilize a brand new kitchen environment with a new countertop; this changes the counter texture, drawer material and color, and  Real + Sim Data Table  9 : Experimental results for incorporating simulation data in RT-1. Adding simulation data does not impact the performance on real objects, while significantly improving real performance on objects that were only introduced in simulation.",
        "Absorbing data from different robots. To push the data absorption limits of RT-1, we conduct an additional set of experiments where we combine two data sources that originate from different robots: Kuka IIWA as well as the Everyday Robots mobile manipulators used in the experiments so far. The Kuka data contains all the successful examples collected in QT-Opt  (Kalashnikov et al., 2018) , which corresponds to 209k episodes, where the robot was indiscriminately grasping objects in a bin (see an example of a Kuka episode in Table . 10). Our goal in this experiment is to analyze whether the performance on the RT-1 tasks drops when adding the additional data and, more importantly, whether we can observe any transfer from data collected by a different robot morphology.",
        "We would like to emphasize the difficulty of this setting by noting the major differences between the datasets. Not only are the robots that collected the data different in appearance and action space, but also the environment they were deployed in has different appearance and dynamics. In addition the QT-Opt data presents a completely different action distribution -it was collected by an RL agent as opposed to human demonstrations present in our dataset.",
        "To mix the Kuka data together with the RT-1 data, we first transform the original Kuka 4-DOF action space into the same action space as RT-1, namely we set the roll and pitch to 0, while keeping the yaw values that were present in the original Kuka data. In addition, we transform the binary gripper-close command into a continuous gripper-closedness command that is present in the RT-1 data. We also need text instructions corresponding to the task performed and since the Kuka data does not contain the name of the object that was grasped, we relabel all the data to the \"pick anything\" instruction. With these modifications, we mix both datasets with the 2:1 (RT-1 data : Kuka data) ratio and train RT-1 to obtain the final model.",
        "To test whether RT-1 can effectively absorb these two very different datasets, we evaluate the performance on the original RT-1 tasks (in this case, we also focus on \"pick\" and \"move to\" skills), which we refer to as the standard \"Classroom eval\", as well as the performance on the newly constructed tasks that reflect the bin-picking setup present in the Kuka data, which we refer to as the \"Bin-picking eval\". For the Bin-picking eval to be close to the original dataset, we put in the same looking bin for the objects as well as modify the robot to be similar to the Kuka manipulators by adding extra wires and coloring the gripper gray. For all of the evaluations we use the Everyday Robots robot with the picking commands and evaluate it based on 72 grasping trials.",
        "The results are presented in Table  10 . We observe that the model that mixes the RT-1 data and the Kuka data has only a minimal decrease in the original tasks' performance (i.e. Classroom eval), i.e. 2%. Even more importantly, in the Bin-picking eval, we observe that the model trained on multirobot data performs at 39% compared to the 22% of the model that was trained only on the RT-1 data. This is a 17% performance difference (almost 2x). Additionally, RT-1 trained on Kuka bin-picking data and evaluated on the bin-picking tasks with the Everyday Robots (EDR) robot achieves 0% performance, confirming that it is difficult to transfer a behavior from another robot morphology. However, mixing the data from both robots allows RT-1 to infer the correct actions of the EDR robot even when faced with the states observed by Kuka robots. This is achieved without explicit demonstrations of bin-picking on EDR robot and by taking advantage of past experiences collected by Kuka robots. These results indicate that RT-1's absorption properties also include the ability to    (Kalashnikov et al., 2018)  in RT-1 minimally impacts the standard classroom evaluation performance and results in almost a 2x improvement in generalization to the Bin-picking evaluation (that is similar to the setup in the Kuka data) on the Everyday Robots manipulator. This demonstrates an effective transfer across two different robot morphologies.",
        "acquire new skills through observing other robots' experiences and present an exciting avenue of future work where we combine many more multi-robot datasets to enhance the robot capabilities.",
        "In addition to short-horizon individual skill evaluations shown in previous sections, we also evaluate how RT-1 performs in a long-horizon realistic kitchen setting that chains multiple manipulation and navigation skills to accomplish natural language instructions within the SayCan framework  (Ahn et al., 2022) . A list of long-horizon instructions used for these evaluations is listed in Table  12 .",
        "The success rate of long-horizon tasks decreases exponentially with the length of the task, so high success rates in manipulation skills are particularly important. Furthermore, as mobile manipulation tasks require both navigation and manipulation, the policies ability to be robust to base position is crucial. Since SayCan combines many low-level instructions to perform high-level instructions, the number of possible high-level instructions increases combinatorially with instructions, so the skill-breadth of RT-1 can be fully seen.",
        "SayCan works by grounding language models in robotic affordances and it leverages few-shot prompting to break down a long horizon task expressed in natural language to a sequence of low level skills. An example of long horizon task would be \"Bring me two different sodas\", and one feasible plan would be \"1. find a coke, 2. pick up the coke, 3. bring it to you, 4. put down the coke, 5. find a pepsi, 6. pick up the pepsi, 7. bring it to you, 8. put down the pepsi, 9. done.\" To obtain the affordance function we use value functions trained with MT-OPT  (Kalashnikov et al., 2021a) . For a detailed description of SayCan algorithm please refer to  (Ahn et al., 2022) .",
        "Since the focus of this paper is acquisition of many generalizable skills, we focus our evaluation on one subset of tasks presented in  Ahn et al. (2022) . It is the long-horizon family of tasks, involving 15 instructions, each instruction requires an average of 9.6 steps to complete, and involves an average of 2.4 manipulation skills per instruction. A full list of the instructions can be found in Table  12 .",
        "We compare against 3 baselines. 1) SayCan with BC-Z, which uses SayCan planning algorithm with BC-Z as manipulation policy, 2) SayCan with Gato, which uses SayCan planning algorithm with Gato as manipulation policy, 3) Originally reported SayCan results, which use SayCan planning algorithm with BC-Z, but since it uses a slightly different prompt, the planning success rate is lower. We reimplemented 3) in 1) for a fair comparison.",
        "As shown in Table  11 , except for original SayCan, all methods get 87% as planning success rate, and RT-1 performs the best, with 67% execution success rate in Kitchen1. Kitchen2 constitutes a much more challenging generalization scene, since the Robot Classroom training scenes are modeled after Kitchen1 (see the pictures of the kitchens in Fig.  2 ). Due to this generalization difficulty, SayCan with Gato is not able to finish any long horizon task, and SayCan with BC-Z is able to achieve a success rate of 13%. The original SayCan paper did not evaluate performance in a new kitchen. Surprisingly, the manipulation performance does not see a visible drop from Kitchen1 to Kitchen2 Preprint for our method. In the supplementary video, we show that this enables us to operate unseen drawers in Kitchen2, and that we can use SayCan-RT1 to plan and execute ultra-long horizon tasks, with as many as 50 steps. To answer this question, we perform a set of ablations over different design decisions in RT-1.",
        "We aim to test a number of hypotheses that will help us disambiguate where the benefits of our method come from. Possible hypotheses about the source of improvement include: (i) the capacity and expressiveness of our model, which we verify by ablating the model size, trying other architectures (e.g., by removing the Transformer component); (ii) the particular action representation, which makes it easy to represent complex multi-modal action distributions, which we test by switching to continuous (normally distributed) actions, as well as by ablating the auto-regressive action representation; (iii) the ImageNet pre-trained initialization of the components, which we test by initializing the model's weights randomly; and (iv) access to the short history, which we test by excluding observation history. More concretely, we ablate our model by (1) decreasing the model size (from 35M to 21M parameters), (2) removing the Transformer architecture (using a pre-trained EfficientNet instead), (3) using a continuous instead of discrete action space (using an MSE loss and multivariate normal output), (4) auto-regressively conditioning on actions, (5) removing ImageNet pre-training of the FiLM EfficientNet, and (6) removing history (reducing the sequence of six images as input to a single image). For each ablation we compare on the axes of performance on seen tasks, performance on unseen tasks, as well as inference speed and robustness to distractors and backgrounds (with a more detailed description of each category in Section 6.1 and Appendix D.1).",
        "Table  13  shows the results of each ablation and the delta performance compared to the full RT-1. RT-1 achieves impressive performance on tasks and new environments, and particularly outperforms baselines on the most challenging robustness problems. We also find that each design decision is important, though at varying levels. We first evaluate a model that replaces the per-dimension discretized action representation in our model with a more standard continuous Gaussian distribution. We observe a significant decline in performance from this modification. The per-dimension discretization allows our model to represent complex multi-modal distributions, while the Gaussian distribution captures only a single mode. These results suggest that this standard and popular choice is highly suboptimal with the more complex and diverse demonstration data used by our system. Im-ageNet pre-training is particularly important for model generalization and robustness, decreasing the unseen task performance rate by 33%, as a result of the large and diverse visuals of the ImageNet dataset. Adding history has an impact primarily on generalization to distractors, while removing the Transformer component has a uniform but small negative impact across the seen tasks, unseen tasks and distractors. In order to keep the ImageNet pre-training while reducing the model size, we reduce the number of parameters only by 40% (from 31M to 25M). Resulting performance drops across training and generalization tasks but not as much as in other ablations. Finally, autoregressively conditioning on actions, as used in  (Reed et al., 2022; Chen et al., 2021; Lee et al., 2022a) , did not benefit performance and slowed inference by more than 2x.",
        "As described in Sec. 5.1, in order to run large Transformer models on real robots, we require a model that supports fast inference for real-time operation. Note that in order to achieve our target control rate of 3Hz (described in Sec. 5.1), we also need to consider other sources of latency in the pipeline, such as the camera latency and communication overhead. However, these factors will be constant Preprint D.5 SUMMARY AND ANALYSIS",
        "In this section, we summarize some of our findings and propose intuition for RT-1's high performance, generalization, and robustness. First, ImageNet pretraining (along with Universal Sentence Encoder language embedding) has a large impact particularly on unseen tasks. We observe that RT-1 inherits some of the knowledge that results from the generality and diversity of the datasets these models were trained on. Second, continuous actions have a large impact across all aspects of performance. This has been previously observed and may be due to the ability to represent more complex action distributions -the per-dimension discretization allows our model to represent complex multi-modal distributions, while the Gaussian distribution captures only a single mode. Third, given such expressive multitask models, data diversity has a larger impact than data size. Indeed, even datasets collected in simulated environments or from different robotic embodiments can be leveraged by RT-1, opening avenues for new regimes of data collection.",
        "Finally, RT-1 fuses language into the image pipeline early via FiLM conditioning, compared to e.g., Gato's late fusion. This enables image tokens that focus only on relevant features for the instruction at hand, which may be the cause of poor distractor performance for Gato. Figure  13  visualizes the attention during rollouts of RT-1. We see that the attention is focused on relevant features and particularly on interaction between the gripper and the object of interest. The bottleneck of attention layers such as these results in a compact representation which effectively ignores distractors and varying backgrounds."
      ],
      "Methods": [
        "As mentioned in Section 4, we evaluate RT-1 with a set of mobile manipulators from Everyday Robots in three environments: two real office kitchens and a training environment modelled off these real kitchens. The training environment, shown in Fig.  2  (a), consists of partial counters while the two real environments, shown in Fig.  2 (b, c ), have similar counter tops to the training environment, but vary in lighting, background, and full kitchen geometry (e.g., there may be a cabinet instead of a drawer or a sink may be visible). The policies are evaluated for performance on training tasks as well as generalization to new tasks, robustness to unseen environments, and performance when chained together for long-horizon tasks, as detailed below.",
        "Seen task performance. To evaluate performance on seen instructions, we evaluate performance on instructions sampled from the training set. Note, however, that this evaluation still involves varying the placement of objects and other factors of the setup (e.g., time of day, robot position), requiring the skills to generalize to realistic variability in the environment. In all, we test over 200 tasks in this evaluation: 36 for picking objects, 35 for knocking objects, 35 for placing things upright, 48 for moving objects, 18 for opening and closing various drawers, and 36 for picking out of and placing objects into drawers.",
        "Unseen tasks generalization. To evaluate generalization to unseen tasks, we test 21 novel, unseen instructions. These instructions are distributed across skills and objects. This ensures that at least some instances of each object and skill were present in the training set but they will be combined in novel ways. For example, if \"pick up the apple\" is held out, then there are other training instructions that include the apple. The list of all unseen instructions can be found in the Appendix D.1.",
        "Robustness. To evaluate robustness, we perform 30 real-world tasks for distractor robustness and 22 tasks for background robustness. The background robustness was tested by evaluating in new kitchens (which have different lighting and background visuals) and with different counter surfaces (e.g., a patterned table cloth). Example configurations of the robustness evaluation scenarios are depicted in Fig.  4 .",
        "Long-horizon scenarios. We also evaluate generalization to more realistic long-horizon scenarios, which each require executing a sequence of skills. The goal of this evaluation is to combine multiple generalization axes such as new tasks, objects, environments and test the overall generalization capabilities in realistic settings. These evaluations consist of 15 long-horizon instructions in two real kitchens, which require executing sequences of skills consisting of ∼ 10 distinct steps, with each step of roughly comparable scope as the training instructions. These steps are obtained automatically from higher level instructions, such as \"how would you throw away all the items on the table?\" by using the SayCan system (Ahn et al., 2022), as described in detail in Section 6.4 and Appendix D.3.",
        "Figure  4 : Evaluation scenarios for distractors (first row), from left to right: easy (0-5 distractors), medium (9 distractors), hard (9 distractors and occluded object); background (second row), from left to right: original environment, patterned table cloth, new kitchen; and realistic scenarios in the real kitchen (third row), generalization levels from left to right: L1, L2 and L3.",
        "In the next set of experiments we evaluate whether our method generalizes enough to be used in long-horizon realistic kitchen settings. To answer this question, we execute RT-1 and various baselines within the SayCan (Ahn et al., 2022) framework in two different real kitchens. Since SayCan combines many low-level instructions to perform high-level instructions, the number of possible high-level instructions increases combinatorially with skills, so the skill-breadth of RT-1 can be fully seen (for more details on the SayCan algorithm please refer to Ahn et al. (  2022 )). The success rate of long-horizon tasks also decreases exponentially with the length of the task, so high success rates in manipulation skills are particularly important. Furthermore, as mobile manipulation tasks require both navigation and manipulation, the policies ability to be robust to base position is crucial. More detail is provided in Appendix D.3.",
        "Table  6  shows our results (on instructions in Appendix Table  12 ). Except for original SayCan, all methods get 87% as planning success rate, and RT-1 performs the best, with 67% execution success rate in Kitchen1. Kitchen2 constitutes a much more challenging generalization scene, since the Robot Classroom training scenes are modeled after Kitchen1 (see the pictures of the kitchens in Fig.  2 ). Due to this generalization difficulty, SayCan with Gato is not able to finish any long horizon task, and SayCan with BC-Z is able to achieve a success rate of 13%. The original SayCan paper did not evaluate performance in a new kitchen. Surprisingly, the manipulation performance does not Preprint see a visible drop from Kitchen1 to Kitchen2 for our method. In the supplementary video, we show that this enables us to operate unseen drawers in Kitchen2, and that we can use SayCan-RT1 to plan and execute ultra-long horizon tasks, with as many as 50 steps. While previous works have shown the scaling abilities of Transformer-based models  (Lee et al., 2022a; Reed et al., 2022; Jiang et al., 2022)  with the number of model parameters, in many robotics works the model size is often not the primary bottleneck, and the maximum size is limited by the latency requirement for running such models on real robots. Instead, in this study we focus on ablating the influence of dataset size and diversity, as they play an important role in the traditionally data-limited robot learning field. Since data collection is particularly expensive for real robots, it is important to quantify what kind of data our models need to achieve a certain performance and generalization. Thus, our last question focuses on the scaling properties of RT-1 with different data properties. Table  7 : Various data ablations of RT-1 across seen tasks, generalization to unseen tasks, and robustness to distractors and backgrounds. Data diversity has a higher impact on the performance and generalization than data quantity.",
        "In Table  7  we show the performance, generalization, and robustness of RT-1 as we decrease the dataset size (% data) and the dataset diversity (% tasks). To separate the axes of dataset size and diversity, we create smaller datasets with the same task diversity by removing data from the tasks with the largest data, capping the number of examples per task at 200 (resulting in 51% of the data), Preprint 100 (37% of the data), and 50 (22.5% of the data). To create a narrow dataset, we remove the tasks with the least data, thus keeping 97% of the overall data but only 75% of the tasks. As we decrease dataset size, we see a general trend of decreasing performance and a steeper trend of decreasing generalization. As we make the dataset more narrow, we see much steeper performance reductions, particularly in terms of generalization. In fact, removing 25% of the tasks while keeping 97% of the data achieves an equivalent generalization performance to reducing the dataset size by as much as 49%. Our key takeaway is thus that data diversity is more essential than data quantity."
      ],
      "Conclusion": [
        "We presented Robotics Transformer 1, RT-1, a robot learning method that can effectively absorb large amounts of data and scales with data quantity and diversity. We trained RT-1 on a large dataset of demonstrations containing over 130k episodes collected over the course of 17 months with 13 robots. In our broad set of experiments, we demonstrated that our method that can perform over 700 instructions at 97% success rate and effectively generalize to new tasks, objects and environments better than previously published baselines. We also demonstrated that RT-1 can successfully absorb heterogeneous data from simulation and other robot morphologies without sacrificing original-tasks performance and while improving generalization to new scenarios. Lastly, we showed how this level of performance and generalization allowed us to execute very long-horizon tasks in the SayCan  (Ahn et al., 2022)  framework, with as many as 50 steps.",
        "While RT-1 presents a promising step towards large-scale robot learning with an data-absorbent model, it comes with a number of limitations. First, it is an imitation learning method, which inherits the challenges of that class of approaches such as the fact that it may not be able to surpass the performance of the demonstrators. Second, the generalization to new instructions is limited to the combinations of previously seen concepts and RT-1 is not yet able to generalize to a completely new motion that has not been seen before. Lastly, our method is presented on a large but not very dexterous set of manipulation tasks. We plan to continue extending the set of instructions that RT-1 enables and generalizes to to address this challenge.",
        "As we explore future directions for this work, we hope to scale the number of robot skills faster by developing methods that allow non-experts to train the robot via directed data collection and model prompting. While the current version of RT-1 is fairly robust especially to distractor objects, its robustness to backgrounds and environments could be further improved by greatly increasing the environment diversity. We also hope to improve the reaction speeds and context retention of RT-1 through scalable attention and memory.",
        "To allow the research community to build on top of this work, we have open-sourced the code for RT-1 4  , which we hope will provide researchers with a valuable resource for future research for scaling up robot learning."
      ]
    }
  },
  {
    "paperId": "5f0b826ffe17faa2cdec21752e7d1863bd909f2c",
    "title": "Foundation models in robotics: Applications, challenges, and the future",
    "sections": {
      "Introduction": [
        "F OUNDATION models are pretrained on extensive internet-scale data and can be fine-tuned for adaptation to a wide range of downstream tasks. Foundation models have demonstrated significant breakthroughs in vision and language processing; examples include BERT  [1] , GPT-3  [2] , GPT-4  [3] , CLIP  [4] , DALL-E  [5] , and PaLM-E  [6] . Foundation models have the potential to unlock new possibilities in robotics domains such as autonomous driving, household robotics, industrial robotics, assistive robotics, medical robotics, field robotics, and multi-robot systems. Pretrained Large Language Models (LLMs), Large Vision-Language Models (VLMs), Large Audio-Language Models (ALMs), and Large Visual-Navigation Models (VNMs) can be utilized to improve various tasks in robotics settings. The integration of foundation models into robotics is a rapidly evolving area, and the robotics community has very recently started exploring ways to leverage these large models within the robotics domain for perception, prediction, planning, and control.",
        "Prior to the emergence of foundation models, traditional deep learning models for robotics were typically trained on limited datasets gathered for distinct tasks  [7] . Conversely, foundation models are pre-trained on extensive and diverse data, which has been proven in other domains (such as natural language processing, computer vision, and healthcare  [8] ) to significantly expand adaptability, generalization capability, and overall performance. Ultimately, foundation models may hold the potential to yield these same benefits in robotics. Knowledge transfer from foundation models may reduce training time and computational resources compared to task-specific 1 Preliminary release. We are committed to further enhancing and updating this work to ensure its quality and relevance models. Particularly relevant to robotics, multimodal foundation models can fuse and align multimodal heterogeneous data gathered from various sensors into compact homogeneous representations needed for robot understanding and reasoning  [9] . These learned representations hold the potential to be used in any part of the autonomy stack including perception, decisionmaking, and control. Furthermore, foundation models provide zero-shot capabilities, which refer to the ability of an AI system to perform tasks without prior examples or dedicated training data for that specific task. The would enable robots to generalize their learned knowledge to novel cases, enhancing adaptability and flexibility for robots in unstructured settings.",
        "Integrating foundation models into robotic systems may enable context-aware robotic systems by enhancing the robot's ability to perceive and interact with the environment. For example in the perception domain, Large Vision-Language Models (VLMs) have been found to provide cross-modal understanding by learning associations between visual and textual data, aiding tasks such as zero-shot image classification, zero-shot object detection  [10] , and 3D classification  [11] . As another example, language grounding in the 3D world  [12]  (aligning contextual understanding of VLMs to the 3dimensional (3D) real world) may enhance a robot's spatial awareness by associating words with specific objects, locations, or actions within the 3D environment.",
        "In the decision-making or planning domain, LLMs and VLMs have been found to assist robots in task specification for high-level planning  [13] . Robots can perform more complex tasks by leveraging linguistic cues in manipulation, navigation, and interaction. For example, for robot policy learning techniques like imitation learning  [14]  and reinforcement learning  [15] , foundation models seem to offer the possibility to improve data efficiency and enhance contextual understanding. In particular, language-driven rewards can be used to guide RL agents by providing shaped rewards  [16] . Also, researchers have employed language models to provide feedback for policy learning techniques  [17] . Some works have shown that a VLM model's visual question-answering (VQA) capability can be harnessed in robotics use cases. For example, researchers have used VLMs to answer questions related to visual content to aid robots in accomplishing their tasks  [18] . Also, researchers have stated utilizing VLMs to help with data annotation, by generating descriptive labels for visual content  [19] .",
        "Despite the transformative capabilities of foundation models in vision and language processing, the generalization and finetuning of foundation models for real-world robotics tasks remain challenging. These challenges include: 1) Data Scarcity: how to obtain internet-scale data for robot manipulation, locomotion, navigation, and other robotics tasks, and how to perform self-supervised training with this data, 2) High Variability: how to deal with the large diversity in physical environments, physical robot platforms, and potential robot tasks while still maintaining the generality required for a foundation model, 3) Uncertainty Quantification: how to deal with (i) instance-level uncertainty such as language ambiguity or LLM hallucination; (ii) distribution-level uncertainty; and (iii) distribution-shift, especially resulting from closed-loop robot deployment, 4) Safety Evaluation: How to rigorously test for the safety of a foundation model-based robotic system (i) prior to deployment, (ii) as the model is updated throughout its lifecycle, and (iii) as the robot operates in its target environments. 5) Real-Time Performance: how to deal with the high inference time of some foundation models which could hinder their deployment on robots and how to accelerate inference in foundation models to the speed required for online decision-making.",
        "In this survey, we study the existing literature on the use of foundation models in robotics. We study current approaches and applications, present current challenges, suggest directions for future research to address these challenges, and identify potential risks exposed by integrating foundation models into robot autonomy. Another survey on foundation models in robotics appeared simultaneously with ours on arXiv  [20] . In comparison with that paper, ours emphasizes future challenges and opportunities, including safety and risk, and ours has a stronger emphasis on comparisons in applications, algorithms, and architectures among the existing papers in this space. In contrast to some existing surveys that focus on a specific incontext instruction, such as prompts  [21] , vision transformers  [22] , or decision-making  [13] ,  [23] , we provide a broader perspective to connect distinct research threads in foundation models organized around their relevance to and application to robotics. Conversely, our scope is much narrower than the paper  [24] , which explores the broad application of foundation models across many disciplines, of which robotics is one. We hope this paper can provide clarity regarding areas of recent progress and existing deficiencies in the research, and point the way forward to future opportunities and challenges facing this research area. Ultimately, we aim to give a resource for robotics researchers to learn about this exciting new area.",
        "We limit the scope of this survey to papers that fall into one of the following categories: 1) Background Papers: Papers that do not explicitly link to robotics, but are nonetheless required for understanding foundation models. These papers are discussed in the background section (section II) of the survey paper. 2) Robotics Papers: Papers that integrate a foundation model into a robotic system in a plug-and-play fashion, papers that adapt or fine-tune foundation models for robotic systems, or papers that build new robotic-specific foundation models. 3) Robotics-Adjacent Papers: Papers that present methods or techniques applied to areas adjacent to robotics (e.g., computer vision, embodied AI), with a clear path to future application in robotics.",
        "This survey is organized as follows: In Section II, we provide an introduction to foundation models including LLMs, vision transformers, VLMs, embodied multimodal language models, and visual generative models. In addition, in the last part of this section, we discuss different training methods used to train foundation models. In Section III, we present a review of how foundation models are integrated into different tasks for decision-making in robotics. First, we discuss robot policy learning using language-conditioned imitation learning, and language-assisted reinforcement learning. Then, we discuss how to use foundation models to design a languageconditioned value function that can be used for planning purposes. Next, robot task specification and code generation for task planning using foundation models are presented. In Section IV, we study various perception tasks in robotics that have the potential to be enhanced by employing foundation models. These tasks include semantic segmentation, 3D scene representation, zero-shot 3D classification, affordance prediction, and dynamics prediction. In Section V, we present papers about Embodied AI agents, generalist AI agents, as well as simulators and benchmarks developed for embodied AI research. In Section VI, we conclude the survey by discussing different challenges for employing foundation models in robotic systems and proposing potential avenues for future research. Finally, in Section VII we offer the concluding remarks.",
        "Foundation models have billions of parameters and are pretrained on massive internet-scale datasets. Training models of such scale and complexity involve substantial costs. Acquiring, processing, and managing data can be costly. The training process demands significant computational resources, requiring specialized hardware such as GPUs or TPUs, as well as software and infrastructure for model training which requires financial resources. Additionally, training a foundation model is time-intensive, which can translate to even higher costs. Hence these models are often used as plug-and-play modules (which refers to the integration of foundation models into various applications without the need for extensive customization). Table  I  provides details about commonly used foundation models. In the rest of this section, we introduce LLMs, vision transformers, VLMs, embodied multi-modal language models, and visual generative models. In the last part of this section, we introduce different training methods that are used to train foundation models."
      ],
      "Conclusion": [
        "Multimodal interaction implicitly assumes that the modality is tokenizable and can be standardized into input sequences without losing information. The Multimodal models provide information sharing between multiple modalities and are some variation of multimodal transformers with cross-modal attention between every pair of inputs. In multimodal representation learning, it is assumed that cross-modal interactions and the dimension of heterogeneity between different modalities can all be captured by simple embeddings. In other words, a simple embedding is assumed to be sufficient to identify the modality or for example, how different language is from vision. In the realm of multimodal representation learning, the question of whether a single multimodal model can accommodate all modalities remains an open challenge.",
        "Additionally, when paired data between a modality and text is available one can embed that modality into text directly. In robotics applications there are some modalities for which sufficient data is not available and to be able to align them with other modalities, they need to be first converted to other modalities and then be used. For example, 3D point cloud data has various applications in robotics but training a foundation model using this type of data is challenging since data is scarce and is not aligned with text. So, one way to overcome this challenge is first converting this 3D point cloud data to other modalities such as images and subsequently images to text as the secondary step of alignment. Then they can be used in foundation model training. As another example, in Socratic models  [194] , each modality, whether visual or auditory, is initially translated into language, after which language models attempt to respond to these modalities.",
        "Through examination of the recent literature, we have surveyed the diverse and promising applications of foundation models in robotics. We have delved into how these models have enhanced the capabilities of robots in areas such as decision-making, planning and control, and perception. We also discussed the literature on embodied AI and generalist AI, with an eye toward opportunities for roboticists to extend the concepts in that research field to real-world robotic applications. Generalization, zero-shot capabilities, multimodal capabilities, and scalability of foundation models have the potential to transform robotics. However, as we navigate through this paradigm shift in incorporating foundation models in robotics applications, it is imperative to recognize the challenges and potential risks that must be addressed in future research. Data scarcity in robotics applications, high variability in robotics settings, uncertainty quantification, safety evaluation, and realtime performance remain significant concerns that demand future research. We have delved into some of these challenges and have discussed potential avenues for improvement."
      ],
      "Results": [
        "The problem of safety evaluation is closely related to uncertainty quantification. How can we rigorously test for the safety of a foundation model-based robotic system (i) before deployment, (ii) as the model is updated during its lifecycle, and (iii) as the robot operates in its target environments? We highlight challenges and research opportunities related to these problems below.",
        "1) Pre-deployment safety tests: Rigorous pre-deployment testing is crucial for ensuring the safety of any robotic system. However, this can be particularly challenging for robots that incorporate foundation models. First, foundation models are trained on vast amounts of data; thus, a rigorous testing procedure should ensure that test scenarios were not seen by the model during training. Second, foundation models often commit errors in ways that are hard to predict a priori; thus, tests need to cover a diverse enough range of scenarios to uncover flaws. Third, foundation models such as LLMs are often used to produce open-ended outputs (e.g., a plan for a robot described in natural language). The correctness of such outputs can be challenging to evaluate in an automated manner if these outputs are evaluated in isolation from the entire system.",
        "The deployment cycle of current foundation models (in non-robotics applications) involves thorough red-teaming by human evaluators  [3] ,  [219] . Recent work has also considered partially automating this process by using foundation models themselves to perform red-teaming  [220] ,  [221] . Developing ways to perform red-teaming (both by humans and in a partially automated way) for foundation models in robotics is an exciting direction for future research.",
        "In addition to evaluating the foundation model in isolation, it is also critical to assess the safety of the end-to-end robotic system. Simulation can play a critical role here, and already does so for current field-deployed systems such as autonomous vehicles  [222] ,  [223] . The primary challenges are to ensure that (i) the simulator has high enough fidelity for results to meaningfully transfer to the real world, and (ii) test scenarios (manually specified, replicated from real-world scenarios, or automatically generated via adversarial methods  [224] ) are representative of real-world scenarios and are diverse enough to expose flaws in the underlying foundation models. In addition, finding ways to augment large-scale simulation-based testing with smaller-scale real-world testing is an important direction for future work. We emphasize the need for performing such testing throughout the lifecycle of a field-deployed robotic system, especially as updates are made to different components (which may interact in unpredictable ways with foundation models).",
        "2) Runtime monitoring and out-of-distribution detection: In addition to performing rigorous testing offline, robots with foundation model-based components should also perform runtime monitoring. This can take the form of failure prediction in a given scenario, which can allow the robot to deploy a safety-preserving fallback policy  [225] -  [229] . Alternately, the robot can perform out-of-distribution (OOD) detection using experiences collected from a small batch of scenarios in a novel distribution  [230] -  [233] ; this can potentially trigger the robot to cease its operations and collect additional training data in the novel distribution in order to re-train its policy. Developing techniques that perform runtime monitoring and OOD detection with statistical guarantees on false positive/negative error rates in a data-efficient manner remains an important research direction."
      ]
    }
  },
  {
    "paperId": "3396609b96dd24cac3b1542aec686ce362f32fe2",
    "title": "Language-Driven Representation Learning for Robotics",
    "sections": {
      "Introduction": [
        "Good words are worth much, and cost little.",
        "-George Herbert Realizing a future of ubiquitous, broadly capable robots is predicated on systems capable of generalizable perception and interaction  [Weiss et al. 1987; Chaumette and Hutchinson 2006; Levine et al. 2016] . Towards this goal, recent work in robotics present approaches for learning visual representations to bootstrap learning for visuomotor control  [Parisi et al. 2022; Nair et al. 2022; Radosavovic et al. 2022] . Critically, these approaches show that we can learn such representations from real-world videos of human behavior -specifically, egocentric video datasets such as Something-Something-v2 and Ego4D  [Goyal et al. 2017; Grauman et al. 2022 ] -instead of solely relying on in-domain robotics data that is scarce and expensive. While prior work has developed and evaluated representations for visuomotor control, robot learning is an expansive discipline, spanning a diverse spectrum of problems: predicting grasp proposals from visual input  [Saxena et al. 2008; Mahler et al. 2017] , languageconditioned imitation learning  [Tellex et al. 2011 ] and belief/intent tracking for human-robot interaction  [Hauser 2012; Javdani et al. 2018] , amongst others. Broadening our focus to problems beyond learning for control enables us to develop flexible, generalizable representations that capture both low-level spatial reasoning and high-level semantic understanding -a flexibility that is a key prerequisite to realizing a foundation model for robotics  [Bommasani et al. 2021 ]. Thus, we ask: how can we learn visual representations that generalize across the diverse spectrum of problems in robot learning?",
        "Recent approaches for learning visual representations for robotics use pretraining objectives that reflect different inductive biases for what the learned representations should capture. Masked Visual Pretraining [MVP;  Radosavovic et al. 2022]  proposes using masked autoencoding  [He et al. 2022]  to prioritize visual reconstruction from heavily masked video frames, encoding representations that facilitate per-pixel reconstruction. Separately, Reusable Representations for Robotic Manipulation [R3M;  Nair et al. 2022 ] eschews pixel reconstruction for two contrastive learning objectives: time contrastive learning  [Sermanet et al. 2018 ] and video-language alignment. These approaches show strong performance on imitation learning in simulated and real-world settings, with sizeable improvements over strong alternatives such as ResNet or CLIP features  [He et al. 2016; Radford et al. 2021] ; however, they have not been evaluated beyond these settings. As a first contribution, we evaluate these representations on problems beyond control and identify inconsistent evaluation performance, with huge penalties depending on the approach and specific application. MVP performs well on problems such as grasp affordance prediction, but struggles with higher-level problems such as language-conditioned imitation. R3M instead excels at the higher-level problems, but degrades completely on problems such as grasp affordance prediction.",
        "Motivated by this, we present Voltron, a framework for languagedriven visual representation learning for robotics that learns representations that capture both low-level and high-level features, empirically outperforming prior approaches over all applications. Voltron models take videos and associated language captions as input to a masked autoencoding pipeline, reconstructing one (or more) frames from a masked context. The novelty of our framework is in how we use language supervision. Depending on a tunable probability 𝛼, we either condition on (𝛼 = 0), or generate (𝛼 > 0) the associated caption. Explicitly conditioning on words in different contexts allows for low-level pattern recognition at the local, spatial level, while generating language from our learned visual Figure  1 : Voltron Evaluation Suite. We introduce a suite of evaluation problems spanning five applications within robotics, including grasp affordance prediction, referring expression grounding, single-task visuomotor control (in simulation), language-conditioned imitation learning (on a real robot), and intent scoring. encoding allow us to infer higher-level features around affordances and intents. Furthermore, guided by the hypothesis that language is especially useful in describing change, we study dual-frame contexts consisting of the initial and current observation in multi-timestep tasks. Altogether, we examine three different Voltron variants: V -Cond (Language Conditioning: single frame, 𝛼 = 0), V -Dual (Adding Context: dual-frame conditioning, 𝛼 = 0), and V -Gen (Adding Language Generation: dual-frame, 𝛼 = 0.5 -we find that 𝛼 = 1 with no language-conditioning at all hurts performance).",
        "To evaluate Voltron and other visual representation learning approaches, we assemble a new evaluation suite (depicted in Figure  1 ) spanning five problem domains within robotics: 1) dense segmentation for grasp affordance prediction  [Zeng et al. 2017] , 2) object detection from referring expressions (e.g., \"the blue coffee mug to the left of the plate\") in cluttered scenes  [Wang et al. 2021] , 3) imitation learning for visuomotor control (in simulation)  [Nair et al. 2022] , 4) learning multi-task language-conditioned policies for real-world manipulation  [Stepputtis et al. 2020 ] (on a real-world Franka Emika fixed-arm manipulator), and 5) zero-shot intent scoring  [Javdani et al. 2018; Chen et al. 2021 ]. We choose these tasks for their broad coverage; tasks such as grasp affordance prediction and referring expression grounding require reasoning over low-level spatial features, while language-conditioned imitation and intent scoring require a deeper understanding of semantics.",
        "Through experiments controlling for pretraining data and model capacity, we show that the simplest Voltron representations (from V -Cond) strictly outperform both MVP and R3M representations across all evaluation domains. Furthermore, by adapting our models to learn from multiple frame contexts and that favor generation (e.g., with V -Dual and V -Gen), we show that we can further boost performance on evaluations requiring higher-level features such as with language-conditioned policy learning (on a real robot) and intent scoring. Though language-conditioning offers universal performance gains, there are tradeoffs between Voltron models; adding language generation hurts performance on some control tasks, even though its necessary for strong performance on intent scoring. Furthermore, Voltron with single-frame language conditioning performs well on non-episodic tasks (e.g., grasping), but underperforms multi-frame models on control tasks. There is not yet a silver bullet -a single representation strong on all tasks -but the ability to balance tradeoffs between encoding low and high-level features offers a net win over restrictions of past work.",
        "Contributions. 1) We present Voltron, a framework for languagedriven visual representation learning. Through controlled experiments and comprehensive ablations we demonstrate that Voltron's representations strictly outperform the prior art across 2) a new evaluation suite composed of five distinct problem domains within robotics. Finally, 3) we analyze the tradeoffs between different Voltron models that balance different types of feature learning, outlining several directions for future work. We release all models, the evaluation suite, code (pretraining and adaptation), and preprocessed data (https://sites.google.com/view/voltron-robotics).",
        "Limitations. We do not have access to the compute resources to train models of the same scale and data used in prior work  [Radosavovic et al. 2022; Nair et al. 2022] . Instead, we carefully reproduce MVP and R3M -the current state-of-the-art approaches -by pretraining on the Something-Something-v2 dataset  [Goyal et al. 2017] , further controlling for batch ordering, model capacity, and other sources of randomness (full details are in §4). However, for full context we also include results from the official release artifacts from both these works, as well as other methods such as CLIP  [Radford et al. 2021 ], though we note these results in gray or with dashed lines as to indicate they are not directly comparable."
      ],
      "Related Work": [
        "Voltron is situated within a rich body of work in visual representation learning for robotics and multimodal pretraining. The Voltron Framework. Central to our approach is language-driven learning on top of a masked autoencoding backbone. We incorporate language in two ways, following §3.2: 1) as a conditioning variable fed to a multimodal encoder that also encodes one or more video frames, or 2) as a generation target for the language generator [Left]. During downstream evaluation, we use the (frozen) outputs from the encoder, adapting evaluation-specific \"heads\" on top  [Right] .",
        "Visual Representation Learning for Robotics. An emerging body of work in robot learning studies learning visual state representations for control. A wealth of prior approaches learn representations from in-domain data taken directly from the target environment (and corresponding task); these techniques range from using data augmentation  [Laskin et al. 2020; Srinivas et al. 2020; Kostrikov et al. 2021; Pari et al. 2022]  to modeling forward dynamics  [Gelada et al. 2019; Hafner et al. 2020]  to using task-specific information  [Jonschkowski and Brock 2015; Zhang et al. 2021 ]. Unlike these approaches, we move beyond task-specific data, instead leveraging large, accessible datasets such as videos of humans performing everyday tasks. Work in this paradigm has exploded in recent years. A number of approaches find that existing representations such as features from models trained on ImageNet  [Deng et al. 2009] , or features from CLIP  [Radford et al. 2021 ] enable more efficient learning  [Shah and Kumar 2021; Khandelwal et al. 2021 ]. More recently, multiple approaches have shown increased dividends in applying such representations to visuomotor control, for example by combining features at different layers of pretrained ResNets  [Parisi et al. 2022]  or by pretraining such representations on human videos, conjecturing that such data captures features useful for robotic manipulation  [Nair et al. 2022; Xiao et al. 2022; Radosavovic et al. 2022; Ma et al. 2022 ]. However, missing from these approaches is a notion of semantics; works such as MVP  [Xiao et al. 2022; Radosavovic et al. 2022 ] purely learn to perform masked reconstruction from a single image, and even works that leverage some temporal and linguistic signals do so in a limited way  [Nair et al. 2022; Ma et al. 2022 ]. Instead, our work is motivated by the hypothesis that language understanding -both via conditioning and generation -is an essential component of learning generalizable visual representations. It is not enough that a representation summarizes an observation; instead, for generalization to new contexts and behaviors, it must capture how observations (and changes thereof) relate to higher-level semantic abstractions.",
        "Voltron aims to do this with its language-driven representation learning objective: by jointly modeling sequences of frames and language, we enable a range of capabilities, from producing representations of single images in isolation, to providing the capability to generate language grounded in visual contexts. We demonstrate the benefits of language-driven learning in our evaluation (see §5): in head-to-head comparisons, Voltron models strictly outperform prior approaches across all evaluation domains.",
        "Learning Multimodal Foundation Models. Our work draws further inspiration from a wave of progress in multimodal foundation models such as CLIP, Multimodal Masked Autoencoders (M3AE), Flamingo, CoCa, and Gato, amongst many others  [Radford et al. 2021; Geng et al. 2022; Alayrac et al. 2022; Yu et al. 2022; Reed et al. 2022; Lu et al. 2023; Aghajanyan et al. 2022 ]. These approaches highlight the myriad benefits of multimodal pretraining: language supervision works to enrich visual representations (even in the absence of language downstream), while visual supervision similarly enriches language representations  [Lu et al. 2019; Singh et al. 2022] . Of the many capabilities afforded by these models, many have applications in embodied AI and robotics. CLIP representations have shown to be effective in applications to various robotics tasks  [Shridhar et al. 2021; Khandelwal et al. 2021; Cui et al. 2022] , while multimodal transformer models have proven effective initializations for training control policies  [Reid et al. 2022; Liu et al. 2022 ]. These approaches are similar to Voltron in their joint use of visual and language inputs; where Voltron differs, however, is in our novel representation learning objective that balances language conditioning and generation, enabling learning representations that transfer to a wide range of applications within robotics.",
        "3 Voltron -Language-Driven Learning",
        "We assume access to a dataset of videos paired with natural language annotations; in each video-language pair (𝑣, 𝑐), language can take the form of a caption (e.g., \"peels the carrot\" in Figure  2 ), narration, or even coarse textual label of a behavior. We assume each video 𝑣 ∈ R 𝑇 ×𝐻 ×𝑊 ×𝐶 consists of a sequence of frames 𝑣 = [𝑜 1 , . . . , 𝑜 𝑇 ], where each frame 𝑜 𝑖 ∈ R 𝐻 ×𝑊 ×𝐶 is RGB-encoded. We tokenize and one-hot encode each utterance into a vocabulary 𝑉 of cardinality |𝑉 |, padding to a max length 𝐿 such that 𝑐 ∈ R 𝐿×|𝑉 | . We define a <NULL> token (separate from the <PAD> token) as a placeholder for an empty language context. Furthermore, following the MAE work, we define a visual masking function Mask(𝑣, 𝛾) → (𝑣 visible ∈ R (1-𝛾 ) (𝑇 ×𝐻 ×𝑊 ×𝐶) , 𝑣 masked ∈ R 𝛾 (𝑇 ×𝐻 ×𝑊 ×𝐶) ) that partitions the regions of a video into a set of visible and masked-out regions subject to a fixed masking ratio 𝛾. We sample a mask once, and apply it uniformly across all frames in the video to prevent leakage  [Tong et al. 2022] ; if the masks were sampled independently, a masked region in one frame could be visible in another, allowing the encoder to \"cheat\" by looking ahead."
      ],
      "Results": [
        "We outline our evaluation suite (Table  1 ) comprised of five problem domains within robotics. Each evaluation consists of adaptation data and evaluation metrics. The adaptation data consists of visual input(s) (as RGB frames) and in some cases, language (e.g., an instruction for language-conditioned imitation). We evaluate representations from Voltron and various baseline models by freezing the pretrained vision and language encoders, instead adapting evaluation-specific \"heads\"(lightweight networks) on top of the extracted representations. We choose evaluations that represent domains that capture different types of understanding; in the following sections, we motivate the role of each application and provide experimental results.",
        "To supplement our single-task visuomotor control results, we run out evaluations on the Adroit dexterous manipulation tasks from the R3M paper  [Nair et al. 2022] . The two tasks we evaluate on, depicted in Figure  12  (left) consist of controlling a high degree-of-freedom robotic hand (24-DoF) for the task of 1) relocating a ball on the table to a specified target position, and 2) reorienting a pen within the hand to reach a target orientation. Given the innate difficulty of controlling a high-dimensional dexterous robotic hand over a 9-DoF fixed arm manipulator, these tasks are evaluated with 𝑛 ∈ [25, 50, 100] demonstrations instead of 𝑛 ∈  [5, 10, 25]  as with the Franka Kitchen evaluation. In general, learning policies in this environment is difficult, especially from limited data.",
        "Looking to the results we see that on this environment, V -Gen and R-R3M models tend to be the most performant, in contrast with the Franka Kitchen results which favored V -Cond and V -Dual (the reconstruction-leaning models). Interestingly, this flipped trend seems to suggest that even within single-task control, different tasks and environments seems to prefer different visual features to perform well -in this case, the more high-level features under models such as R-R3M and V -Gen seem to be preferred. In a way, this makes sense; unlike with Franka Kitchen, the actual background objects and interactions thereof -turning knobs, opening microwaves, or sliding doors with clearly marked handles -seem more sensitive to low-level features (where on the microwave is the handle, which knob of the various possible needs to be turned). In Adroit however, these tasks are on clean backgrounds, with individual objects; the high-level behaviors instead that are more important (e.g., \"is the ball getting closer to the target location?\"). It would be an interesting direction for future work Figure  13 : Real-World Language-Conditioned Imitation Rollouts from V -Gen. We visualize some rollouts from the best-performing real-world language-conditioned imitation learning model, V -Gen. While some tasks -e.g., discarding the plate of used coffee pods in the trash -prove hard for all methods, V -Gen shows smooth motion on a series of tasks, even when challenging visual distractors are present. Videos with evaluation rollouts for each method are on our project page.",
        "to further profile other \"common\" visuomotor control tasks along this axis, to get a better understanding of what visual representations must capture to be useful in general tasks -to the extent of predicting ahead of time what features would be useful to aid in solving a task."
      ],
      "Discussion": [
        "The comparative results across the various evaluation problem domains paint Voltron's language-driven representations in a favorable light relative to MVP and R3M baselines. Yet, there remain key questions that we address in this section: is language supervision actually driving these results? Why generative language modeling over masked language modeling? Will Voltron scale?",
        "Ablation: The Impact of Language Supervision. The second row of Table  4  shows a subset of evaluation results across three different problem domains when training a \"no-language\" variant of the V -Cond architecture -this variant is in essence an alternate version of a masked autoencoder that uses the small architecture modifications we added for training stability in §4. As such, it also serves as an architecture ablation when compared to the R-MVP results, enabling us to isolate the impact of the small stability modifications described in §4. Indeed, the results confirm our hypotheses: first, removing language results in a definitive drop in performance across all evaluation applications. Second, the respective results for each evaluation application are on par with the corresponding results for the R-MVP model, demonstrating that the performance of Voltron models does not stem from the architecture. We delve further into this ablation in §C.1.",
        "Ablation: Generative vs. Masked Language Modeling. Looking at the Voltron objective, a natural question to ask is why we chose language generation over masked language modeling. Furthermore, recent and concurrent work propose learning multimodal masked autoencoders (M3AE) both within and outside of robotics  [Geng et al. 2022; Liu et al. 2022] , showing promising results in learning visual representations for image classification tasks, amongst others. To assess the differences, we choose to reproduce the M3AE model in a manner similar to our reproduction of MVP and R3M; we keep the same Something-Something-v2 pretraining data, adopting the exact procedure described in  Geng et al. [2022] , then evaluating the resulting representations on the same subset of evaluation domains as in the prior ablation (third row of Table  4 ). Surprisingly, we see drastic drops in performance across the board. Looking at the pretraining curves, we identify a possible reason for this failure: in optimizing M3AE on Sth-Sth, we see the language modeling loss go to zero almost immediately, leading to overfitting. A possible explanation is that the masked language modeling conditioned on visual contexts in datasets annotated with short, predictable narrations leads to degenerate representations, while generative language modeling is not susceptible to the same types of collapse; looking at ways to mitigate this seems like a promising direction for future work. Explicit details around pretraining and evaluating R-M3AE, with an in-depth discussion are in §C.2. the 22M in the ViT-Small). We see universal improvement: Top-5% precision for grasping (Table  2 ; middle row) increases by 15%, expression grounding accuracy improves (Table  3 ; middle row), as does performance on control.",
        "Extension: Robustness to Real-World Distractors. Factors such as lighting conditions, time of day, and accidental environment perturbations (e.g., a colleague knocking over the camera) can have a profound impact on performance of robotic systems, especially if learned representations are not robust. We run a limited \"robustness\" evaluation after training language-conditioned policies from the demonstrations described in §5.4. Success rates before and after introducing visual distractors for two of the \"meta-tasks\" are in Figure  6  (bottom right).  3  We find that Voltron and R-MVP models are robust to even the most extreme distractors -seemingly a benefit of per-patch masking coupled with MAP-based extraction.",
        "We propose Voltron, a framework for language-driven representation learning that balances conditioning and generation to shape the balance of low and high-level features captured. We introduce an evaluation suite spanning five diverse problems within robotics for holistically evaluating visual representations. Through controlled experiments and ablations, we validate the strengths of our representations; across all evaluation tasks, Voltron models that balance language conditioning and generation strictly outperform prior approaches such as R3M and MVP, and in many cases show performance competitive with or exceeding that of approaches that use orders of magnitude more data or more expressive models. Yet, while language is a pivotal source of supervision, there are still key questions to answer. Why is language-based pretraining helpful on tasks that have nothing to do with language? Why not try to learn one model that can encode both low-level and high-level features, without tradeoffs? While there is not a silver bullet yet we hope that future work takes a deep, grounded look at these questions, identifying what existing representations capture -and more importantly, what they miss. Our hope is that Voltron serves as a starting point; a flexible, unified framework for future improvements in visual representation learning for robotics.",
        "A Motivating Questions Q1. From the results, some Voltron models outperform larger models such as MVP-Base trained on significantly more data, even on tasks that do not necessarily need language information. How do you make sense of this?",
        "We find that in many of our evaluation domains, especially domains with episodic tasks such as single-task and language-conditioned imitation learning, it is important to discern differences across frames in the same overall visual context, or otherwise pay attention to small visual distinctions. Looking at the original MVP work  [Radosavovic et al. 2022] , we see that the original pretraining datasets are compiled by sampling frames from various video datasets once, in a single-step procedure, at low sampling rates. For many datasets (such as Sth-Sth and Ego4D), this means only seeing 1-2 frames per video clip in total during training.",
        "In contrast, when we sample data from Sth-Sth, we ensure to sample at least 5 frames per clip, per epoch; while the aggregate amount of diverse contexts is much lower than in the original MVP work, seeing multiple frames per context seems to significantly help learning, and not just for Voltron models! On the tasks where Voltron models outperform MVP (EgoSoup) (with a larger ViT-Base encoder), we also see commensurate gains in our reproductions R-MVP and R-R3M. For example, R-MVP is at par with or only slightly less performant than MVP (EgoSoup) on grasp affordance prediction and single-task control. We offer further discussion in §D.1.",
        "Q2. Why don't you evaluate models trained with 𝛼 = 1 (pure language generation)?",
        "In preliminary experiments, we partially pretrained variants of V -Gen with values 𝛼 = 0.25, 0.5, 0.75; we focused on evaluating the downstream performance of these representations in the context of the single task visuomotor control evaluation. With 𝛼 = 0.75 we observed significant performance degradation on control tasks; furthermore, looking at the pretraining loss curves, we saw the reconstruction error plateau early in training. We found that 𝛼 = 0.5 balanced learning, and allowed us to continue to push reconstruction error down while also pushing the language generator loss (cross-entropy) lower; with 𝛼 = 0.25, we saw the opposite trend as with 𝛼 = 0.75.",
        "These results are to be taken with a grain of salt, given the limited pretraining duration. However, we worry that with 𝛼 = 1, we might suffer doubly for 1) never conditioning on language, which is so clearly helpful from our results, and 2) potentially fall into the same failure mode as the R-M3AE multimodal masked autoencoder from Section §6 in the main text, overfitting to the language loss. In general, V -Gen with 𝛼 = 0.5 already converges to a substantially higher reconstruction loss as V -Cond and V -Dual, as shown in the pretraining curves in §B.3. That being said, it is a promising avenue for future work to understand if this is inherent or a problem with the specific optimization procedure we used -perhaps changing the relative scaling of the two losses over the course of pretraining may mitigate this issue, or even adaptively clipping the gradient updates depending on the relative contribution of the visual reconstructor or language generator. Q3. Why does language during pretraining help for downstream tasks that don't use language?",
        "Consider a masked visual input of a \"black, curved object above a wooden surface. \" Given this information -and this information alonewhat is a plausible reconstruction? There are myriad objects that fit those percepts -a black, curved object: we could lbe looking at the side of a bowl, the handle of a briefcase, the arm of a chair or stool, or in general, any number of possible options. A masked autoencoder optimizing for reconstruction must embed in the representation of this input as many of the features possible to enable good downstream reconstruction loss. It needs to model everything, as the visual context is under-specified and ambiguous. This compressed bottleneck is core to learning a masked autoencoder, but the unfortunate byproduct of this -in light of a vast world of possibilities -are representations that try to capture everything they possibly can.",
        "Contrast this with a world in which you are told that the same visual context is associated with the language caption \"lifting a black coffee mug on the table. \" What changes? The posterior over possible objects collapses down to the narrow slice of possibilities captured by \"black coffee mug\"; under this new set of possibilities, what does the encoder focus on? What type of black coffee mug is on the table? If it is being lifted, how is it being lifted? From what part of the object -the handle (seen in frame), or somewhere else? What are the features that help further reconstruct the black coffee mug? The other nearby surfaces -what is the mug resting on (a wooden table? the wooden arm of a chair?), is it at an angle? The additional visual context -what type of scene are we in -a living room, a coffeehouse? What else can I specifically encode that helps me reconstruct this cup in high-fidelity? The edges of the cup, its texture, the way the light is reflecting off of it in this particular visible context? Conditioning on a language description both simplifies and focuses what I need to represent. My encoded features are no longer general enough to cover the full range of objects that could follow from the visible context alone; instead, I can use that same capacity to represent this specific context, as denoted by language. The encoder can focus on all of things left unspecified by language -arguably, the very things we want a visual encoder for robotics to represent. Because we know that it is a \"black coffee mug, \" we can encode features around different types of black coffee mugs as a first level, and at a second level, go deeper, and actually model the low-level features that are not tied to semantics, but tied to core, perceptual primitives: the texture of the mug, the edges/boundaries of the object relative to other objects, even the way light reflects off of the surface. These are the features that help in tasks like grasp affordance prediction (the edges of objects), and when we learn joint representations of language and vision, the features that help with localization (grounding referring expressions) and detection. Though speculative, we can attempt to make this concrete with results: if language is indeed reducing the space over plausible reconstructions (and focusing the encoder), we might expect lower reconstruction error when language-conditioning vs. when we condition solely on the visual context alone. This is exactly what we show in §C.1, and a hint at why Voltron is able to perform so strongly downstream (even without language input). The simple presence of language during pretraining refocuses the features in our representations.",
        "Q4. Why only evaluate frozen representations? Why not fully finetune the backbones for each downstream evaluation?",
        "Both MVP and R3M  [Radosavovic et al. 2022; Nair et al. 2022]  only evaluate frozen visual representations, following a precedent set by a long tradition of work in self-supervised learning from the computer vision community  [Chen et al. 2020; Radford et al. 2021; He et al. 2022 ]. There are two reasons for the validity of evaluating frozen representations. First, the hope is that evaluating frozen representations (via adapted per-evaluation \"heads\" on top) help us isolate the relative impact of what the representations contain -otherwise, the separation between the standalone representations and the downstream evaluation parameters (and the co-mingling of the two when optimizing all weights via gradient descent) becomes much less clear. Second, for many of the evaluations we look at, we have extremely small amounts of data -on the order of 1000 examples for grasp affordance prediction, 10 -20 demonstrations for single task and language-conditioned imitation. There is a valid fear that full-finetuning the sizeable visual encoders vs. just the adaptation parameters (< 50K parameters) could lead to extreme overfitting. In general, finetuning large-scale Transformers from minimal data is an active area of research in and of itself, with work like adapters, low-rank approximations, and partial finetuning  [Houlsby et al. 2019; Hu et al. 2021; Ben-Zaken et al. 2022 ].",
        "Q5. Assuming pretraining datasets of (video, language) pairs feels restrictive; is there a way to leverage other sources of data?",
        "While Voltron expects a dataset of videos and associated language narrations, there is a wealth of visually diverse and relevant data that does not subscribe to this type signature:: datasets of standalone images from curated datasets  [Deng et al. 2009; Geiger et al. 2012; Yu et al. 2015] , curated images paired with language captions as in Conceptual Captions  [Lin et al. 2014; Sharma et al. 2018] , and large in-the-wild datasets of images paired with text scraped from the internet  [Schuhmann et al. 2021; Srinivasan et al. 2021; Schuhmann et al. 2022] .",
        "Luckily (though beyond the scope of this initial work), incorporating this data into the existing Voltron learning pipeline is straightforward; for image data without language, we can simply \"annotate\" each example with an empty <NULL> token in the worst case, or alternatively, with some minimal textual metadata (e.g., a class label, dataset descriptor, or even a URL if available). To accommodate for training on variable length image contexts, a naive solution would be adopting frame dropout or padding; there are myriad ways to do this efficientlyfrom Perceiver-based resampling of large patch sequences  [Jaegle et al. 2021; Alayrac et al. 2022]  to different position encoding schemes  [Su et al. 2021; Press et al. 2022] , to more efficient attention variants  [Beltagy et al. 2020 ].",
        "Later in §6, we raise the question: why generative (autoregressive) language modeling over masked language modeling? To help contextualize this choice, we look at recent work on combining masked autoencoders (for vision) with masked language modeling (for text), through multimodal masked autoencoders [M3AE;  Geng et al. 2022 ]. We reimplement this M3AE model, pretraining on the same Sth-Sth dataset used throughout this work, following the same standard of quality as for R-MVP and R-R3M. When we evaluate the corresponding R-M3AE model, we notice substantially worse performance across all evaluation domains; in the main text we attributed this to overfitting during pretraining -here, we provide that concrete evidence.",
        "Figure  11  shows the language model perplexity over time for both the R-M3AE, and the V -Gen model (trained with 𝛼 = 0.5). Perplexity (PPL) = exp(NLL) is a monotonic function of the cross-entropy loss; lower values are \"better\" with a lower bound value of 1.0. Almost immediately, the R-M3AE model overfits to the masked language modeling task, hitting a \"perfect\" perplexity of 1 (loss of 0.0) within the first 20 epochs. Contrast this with V -Gen that learns to gradually lower perplexity of the entire course of training, almost driving down to a PPL of 1.0 by the 400th epoch. We attribute R-M3AE's poor performance to this extremely early overfitting of the language loss, again echoing the hypothesis that language generation is slightly more robust to these settings -predict short language captions given visual context -than a masked language modeling objective. We note that this pretraining data (Sth-Sth) is significantly different than the data used to train the original M3AE model in  Geng et al. [2022] ; the original M3AE work used Conceptual Captions 12M  [Sharma et al. 2018 ], a rich dataset of images paired with long, descriptive captions. Further work on extending M3AE models as in  Liu et al. [2022]  further pretrain on text-only datasets such as Wikipedia and Toronto Books  [Devlin et al. 2019]  suggesting the need for diverse, broad coverage text when training (multimodal) masked language models."
      ]
    }
  },
  {
    "paperId": "65438e0ba226c1f97bd8a36333ebc3297b1a32fd",
    "title": "Reinforcement learning in robotics: A survey",
    "sections": {
      "Introduction": [
        "A remarkable variety of problems in robotics may be naturally phrased as ones of reinforcement learning. Reinforcement learning (RL) enables a robot to autonomously The OBELIX robot is a wheeled mobile robot that learned to push boxes  (Mahadevan and Connell, 1992)  with a value function-based approach (Picture reprint with permission of Sridhar Mahadevan). (b) A Zebra Zero robot arm learned a peg-in-hole insertion task  (Gullapalli et al., 1994)  with a model-free policy gradient approach (Picture reprint with permission of Rod Grupen). (c) Carnegie Mellon's autonomous helicopter leveraged a model-based policy search approach to learn a robust flight controller  (Bagnell and Schneider, 2001) . (d) The Sarcos humanoid DB learned a pole-balancing task  (Schaal, 1996)  using forward models (Picture reprint with permission of  Stefan Schaal) .",
        "discover an optimal behavior through trial-and-error interactions with its environment. Instead of explicitly detailing the solution to a problem, in reinforcement learning the designer of a control task provides feedback in terms of a scalar objective function that measures the one-step performance of the robot. Figure  1  illustrates the diverse set of robots that have learned tasks using reinforcement learning. Consider, for example, attempting to train a robot to return a table tennis ball over the net  (Muelling et al., 2012) . In this case, the robot might make an observations of dynamic variables specifying ball position and velocity and the internal dynamics of the joint position and velocity. This might in fact capture well the state s of the systemproviding a complete statistic for predicting future observations. The actions a available to the robot might be the torque sent to motors or the desired accelerations sent to an inverse dynamics control system. A function π that generates the motor commands (i.e., the actions) based on the incoming ball and current internal arm observations (i.e., Figure  2 : A illustration of the inter-relations between well-studied learning problems in the literature along axes that attempt to capture both the information and complexity available in reward signals and the complexity of sequential interaction between learner and environment. Each problem subsumes those to the left and below; reduction techniques provide methods whereby harder problems (above and right) may be addressed using repeated application of algorithms built for simpler problems.  (Langford and Zadrozny, 2005)  the state) would be called the policy. A reinforcement learning problem is to find a policy that optimizes the long term sum of rewards R(s, a); a reinforcement learning algorithm is one designed to find such a (near)-optimal policy. The reward function in this example could be based on the success of the hits as well as secondary criteria like energy consumption.",
        "In reinforcement learning, an agent tries to maximize the accumulated reward over its life-time. In an episodic setting, where the task is restarted after each end of an episode, the objective is to maximize the total reward per episode. If the task is on-going without a clear beginning and end, either the average reward over the whole life-time or a discounted return (i.e., a weighted average where distant rewards have less influence) can be optimized. In such reinforcement learning problems, the agent and its environment may be modeled being in a state s ∈ S and can perform actions a ∈ A, each of which may be members of either discrete or continuous sets and can be multi-dimensional. A state s contains all relevant information about the current situation to predict future states (or observables); an example would be the current position of a robot in a navigation task 1  . An action a is used to control (or change) the state of the system. For example, in the navigation task we could have the actions corresponding to torques applied to the wheels. For every step, the agent also gets a reward R, which is a scalar value and assumed to be a function of the state and observation. (It may equally be modeled as a random variable that depends on only these variables.) In the navigation task, a possible reward could be designed based on the energy costs for taken actions and rewards for reaching targets. The goal of reinforcement learning is to find a mapping from states to actions, called policy π, that picks actions a in given states s maximizing the cumulative expected reward. The policy π is either deterministic or probabilistic. The former always uses the exact same action for a given state in the form a = π(s), the later draws a sample from a distribution over actions when it encounters a state, i.e., a ∼ π (s, a) = P (a|s). The reinforcement learning agent needs to discover the relations between states, actions, and rewards. Hence exploration is required which can either be directly embedded in the policy or performed separately and only as part of the learning process.",
        "Classical reinforcement learning approaches are based on the assumption that we have a Markov Decision Process (MDP) consisting of the set of states S, set of actions A, the rewards R and transition probabilities T that capture the dynamics of a system.",
        "Transition probabilities (or densities in the continuous state case) T (s , a, s) = P (s |s, a) describe the effects of the actions on the state. Transition probabilities generalize the notion of deterministic dynamics to allow for modeling outcomes are uncertain even given full state. The Markov property requires that the next state s and the reward only depend on the previous state s and action a  (Sutton and Barto, 1998) , and not on additional information about the past states or actions. In a sense, the Markov property recapitulates the idea of state -a state if a sufficient statistic for predicting the future, rendering previous observations irrelevant. In general in robotics, we may only be able to find some approximate notion of state.",
        "Different types of reward functions are commonly used, including rewards depending only on the current state R = R(s), rewards depending on the current state and action R = R(s, a), and rewards including the transitions R = R(s , a, s). Most of the theoretical guarantees only hold if the problem adheres to a Markov structure, however in practice, many approaches work very well for many problems that do not fulfill this requirement."
      ],
      "Results": [
        "The first run on the real robot used the demonstration shown in Figure  8  and directly worked without any further parameter tuning. For the five runs with this demonstration, which took approximately one hour each, the robot got the ball into the cup for the first time after 42-45 episodes and regularly succeeded at bringing the ball into the cup after 70-80 episodes. The policy always converged to the maximum after 100 episodes. Running the real robot experiment was tedious as the ball was tracked by a stereo vision system, which sometimes failed and required a manual correction of the reward. As the string frequently entangles during failures and the robot cannot unravel it, human intervention is required. Hence, the ball had to be manually reset after each episode.",
        "If the constraint of getting the ball above the rim of the cup for the initial policy is fulfilled, the presented approach works well for a wide variety of initial demonstrations including various teachers and two different movement strategies (swinging the ball or pulling the ball straight up). Convergence took between 60 and 130 episodes, which largely depends on the initial distance to the cup but also on the robustness of the demonstrated policy.  Nemec et al. (2010)  employ an alternate reinforcement learning approach to achieve the ball-in-a-cup task with a Mitsubishi PA10 robot. They decomposed the task into two sub-tasks, the swing-up phase and the catching phase. In the swing-up phase, the ball is moved above the cup. In the catching phase, the ball is caught with the cup using an analytic prediction of the ball trajectory based on the movement of a flying point mass. The catching behavior is fixed; only the swing-up behavior is learned. The paper proposes to use SARSA to learn the swing-up movement. The states consist of the cup positions and velocities as well as the angular positions and velocities of the ball. The actions are the accelerations of the cup in a single Cartesian direction. Tractability is achieved by discretizing both the states (324 values) and the actions (5 values) and initialization by simulation. The behavior was first learned in simulation requiring 220 to 300 episodes. The state-action value function learned in simulation was used to initialize the learning on the real robot. The robot required an additional 40 to 90 episodes to adapt the behavior learned in simulation to the real environment."
      ],
      "Discussion": [
        "We have surveyed the state of the art in robot reinforcement learning for both general reinforcement learning audiences and robotics researchers to provide possibly valuable insight into successful techniques and approaches.",
        "From this overview, it is clear that using reinforcement learning in the domain of robotics is not yet a straightforward undertaking but rather requires a certain amount of skill. Hence, in this section, we highlight several open questions faced by the robotic reinforcement learning community in order to make progress towards \"off-the-shelf\" approaches as well as a few current practical challenges. Finally, we try to summarize several key lessons from robotic reinforcement learning for the general reinforcement learning community."
      ]
    }
  },
  {
    "paperId": "b78626ce1a562c05b1c06f9c805e839f9760b9ab",
    "title": "A Theory for Multiresolution Signal Decomposition: The Wavelet Representation",
    "sections": {
      "Introduction": [
        "In the physical world, the important structures to recognize are of very different sizes. Furthermore, depending on the distance to the focal plane of a video camera, these elements will appear at different scales on the image. In computer vision we would like to analyze each structure and at the same time process the minimum amount of details necessary to recognize them. One strategy is to process gradually the details with an increasing resolution until the recognition is achieved. For this purpose Witkin  [28]  has created the scale space representation in which the image appears at several resolutions (or scales) ( r , ) ~~, a (rj < T , + ~) . With this representation, after analyzing the image at a given resolution r, , for more details we have to process the image at a higher resolution r,+l . The images at resolutions r, and r;+l give a lot of redundant information. It would be more efficient to process only the additional details available at the resolution r,+l , which we shall call the detail signal at resolution r,+l . For this purpose we will define a new discrete representation called the wavelet representation which provides a coarse approximation of the signal (resolution ro) plus the detail signals at the successive resolutions r; for l l j < N .",
        "The concepts of scale and resolution are interdependent in computer vision. The resolution of an object image gives the minimum size of the object details which will be distinguishable in the image. It is a function of the number of photoreceptors which have measured the light reflected by the object. As there is a finite number of photoreceptors per unit area in the camera focal plane, the resolution will thus depends on the size (or scale) of the projection of the object on this plane. In this paper we will first model the scaling transform which modifies the resolution of the image components. This will then enable us to mathematically define the detail signal at each resolution through a new interpretation of the wavelet theory. Wavelets are particular functions studied by the mathematician Y.  Meyer [19]  which allow us to build interesting orthonormal bases of L2(Rn) . We will show that the detail signal can be computed by decomposing the original signal on such an orthonormal basis and that it can be efficiently calculated with a pyramid architecture using quadrature mirror filters. The wavelet representation is complete and we will give a similar algorithm for reconstructing the original signal from its decomposition. Like Marr's  [18]  model the wavelet representation can be interpreted as a decomposition of the original signal into a set of independent orientation selective frequency channels. For two-dimensional signals such as images we will present a separable decomposition which privileges the horizontal and vertical orientations. However the mathematical model enables us to build non-separable representations with as many orientation tunings as desired.",
        "The wavelet representation has many applications in computer vision and signal processing in general. We will describe in particular how it can be used for signal matching , data compression , edge detection , texture discrimination and fractal analysis. Finally, the wavelet representation will be compared with the DOG and Gabor representations currently used in computer vision and having some similar features. We have tried in this paper to give the mathematical insights of the model without justifying in details all the results. The main steps of the theorem proofs are given in the appendices."
      ],
      "Conclusion": [
        "In this paper we have described a mathematical model which enables us to understand the concept of resolution and how it relates to scale. We have seen that it is possible to compute the difference of information between different resolutions and thus define a new complete representation called the wavelet representation. It corresponds to an expansion of the original continuous signal in a wavelet orthonormal basis but can also be interpreted as a decomposition in a set of independent frequency channels with orientation tunings. The wavelet functions can be well localized both in the spatial and frequency domains so that this decomposition gives an intermediate representation between both domains. The representation is not redundant because the wavelet functions are orthogonal; it thus keeps constant the number of pixels to code the image. The wavelet representation can be efficiently implemented with a pyramid architecture using quadrature mirror filters and the original signal can also be reconstructed with a similar architecture. The numerical stability is well illustrated by the quality of the reconstruction.",
        "The orientation selectivity of this representation is useful for many applications. We have discussed in particular the application of the wavelet representation to signal matching, data compression, edge detection, texture discrimination and fractal analysis. Computer vision applications have been emphasized but this representation can also be helpful for pattern recognition in other domains. Grossmann and Kronland-Martinet [14] are currently working on speech recognition and J.Morlet on seismic signal analysis. The wavelet orthonormal bases are also studied both in pure and applied mathematics and have found some applications in Quantum Mechanics with the work of T. Paul [23] and P. Federbush  [6] ."
      ]
    }
  },
  {
    "paperId": "44dc896ec1d1abf65a88a8f076515d26b54c6047",
    "title": "Universally composable security: a new paradigm for cryptographic protocols",
    "sections": {
      "Introduction": [
        "Rigorously demonstrating that a protocol \"does its job securely\" is an essential component of cryptographic protocol design. Doing so requires coming up with an appropriate mathematical model for representing protocols, and then formulating, within that model, a definition of security that captures the requirements of the task at hand. Once such a definition is in place, we can show that a protocol \"does its job securely\" by demonstrating that its mathematical representation satisfies the definition of security within the devised mathematical model.",
        "However, devising a good mathematical model for representing protocols, and even more so formulating adequate definitions of security within the devised model, turns out to be a tricky business. First, the model should be rich enough to represent all realistic adversarial behaviors, as well as the plethora of prevalent design techniques for distributed systems and networks. Next, the definition should guarantee that the intuitive notion of security is captured with respect to any adversarial behavior under consideration.",
        "One main challenge in formulating the security of cryptographic protocols is capturing the threats coming from the execution environment, and in particular potential \"bad interactions\" with other protocols that are running in the same system or network. Another, related challenge is the need to come up with notions of security that allow for modular design of cryptographic protocols and applications from simpler building blocks in a way that guarantees overall security. Addressing these challenges is the focal point of this work.",
        "Initial definitions of security for specific cryptographic tasks (e.g.  [gm84, gmra89] ) considered models which capture only a single execution of the analyzed protocol. This is indeed a good choice for first-cut definitions of security. In particular, it allows for relatively concise and intuitive problem statement, and for simpler analysis of protocols. However, in many cases it turned out that these initial definitions were insufficient when used within contexts and applications of interest. Examples include Encryption, where the basic notion of semantic security  [gm84]  was later augmented with several flavors of security against chosen ciphertext attacks  [ny90, ddn00, rs91, bdpr98]  in order to address general protocol settings; Commitment, where the original notions were later augmented with some flavors of non-malleability  [ddn00, dio98, ff00]  and equivocation  [bcc88, b96]  in order to address the requirement of some applications; Zero-Knowledge protocols, where the original notions  [gmra89]  were shown not to be closed under composition, and new notions and constructions were needed  [go94, gk89, f91, dns98, rk99, bggl04] ; Key Exchange, where the original notions allow protocols that fail to provide secure communication, even when combined with secure symmetric encryption and authentication protocols  [br93, bck98, sh99, ck01, gl01] ; Oblivious Transfer  [r81, egl85, gm00]  and secure multiparty funtion evaluation  [gl90, b91, mr91, pw00, c00, dm00, g04]  where the first definitions do not guarantee security under concurrent composition.",
        "One way to capture the security concerns that arise in a specific protocol environment or in a given application is to directly represent the given environment or application within an extended definition of security. Such an approach is taken, for instance in the cases of key-exchange  [br93, ck01, gl01] , non-malleable commitments  [ddn00] , concurrent zero-knowledge  [dns98] , and general concurrently secure protocols  [p04, bs05]  where the definitions explicitly model several adversarially coordinated sessions of the protocol in question. This approach, however, results in definitions with ever-growing complexity, and whose scope is inherently limited to specific environments and concerns.",
        "An alternative approach, taken in this work, is to use definitions that consider the protocol in isolation, but guarantee secure composition. In other words, here definitions of security refer only to a single session of the protocol \"in vitro\". Security \"in vivo\", namely in more realistic settings where a protocol session may run concurrently with other protocols, is obtained by formulating the definitions of security in a way that guarantees the preservation of security under a general composition operation on protocols. This approach considerably simplifies the process of formulating a definition of security and analyzing protocols. Furthermore, it guarantees security in arbitrary protocol environments, even ones which have not been explicitly considered.",
        "In order to make such an approach meaningful, we first need to have a general framework for representing cryptographic protocols and their security properties. Indeed, otherwise it is not clear what \"preserving security when running alongside other protocols\" means, especially when these other protocols and their security properties are arbitrary. Several general definitions of secure protocols were developed over the years, e.g.  [gl90, mr91, b91, bcg93, pw94, c00, hm00, psw00, dm00, pw00] . These definitions are obvious candidates for such a general framework. However, the composition operations considered in those works fall short of guaranteeing general secure composition of cryptographic protocols, especially in settings where security holds only for computationally bounded adversaries and multiple protocol sessions may be running concurrently in an adversarially coordinated way. We further elaborate on these works and their relation to the present one in Appendix A.",
        "This work proposes yet another framework for representing and analyzing the security of cryptographic protocols. Within this framework, we formulate a general methodology for expressing the security requirements of cryptographic tasks. Furthermore, we define a general formal operation for composing protocols, and show that notions of security expressed within this framework preserve security under this composition operation. We call this composition operation universal composition and say that definitions of security in this framework (and the protocols that satisfy them) are universally composable (UC). Consequently, we dub this framework the UC security framework.  1  As we shall see, the fact that security in this framework is preserved under universal composition implies that a secure protocol for some task remains secure even it is running in an arbitrary and unknown multi-party, multi-execution environment. In particular, this implies significantly stronger and more general variants of some standard security concerns, such as non-malleability and security under concurrent composition: Here security is preserved even with respect to an unbounded number of sessions of either the same protocol or other protocols.",
        "A fair number of frameworks for defining security of protocols in a way that guarantees securitypreserving composition have been proposed since the first publication of this work  [c01] . Many of these works are influenced by this work, and many of them influenced later versions of this work, this one included. Here let us mention only  [n03, ps04, w05, k06, bpw07, cdpw07, mr11, kt13, mt13, hs11, ccl15] . Specific influences are mentioned when relevant.",
        "The rest of the introduction is organized as follows. Section 1.1 presents the basic definitional approach and the ideas underlying the formalism. Section 1.2 presents the universal composition operation and theorem. Section 1.3 discusses the issues associated with instantiating the general approach within a framework that is both expressive and usable. Related work, including both prior work and work that was done following the publication of the first version of this work, is reviewed in Appendix A."
      ],
      "Discussion": [
        "We further discuss the model of computation and compare it with other general models in the literature that are aimed at capturing distributed computation with concurrently running processes -some of which explicitly aim at modeling security of protocols. A very incomplete list of such models includes the CSP model of Hoare  [h85] , the CCS model and π-calculus of Milner  [m89, m99]  (that is based on the λ-calculus as its basic model of computation), the spi-calculus of Abadi and Gordon  [ag97]  (that is based on the π-calculus), the framework of Lincoln et al.  [lmms98]  (that uses the functional representation of probabilistic polynomial time from  [mms98] ), the I/O automata of Merritt and Lynch  [ly96] , the probabilistic I/O automata of  Lynch, Segala and Vaandrager [sl95, lsv03] , the Abstract Cryptography model of  Maurer and Renner [mr11] , and the equational approach of  Micciancio and Tessaro [mt13] . (Other approaches are mentioned in the Appendix.)",
        "We partition the discussion to four parts: The use of ITMs, the use of identities, the externalwrite mechanism, and the modeling of resource-bounded computation. It is stressed however that the partitioning is somewhat arbitrary and all topics are of course inter-related.",
        "Some aspects of the universal composition theorem were discussed in Section 2.3. This section highlights additional aspects, and presents some extensions of the theorem.",
        "On composability with respect to closed environments. Recall that the closed-environment variant of the definition of emulation (Definition 9) considers only environments that take external input that contains no information other than its import. We note that the UC theorem still holds even for this variant, with the same proof.",
        "Composing multiple different protocols. The composition theorem (Theorem 22) is stated only for the case of replacing sessions of a single protocol φ with sessions of another protocol. The theorem holds also for the case where multiple different protocols φ 1 , φ 2 , ... are replaced by protocols π 1 , π 2 , ..., respectively. (This can be seen either by directly extending the current proof, or by defining a single \"universal\" protocol that mimics multiple different ones.)",
        "Nesting of protocol sessions. The universal composition operation can be applied repeatedly to perform \"nested\" replacements of calls to sub-protocols with calls to other sub-protocols. For instance, if a protocol π 1 UC-emulates protocol φ 1 , and protocol π 2 UC-emulates protocol φ 2 using calls to φ 1 , then for any protocol ρ that uses calls to φ 2 it holds that the composed protocol ρ φ 2 →π φ 1 →π 1 2 = uc(ρ, uc(π 2 , π 1 , φ 1 ), φ 2 ) UC-emulates ρ. Recall that the UC theorem demonstrates that the simulation overhead grows under composition only by an additive factor that depends on the protocols involved. This means that security is preserved even if the nesting has polynomial depth (and, consequently, the UC theorem is applied polynomially many times).",
        "The fact that the UC theorem extends to arbitrary polynomial nesting of the UC operation was independently observed in  [bm04]  for their variant of the UC framework.",
        "Beyond PPT. The UC theorem is stated and proven for PPT systems of ITMs, namely for the case where all the involved entities are PPT. It is readily seen that the theorem holds also for other classes of ITMs and systems, as long as the definition of the class guarantees that any execution of any system of ITMs can be \"simulated\" on a single ITM from the same class.",
        "More precisely, say that a class C of ITMs is self-simulatable if, for any system (I, C) of ITMs where both I and C (in its ITM representation) are in C, there exists an ITM µ in C such that, on any input and any random input, the output of a single session of µ equals the output of (I, C). (Stated in these terms, Proposition 7 on page 29 asserts that for any super-additive function T (), the class of ITMs that run in time T () is self-simulatable.)",
        "Say that protocol π UC-emulates protocol φ with respect to class C if Definition 9 holds when the class of PPT ITMs is replaced with class C, namely when π, A, S, and E are taken to be ITMs in C. Then we have:",
        "Proposition 25 Let C be a self-simulatable class of ITMs, and let ρ, π, φ be protocols in C such that π UC-emulates φ with respect to class C. Then protocol ρ φ→π UC-emulates protocol ρ with respect to class C.",
        "It is stressed, however, that the UC theorem is, in general, false in settings where systems of ITMs cannot be simulated on a single ITM from the same class. We exemplify this point for the case where all entities in the system are bound to be PPT, except for the protocol φ which is not PPT.  23  More specifically, we present an ideal functionality F that is not PPT, and a PPT protocol π that UC-realizes F with respect to PPT environments. Then we present a protocol ρ, that calls two sessions of the ideal protocol for F, and such that ρ F →π does not UC-emulate ρ. In fact, for any PPT π we have that ρ F →π does not emulate ρ.",
        "In order to define F, we first recall the definition of pseudorandom ensembles of evasive sets, defined in  [gk89]  for a related purpose. An ensemble S = {S k } k∈N where each",
        "evasive, that is for any non-uniform PPT algorithm A and for any z ∈ {0, 1} * , we have that Prob",
        "where k = |z|. It is shown in  [gk89] , via a counting argument, that pseudorandom evasive set ensembles exist. Now, define F as follows. F uses the ensemble S and interacts with one ITI only. Given security parameter k, it first chooses i R ← {0, 1} k and outputs i. Then, given an input (x, i",
        "It is easy to see that π UC-realizes F: Since S is evasive, then the probability that the input x is in the set s k,i is negligible, thus F outputs success only with negligible probability. Furthermore, F outputs a pseudorandom k-bit value, which is indistinguishable from the output of π. Now, consider the following F-hybrid protocol ρ. ρ runs two sessions of F, denoted F 1 and F 2 . Upon invocation with security parameter k, it activates F 1 and F 2 with k, and obtains the indices i 1 and i 2 . Next, it chooses x 1 R ← {0, 1} k , and feeds (x 1 , i 2 ) to F 1 . If F 1 outputs success then ρ outputs success and halts. Otherwise, π feeds the value x 2 obtained from F 1 to F 2 . If F 2 outputs success then ρ outputs success; otherwise it outputs fail. It is easy to see that ρ always outputs success. However, ρ F →π never outputs success. In fact, the separation is stronger: F any PPT protocol π that UC-realizes F, we have that ρ F →π outputs success only with negligible probability.",
        "As discussed earlier, the basic model of computation provides no explicit mechanism for modeling communication over a network. It also provides only a single, limited mechanism for scheduling processes in a distributed setting, and no explicit mechanism for expressing adversarial control over, or infiltration of, computational entities. It also does not provide explicit ways to express leakage of information from computing devices. Indeed, the bare model does not immediately provide natural ways to represent realistic protocols, attacks, or security requirements.",
        "This section puts forth mechanisms for capturing realistic protocols, attacks and security requirements, by way of setting conventions on top of the basic model of Section 4.1. It also formulates a number of basic ideal functionalities that capture common abstractions, or models of communication; As motivated in the introduction, these abstract models allow composing protocols that use the ideal functionality as an abstract model with protocols that realize the functionality using less abstract modeling, while preserving overall security.",
        "In addition to capturing some specific conventions and ideal functionalities, this section exemplifies how the basic model can be used as a platform for more fine-tuned and expressive models. It also provides a general toolbox of techniques for writing ideal functionalities that capture other situations, concerns, and guarantees.",
        "Section 7.1 presents a mechanism for expressing various forms of party corruption, namely modeling situations where computational entities deviate from their prescribed protocol in a potentially adversarial way. Section 7.2 presents some useful conventions for writing ideal functionalities.",
        "Section 7.3 then presents ideal functionalities that capture some commonplace abstract models of communication, specifically authenticated, secure, and synchronous communication. Finally, Section 7.4 presents an ideal functionality that captures non-concurrent protocol execution."
      ],
      "Related Work": [
        "This section surveys some related work. For brevity, we concentrate on works that led to the present framework or directly affect it. This includes works that affect the first version (from December 2000), as well as works that influenced subsequent revisions. Still, we omit many works that use this framework, study it and extend it. The review sections in  [c01, c06, c07, c08, c13]  cover some of these works. For simplicity of exposition we mostly present the works in a rough chronological order rather than in thematic order. Also we concentrate on contributions to the definitional aspects cryptographic protocols rather than protocol design (although the two naturally go hand in hand).",
        "Prior work. Two works that laid the foundations of general notions of security for cryptographic protocols are the work of Yao  [y82] , which explicitly expressed the need for a general \"unified\" framework for expressing the security requirements of secure computation protocols, and the work of  Goldreich, Micali and Wigderson [gmw87]  which put forth the \"trusted-party paradigm\", namely the approach of defining security via comparison with an ideal process involving a trusted party (albeit in a very informal way).",
        "Another work that greatly influenced the UC framework is the work of  Dolev, Dwork and Naor [ddn00] . This work points out some important security concerns that arise when cryptographic protocols run concurrently within a larger system. In particular, making sure that the concerns pointed out in  [ddn00]  are addressed is central to the present framework.",
        "The first rigorous general definitional framework for secure protocols is due to  Goldwasser and Levin [gl90] , and was followed shortly by the frameworks of  Micali and Rogaway [mr91]  and Beaver  [b91] . In particular, the notion of \"reducibility\" in  [mr91]  directly underlies the notion of protocol composition in many subsequent works including the present one. Beaver's framework was the first to directly formalize the idea of comparing a run of a protocol to an ideal process. (However, the  [mr91, b91]  formalisms only address security in restricted settings; in particular, they do not deal with computational issues.)  [gl90, mr91, b91]  are surveyed in  [c00]  in more detail.",
        "The frameworks of  [gl90, mr91, b91]  concentrate on synchronous communication. Also, although in  [gmw87]  the trusted-party paradigm was put forth for reactive functionalities, the three frameworks concentrate on the task of secure function evaluation. An extension to asynchronous communication networks with eventual message delivery is formulated in  [bcg93] . A system model and notion of security for reactive functionalities is sketched in Pfitzmann and Waidner  [pw94] .",
        "The first ideal-process based definition of computational security against resource bounded adversaries is given in  [c95] . In  [c00]  the framework of  [c95]  is strengthened to handle secure composition. In particular,  [c00]  defines a general composition operation, called modular composition, which is a non-concurrent version of universal composition. That is, only a single protocol session can be active at any point in time. (See more details in Section 7.4.) In addition, security of protocols in that framework is shown to be preserved under modular composition. A closely related formulation appears in  [g04, Section 7.7.2] .",
        "[c00] also sketches how to strengthen the definition there to support concurrent composition. The UC framework implements these sketches in a direct way.",
        "The framework of  Hirt and Maurer [hm00]  provides a rigorous treatment of reactive functionalities.  Dodis and Micali [dm00]  build on the definition of  Micali and Rogaway [mr91]  for unconditionally secure function evaluation, which is specific to the setting where the communication between parties is ideally private. In that setting, they prove that their notion of security is preserved under a general concurrent composition operation similar to universal composition. They also formulate an additional composition operation (called synchronous composition) that provides stronger security guarantees, and show that their definition is closed under that composition operation in cases where the scheduling of the various sessions of the protocols can be controlled. However, it is not clear how to extend their definition and modeling to settings where the adversary has access to the communication between honest parties.  Lincoln, Mitchell, Mitchell and Scedrov [lmms98, lmms99]  develop a process calculus, based on the π-calculus of Milner  [m89, m99] , that incorporates random choices and computational limitations on adversaries. (In  [mms98]  it is demonstrated how to express probabilistic polynomial time within such a process calculus.) In that setting, their definitional approach has a number of similarities to the simulation-based approach taken here: They define a computational variant of observational equivalence, and say that a real-life process is secure if it is observationally equivalent to an \"ideal process\" where the desired functionality is guaranteed. This is indeed similar to requiring that no environment can tell whether it is interacting with the ideal process or with the protocol execution. However, their ideal process must vary with the protocol to be analyzed, and they do not seem to have an equivalent of the notion of an \"ideal functionality\" which is associated only with the task and is independent of the analyzed protocol. This makes it harder to formalize the security requirements of a given task.",
        "The modeling of randomized distributed computation in an asynchronous, event-driven setting is an important component of this work. Works that considerably influenced the present modeling include  Chor and Moscovici [cm89] ,  Chor and Nelson [cn99] , Bird et al. [B + 91], and Canetti and Krawczyk  [ck01] .",
        "Concurrent work. The framework of  Pfitzmann, Schunter and Waidner [psw00, pw00]  is the first to rigorously address concurrent universal composition in a computational setting. (This work is based on the sketches in  [pw94] ). They define security for reactive functionalities in a synchronous setting and prove that security is preserved when a single session of a subroutine protocol is composed concurrently with the calling protocol. An extension of the  [psw00, pw00]  framework to asynchronous networks appears in  [pw01] .",
        "At high level, the notion of security in  [psw00, pw00, pw01] , called reactive simulatability, is similar to the one here. In particular, the role of their \"honest user\" can be roughly mapped to the role of the environment as defined here. However, there are several differences. They use a finite-state machine model of computation that builds on the I/O automata model of  [ly96] , as opposed to the ITM-based model used in this work. Their model provides a rich set of methods for scheduling events in an execution. Still, they postulate a static system where the number of participants and their identities are fixed in advance (this is somewhat similar to the model of Section 2 in this work). In particular, the number of protocol sessions run by the parties is constant and fixed in advance, thus it is impossible to argue about the security of systems where the number of protocol sessions may be a non-constant function of the security parameter (even if this number is known in advance). Other technical differences include the notion of polynomial time computation (all entities are bounded by a fixed polynomial in the security parameter regardless of the input length -see discussion in Section 3.3.4), and scheduling of events. Subequent work.  Backes, Pfitzmann and Waidner [bpw04]  extend the framework of  [pw01]  to deal with the case where the number of parties and protocol sessions depends on the security parameter (still it is otherwise static as in  [pw00] ). In that framework, they prove that reactive simulatability is preserved under universal composition. The  [bpw07]  formulation returns to the original approach where the number of entities and protocol sessions is fixed irrespective of the security parameter.  Mateus, Mitchell and Scedrov [mms03]  and  Datta, Küsters, Mitchell, and Ranamanathan [dkmr05]  (see also  [d05] ) extend the  [lmms98, lmms99]  framework to express simulatability as defined here, cast in a process calculus for probabilistic polynomial time computation, and demonstrate that the universal composition theorem holds in their framework. They also rigorously compare certain aspects of the present framework (as defined in  [c01] ) and reactive simulatability (as defined in  [bpw07] ). Tight correspondence between the [mms03] notion of security and the one defined here is demonstrated in Almansa  [a04] . We note that all of these frameworks postulate a static execution model which is most similar to the one in Section 2.  Canetti et al. [c+05]  extend the probabilistic I/O automata of  Lynch, Segala and Vaandrager [sl95, lsv03]  to a framework that allows formulating security of cryptographic protocols along the lines of the present UC framework. This involves developing a special mechanism, called the task schedule, for curbing the power of non-deterministic scheduling; it also requires modeling resource-bounded computations. The result is a framework that represents the concurrent nature of distributed systems in a direct way, that allows for analyzing partially-specified protocols (such as, say, standards), that allows some scheduling choices to be determined non-deterministically during run-time, and at the same time still allows for meaningful UC-style security specifications.  Micciancio and Tessaro [mt13]  provide an alternative, simplified formalism for composable simulation-based security of protocol. The formalism, which is a generalization of  Kahn networks [k74] , allows for equational (rather than temporal) representation and analysis of protocols and their security.  Küsters and Tüngerthal [k06, kt13]  formulate an ITM-based model of computation that allows for defining UC-style notions of security. The model contains new constructs that facilitate both flexible addressing of messages and a flexible notion of resource-bounded computation in a distributed environment. This work also adapts abstract notations from the process calculus literature to an ITM-based model, allowing for succinct and clear presentation of composition theorems and proofs. In particular the accounting of the import of messages in this work is influenced by the modeling of  [k06] .",
        "Hofheinz, Müller-Quade and Unruh  [hmu09]  give an alternative definition of polynomial time ITMs (see discussion in Section 3.3.4).  Hofheinz and Shoup [hs11]  point to a number of flaws in previous versions of this work and formulate a variant of the UC framework that avoids these flaws. Their framework (called GNUC) differs from the present one in two main ways: First, their notion of polynomial time is close to that of  [hmu09] . Second, they mandate a more rigid subroutine structure for protocols, as well as a specific format for session IDs that represents the said subroutine structure. While indeed simplifying the argumentation on a natural class of protocols, the GNUC framework does not allow representing and arguing about other natural classes (see e.g. Footnote 25).",
        "Nielsen  [n03] , Hofheinz and Müller-Quade  [hm04a] , and  Katz et al. [kmtz13]  formulate synchronous variants of the UC framework.  Wikström [w05, w16] , as well as  Canetti, Cohen and Lindell [ccl15]  present simplified formulations of the UC framework, geared as simplifying the presentation and analysis of protocols in more \"standard\" multiparty computation settings."
      ]
    }
  },
  {
    "paperId": "4fbd174e80502f8f3c4b9f48054872b028e2445a",
    "title": "Large Language Models for Software Engineering: Survey and Open Problems",
    "sections": {
      "Introduction": [
        "This paper surveys the recent developments, advances and empirical results on LLM-based SE; the application of Large Language Models (LLMs) to Software Engineering (SE) applications. We use the survey to highlight gaps in this rapidly developing, but as yet embryonic, research literature. Based on gaps in the literature and technical opportunities, we also identify open problems and challenges for the software engineering research community.",
        "While any survey of such a rapidly expanding area can neither aspire nor claim to be comprehensive, we hope that this survey will provide a useful and relatively complete snapshot of the early universe of this exciting new subdiscipline of Software Engineering: LLM-based Software Engineering. Although the scientific and technical structure of the field is still emerging, it is already possible to identify trends, productive avenues for future research, and important technical challenges that need to be addressed.",
        "In particular, we are already able to discern important connections to (and resonance with) existing trends and wellestablished approaches and subdisciplines within Software Engineering. Furthermore, although we find considerable grounds for optimism, there remain important technical challenges, which are likely to inform the research agenda for several years. Many authors have highlighted, both scientifically and anecdotally, that hallucination is a pervasive problem for LLMs  [1]  and also that it poses specific problems for LLMbased SE  [2] . As with human intelligence, hallucination means that the LLM can create fictitious output. In the context of software engineering, it means that the engineering artefacts created could be incorrect, yet appear plausible; LLMs may introduce bugs.",
        "However, unlike many other applications of LLMs, software engineers are typically blessed with automatable ground truth (software execution), against which most software engineering artefacts can be evaluated. Also, the software engineering research community has already devoted a great deal of time to producing automated and semi-automated techniques for checking the potentially incorrect results produced by humans. This means that, for the discipline and the research community, there is a great deal of experience and expertise on which to draw, when tackling the challenges posed by issues like hallucination.",
        "Clearly, automated testing techniques  [3] -  [5]  will have a central role to play in ensuring correctness, just as they already do for human-engineered artefacts. When generating entirely new features and systems, automated test data generation suffers from the lack of an automatable oracle  [6]  (an automated technique for determining whether output behaviour is correct for a given input stimulus). Given LLMs' propensity to hallucinate, the Oracle Problem will remain highly relevant, and solutions to it will become all the more impactful  [7] .",
        "However, some SE applications concern adaption, improvement and development of existing software systems, for which there is a readily-available automatable oracle: the functional behaviour of the original system.",
        "In this paper, we call this the 'Automated Regression Oracle', an approach that has already proved advantageous in the field of Genetic Improvement  [8] . The Automated Regression Oracle simply uses the existing version of the software system as a reference against which to benchmark output from any subsequent adaptions and changes.",
        "Of course, there is a risk of 'baking in' functional incorrectness, since the Automated Regression Oracle cannot detect what the system should do, but only capture what it currently does. Therefore, the Automated Regression Oracle can test only for functional regressions so it is best suited to use cases where the existing functionality is to be maintained. For example, for non-functional improvements such as performance optimisation and for semantics-preserving refactoring.",
        "The input provided to an LLM will be a natural focus of growing research, and we can expect a rapid development of the literature on prompt engineering and prompt optimisation  [9] . In this survey, we highlight existing work and open challenges for prompt engineering with regard to several specific aspects of software engineering.",
        "The output from an LLM need not be confined purely to code, but can also include other software engineering artefacts, such as requirements, test cases, design diagrams, and documentation. In general, the language-based nature of an LLM, allows it to generate any linguistically-defined software engineering artefact.",
        "We typically think of the software engineering artefact as the primary output of the LLM, but it is not the only output. The explanation provided with the primary output is also an important output of any LLM. Our survey highlights the need for much more research, not only into optimising prompt engineering (which focuses on the input to the LLM) but also the need for work on the optimisation of explanations provided with the primary output.",
        "LLMs are inherently nondeterministic: the same prompt produces different answers on different inference executions (unless the temperature is set to zero, which has often been found to be suboptimal over multiple executions)  [10] . Furthermore, irrespective of the temperature setting, subtle changes in the prompt can lead to very different outputs  [10] . As well as motivating 'prompt engineering' and output processing, this nondeterministic behaviour raises challenges for the scientific evaluation of LLM-based Software Engineering:",
        "If results can vary each time we run the process, how can we determine whether a proposed technique achieves an advance over the state of the art? This is a problem that has already been well studied in the context of Empirical Software Engineering  [11]  and Search Based Software Engineering (SBSE)  [12] . In particular, SBSE bears many similarities to LLM-based Software Engineering, sharing with it the need to achieve robust scientific evaluation in the presence of noisy, non-deterministic, and incomplete results  [13] ,  [14] . There is, therefore, already a mature software engineering literature on just the kind of robust scientific evaluation techniques needed to cater for LLM-based scientific evaluation.",
        "For example, well-studied techniques, such as parametric and non-parametric inferential statistics, are now routinely used to provide robust scientific conclusions in the presence of highly non-deterministic algorithms in the SBSE discipline. In order to understand the growth trends within LLM-based Software Engineering, we performed a manual analysis of data on the number of publications on specific topics from arXiv. Table  I  contains the raw data  1  , which was manually extracted from the arXiv metadata dump made publicly available via Kaggle (https://www.kaggle.com/datasets/Cornell-University/ arxiv), accessed on the 27 th. July 2023. We first filtered out publications for which the classification code does not start with the cs prefix (i.e., Computer Science), resulting in column A.",
        "To identify Computer Science papers that are relevant to LLMs, we filtered the publications into subcategories on artificial intelligence (cs.AI), machine learning (cs.LG), neural and evolutionary computation (cs.NE), software engineering (cs.SE), and programming language (cs.PL) using the queries \"Large Language Model\", \"LLM\", and \"GPT\" in either the title or the abstract (we manually excluded instances of overloaded acronyms such as GPT for General Planning Tool), resulting in column L. Finally, we used the same queries to identify LLM-based Software Engineering papers in software engineering (cs.SE) and programming language (cs.PL). These queries are inherently approximate, so we confine ourselves only to conclusions based on overall trends for which there is strong evidence rather than specific details of the numbers observed. Nevertheless, we report the raw numbers observed to support replication by others.   LLMs\", we mean that either the title or the abstract of a preprint contains \"LLM\", \"Large Language Model\", or \"GPT\". The blue line denotes the percentage of the number of preprints about LLMs out of the number of all preprints in the CS category. The orange line denotes the percentage of the number of preprints about LLMs in cs.SE and cs.PL categories out of all preprints about LLMs Figure  2 , shows the growth in the number of arXivpublished papers on Computer Science (|A|, in Blue), and on LLMs (|L|, in orange). Those papers specifically on Software Engineering and LLMs are depicted in Green (|L ∩ S|). Given the rapid rise in overall publication volumes, we use a logarithmic scale for the vertical axis. Unsurprisingly, we see an overall rise in the number of CS publications. Also, given the recent upsurge in attention for LLMs, the exponential rise in the number of papers on LLMs is relatively unsurprising.",
        "Perhaps more interesting is the rapid uptake of Software Engineering applications of LLMs, as revealed by the growth trend, pictured in green on this figure. In order to examine this trend in more detail, we plot the proportion of LLM publications (L) to all CS publications (A) in blue, as well as the proportions of LLM-based software engineering publications (L ∩ S) to all LLM publications in orange in Figure  3 . As can be seen, the proportion of LLM papers on LLM-based Software Engineering has been rising dramatically since 2019. Already, more than 10% of all papers on LLMs are concerned with LLM-based Software Engineering.",
        "As a result of this growth, we can expect many other surveys of LLM-Based SE. The rapid expansion of the literature makes it unlikely that further comprehensive SE-wide studies will fit the space constraints of a single paper, but we can expect many specific comprehensive surveys of sub-areas of interest, and also Systematic Literature Reviews (SLRs) that tackle SE-wide crosscutting issues by asking specific research questions of the primary literature in the systematic review. Already, such SLRs are appearing. For example, Hou et al.  [15]  provided an excellent recent SLR covering 229 research papers from 2017 to 2023 reporting SE tasks tackled, data collection and preprocessing techniques, and strategies for optimising LLM performance (such as prompt engineering).",
        "The remainder of this paper is organised to follow the toplevel software development activities and research domains as depicted in Figure  1 ."
      ],
      "Results": [
        "There is a pressing need for more thorough scientific evaluation. Many authors have anecdotally reported on cases where LLMs failed to generate correct, secure, and reliable code. Poldrack et al.  [75]  also highlight the need for substantial human validation. In this section, we survey the literature on the empirical evaluation of LLM-based code generation in terms of correctness, robustness, explainability, determinism, and security.",
        "1) Correctness Evaluation: The GPT-4 Technical Report  [28]  evaluated the correctness of GPT-4's code generation on the HumanEval dataset, reporting a zero-shot accuracy of 67%, a modest improvement on the (earlier ChatGPT) results reported by Yetistiren et al.  [76] .",
        "Borji  [77]  presented a rigorous, categorised and systematic analysis of LLM code generation failures for ChatGPT. Eleven categories of failures, including reasoning, factual errors, mathematics, coding, and bias, are presented and discussed in their work.",
        "Figure  4  shows the leaderboard of code generation correctness in terms of the pass@1 (i.e., the test pass rate for the top-1 code candidate) on the HumanEval dataset according to Papers With Code, a platform that highlights trending AI research and the code behind the method and models.  4  The LLM models behind each method are shown in brackets. At the time of writing, the best code generation model, Reflexion  [78] , can generate correct code for over 90% of the generation tasks. However, these numbers and the relative rankings of different language models are inherently subject to change in such a rapidly developing field. For example, the figure given for correct code on HumanEval in the original GPT-4 Report  [28]  was only 67%, so the updated figure of 80% (at the time of writing, which is five months later) retrieved from the Papers-With-Code website presumably represents the evolution of GPT4 since then.",
        "Despite the promising results in the literature on code generation and completion, Din et al.  [79]  reported that the performance of code completion dropped by more than 50% on HumanEval when the context contains bugs. 2) Robustness Evaluation: LLM code generation robustness is the degree to which similar prompts elicit semantically and syntactically similar code generation. Treude  [80]  introduced GPTCOMPARE, a prototype tool for visually highlighting similarities and differences between LLM code outputs. Yan et al.  [81]  introduced COCO to test the robustness and consistency of LLM-based code generation systems.",
        "3) Explainability Evaluation: One considerable advantage of LLMs, over previous machine learning techniques, is the way in which the code generation artefacts are accompanied by explanations. Such explanations have the potential to increase adoption, by providing additional confidence and faster understanding. More work is needed to evaluate and optimise explanations that accompany generated code and other software engineering artefacts.",
        "Initial evaluation by MacNeil et al.  [82]  on their interactive Web development e-book, suggested that a majority of students perceived LLM-generated code explanations to be helpful. Noever and Williams  [83]  also showed the potential for explanations to help human engineers, particularly where code is obfuscated or lacks sufficient existing documentation. In this way, the ability to produce insight and explanation may go beyond simply justifying the code generated by the LLM itself, but may become a valuable source of education and documentation (See Section XI).",
        "Sun et al.  [84]  focus on users' explainability needs for generative AI in three software engineering use cases: code generation based on natural language description (with Copilot), translation between different programming languages (with Transcoder), and code autocompletion (with Copilot). Their investigation was conducted as 9 workshops with 43 software engineers and identified 11 categories of explainability needs in the context of Generative AI (GenAI) for code. It also proposed 4 types of features for generative AI: AI documentation, model uncertainty, attention distribution, and social transparency (i.e., making visible the socio-organizational factors that govern the use of AI).",
        "Mohammadkhani et al.  [85] used the attention mechanism to study CodeBERT and GraphCodeBERT on tasks including code documentation generation, code refinement, and code translation.",
        "4) Determinism Evaluation: LLMs are nondeterministic. Ouyang et al.  [10]  empirically studied the non-determinism of ChatGPT in code generation, founding that over 60% of tasks had zero equal test output across different requests. Nevertheless, their study of the literature in LLM-based code generation demonstrate that only 21.1% of these papers consider the nondeterminism threat in their experiments.",
        "5) Security Evaluation: Hajipour et al.  [86]  proposed a fewshot prompting approach to detecting security vulnerabilities, reporting that their approach automatically finds thousands of security vulnerabilities in several models. Khoury et al.  [87]  found that the code generated by ChatGPT often fell way below even minimal standards of secure coding. Risse and Böme  [88]  reported results that indicated vulnerability detection accuracy may be over-reported, due to the model overfitting to unrelated training set features .",
        "In addition, Yetistiren et al.  [76]  presented a comprehensive evaluation of the performance of Copilot, CodeWhisperer, and ChatGPT, covering different aspects including code validity, code correctness, code security, code reliability, and Their results show a wide degree of divergence in performance, motivating the need for further research and investigation. For example, they found 65%, 46%, and 31% of the programs generated by ChatGPT, Copilot, and CodeWhisperer (respectively) were correct.",
        "6) Benchmarks: As with other scientific evaluations, software engineering evaluation relies on publicly available and representative benchmark suites. A number of these have already emerged and can support software engineering evaluation of LLM-based applications. The Papers-With-Code platform  5  provides a summary of 15 benchmarks for evaluating code generation.",
        "Evaluations have often relied on small programming problems from programming courses  [89] , synthetically generated problem sets  [90] , and online judging platforms such as Leetcode  [29] ,  [65] ,  [91] . Although results reported naturally vary by LLM in training sets, the overall conclusions of these evaluations indicate success rates of between 20% and 80%.",
        "Nevertheless, existing code generation benchmarks tend to rely on test suites to automatically judge code correctness, which can be inadequate, leading to false judgements  [92] . This highlights the need for more work on evaluation benchmarks that are specifically tailored to LLM-based code generation evaluation. Liu et al.  [93]  draw attention to the problem, showing how existing test suites can lead to high degrees of false positive conclusions (also a serious problem for online judge platforms  [92] ). To alleviate this problem, they propose EvalPlus -a code synthesis benchmarking framework that automatically generates test inputs and rigorously evaluates the functional correctness of LLM-generated code. Their evaluation of 14 popular LLMs (including GPT-4 and ChatGPT) demonstrated that with the newly generated tests for HumanEval, the assessment of pass@k drops by up to 15%, averaged over problems considered.",
        "Jimenez et al.  [94]  introduced SWE-bench with the aim of evaluating LLMs on code generation problems in a realistic software engineering setting. SWE-bench contains 2,294 software engineering problems, drawn from real GitHub issues. The results suggest that Claude 2 and GPT-4 solve only 4.8% and 1.7% of the coding tasks, respectively.",
        "Test effectiveness is typically measured in terms of 'adequacy criteria'  [121] ,  [122] . Since testing cannot exhaustively explore every possibility, adequacy criteria provide a form of lower bound on the effectiveness achieved by a suite of tests. Mutation testing is a widely-studied technique for assessing the adequacy of software test suites  [123] ,  [124] , in which synthetic faults (called 'mutants'), are deliberately injected in order to assess test adequacy. Mutation testing has been shown to provide more stringent adequacy criteria than other structural coverage-based criteria such as statement and branch coverage  [125] .",
        "One of the challenging open problems for mutation testing is to generate mutants that faithfully model important classes of real-world faults. Khanfir et al.  [126]  used CodeBert to generate developer-like mutants and found that their approach has better fault revelation ability than PiTest. Garg et al.  [127]  applied CodeBERT to generate mutants that faithfully capture vulnerabilities. They evaluation found that 17% of the mutants fail the tests that are failed by 89% of the respective vulnerabilities. Brownlee  [128]  used GPT-3.5 to generate mutants for genetic improvemnt and observed that randomly sampled LLM-based edits compiled and passed unit tests more often compared to standard GI edits.",
        "Hort et al.  [234]  conducted a review of 293 papers on LLMs for code generation, to determine the degree to which sufficient information was shared to support replication. They found that only 33% shared source code and 27% shared trained artefacts. They also evaluated the papers from the perspective of energy consumption, assessing the degree to which it was possible for an independent researcher to assess the energy consumed during training. They report that approximately 38% (30 out of 79 publications which involve model training) shared sufficient information to estimate their energy consumption during training.",
        "Further evidence that there may be a growing issue with scientific evaluation quality in the literature on LLM-Based Software Engineering can be found in the survey of LLM-Based Testing by  Wang et al. [98] . In their survey, they filtered an initial pool of papers on LLM-Based Testing to remove those that did not meet standard evaluation quality constraints. These constraints required papers to include a clear, defined, repeatable evaluation methodology that includes a control or baseline against which to measure effectiveness. This filtration criterion removed more than 90% of the papers that initially met keyword search criteria.",
        "As these analyses of the literature demonstrate, more work is clearly needed to establish firm scientific foundations for the emerging discipline of LLM-based Software Engineering. Such foundations may draw on existing foundations for Empirical Software Engineering in general and, more specifically, on AI-based Software Engineering, such as SBSE (where there is a natural similarity  [105] ,  [235] ).",
        "Nevertheless, LLMs have their own unique properties, such as the ability to provide explanations, which will require domain-specific theoretical and empirical scientific foundations.",
        "LLMs inherently exhibit non-deterministic behaviour. Researchers need to carefully design their experiments, configure their LLMs (e.g., evaluating the effects of different distribution sampling strategies), and take into account non-determinism when drawing their conclusions on LLMs. The SBSE literature provides advice on the inferential statistics required to support such evaluation  [13] ,  [14] .",
        "We will witness a rapid growth in the number and diversity of language models for software engineers in the coming years. Both practitioners and practising software engineers will need reliable, efficient and comprehensive benchmarking systems. Benchmarking platforms such as TESTPILOT  [116]  and platforms such as Papers With Code (https://paperswithcode.com/ sota/code-generation-on-humaneval/) will become increasingly important.",
        "As well as generic scientific foundations, benchmarks and evaluation platforms, we also expect to see longitudinal studies of developer behaviour when programming with LLM assistance, so that we can understand the programmer-LLM interaction better and design more effective use case scenarios."
      ]
    }
  },
  {
    "paperId": "27e57cc2f22c1921d2a1c3954d5062e3fe391553",
    "title": "Guidelines for conducting and reporting case study research in software engineering",
    "sections": {
      "Introduction": [
        "The acceptance of empirical studies in software engineering and their contributions to increasing knowledge is continuously growing. The analytical research paradigm is not sufficient for investigating complex real life issues, involving humans and their interactions with technology. However, the overall share of empirical studies is negligibly small in computer science research;  Sjøberg et al. (2005) , found 103 experiments in 5,453 articles  Ramesh et al. (2004)  and identified less than 2% experiments with human subjects, and only 0.16% field studies among 628 articles. Further, existing work on empirical research methodology in software engineering has a strong focus on experimental research; the earliest by  Moher and Schneider (1981) ,  Basili et al. (1986) , the first methodology handbook by  Wohlin et al. (2000) , and promoted by  Tichy (1998) . All have a tendency towards quantitative approaches, although also qualitative approaches are discussed during the later years, e.g. by  Seaman (1999) . There exist guidelines for experiments' conduct  (Kitchenham et al. 2002; Wohlin et al. 2000)  and reporting  (Jedlitschka and Pfahl 2005) , measurements  (Basili and Weiss 1984; Fenton and Pfleeger 1996; van Solingen and Berghout 1999) , and systematic reviews  (Kitchenham 2007) , while only little is written on case studies in software engineering  (Höst and Runeson 2007; Kitchenham et al. 1995; Wohlin et al. 2003)  and qualitative methods  (Dittrich 2007; Seaman 1999; Sim et al. 2001) . Recently, a comprehensive view of empirical research issues for software engineering has been presented, edited by  Shull et al. (2008) .",
        "The term \"case study\" appears every now and then in the title of software engineering research papers. However, the presented studies range from very ambitious and well organized studies in the field, to small toy examples that claim to be case studies. Additionally, there are different taxonomies used to classify research. The term case study is used in parallel with terms like field study and observational study, each focusing on a particular aspect of the research methodology. For example,  Lethbridge et al.  use field studies as the most general term  (Lethbridge et al. 2005) , while  Easterbrook et al. (2008)  call case studies one of five \"classes of research methods\". Zelkowitz and Wallace propose a terminology that is somewhat different from what is used in other fields, and categorize project monitoring, case study and field study as observational methods  (Zelkowitz and Wallace 1998) . This plethora of terms causes confusion and problems when trying to aggregate multiple empirical studies.",
        "The case study methodology is well suited for many kinds of software engineering research, as the objects of study are contemporary phenomena, which are hard to study in isolation. Case studies do not generate the same results on e.g. causal relationships as controlled experiments do, but they provide deeper understanding of the phenomena under study. As they are different from analytical and controlled empirical studies, case studies have been criticized for being of less value, impossible to generalize from, being biased by researchers etc. This critique can be met by applying proper research methodology practices as well as reconsidering that knowledge is more than statistical significance  (Flyvbjerg 2007; Lee 1989 ). However, the research community has to learn more about the case study methodology in order to review and judge it properly.",
        "Case study methodology handbooks are superfluously available in e.g. social sciences  (Robson 2002; Stake 1995; Yin 2003)  which literature also has been used in software engineering. In the field of information systems (IS) research, the case study methodology is also much more mature than in software engineering. For example, Benbasat et al. provide a brief overview of case study research in information systems  (Benbasat et al. 1987) , Lee analyzes case studies from a positivistic perspective  (Lee 1989 ) and Klein and Myers do the same from an interpretive perspective  (Klein and Myers 1999) .",
        "It is relevant to raise the question: what is specific for software engineering that motivates specialized research methodology? In addition to the specifics of the examples, the characteristics of software engineering objects of study are different from social science and also to some extent from information systems. The study objects are 1) private corporations or units of public agencies developing software rather than public agencies or private corporations using software systems; 2) project oriented rather than line or function oriented; and 3) the studied work is advanced engineering work conducted by highly educated people rather than routine work. Additionally, the software engineering research community has a pragmatic and result-oriented view on research methodology, rather than a philosophical stand, as noticed by  Seaman (1999) .",
        "The purpose of this paper is to provide guidance for the researcher conducting case studies, for reviewers of case study manuscripts and for readers of case study papers. It is synthesized from general methodology handbooks, mainly from the social science field, as well as literature from the information systems field, and adapted to software engineering needs. Existing literature on software engineering case studies is of course included as well. The underlying analysis is done by structuring the information according to a general case study research process (presented in Section 2.4). Where different recommendations or terms appear, the ones considered most suited for the software engineering domain are selected, based on the authors' experience on conducting case studies and reading case study reports. Links to data sources are given by regular references. Specifically, checklists for researchers and readers are derived through a systematic analysis of existing checklists  (Höst and Runeson 2007) , and later evaluated by PhD students as well as by members of the International Software Engineering Research Network and updated accordingly.",
        "This paper does not provide absolute statements for what is considered a \"good\" case study in software engineering. Rather it focuses on a set of issues that all contribute to the quality of the research. The minimum requirement for each issue must be judged in its context, and will most probably evolve over time. This is similar to the principles by Klein and Myers for IS case studies  (Klein and Myers 1999) , \"it is incumbent upon authors, reviewers, and exercise their judgment and discretion in deciding whether, how and which of the principles should be applied\". We do neither assess the current status of case study research in software engineering. This is worth a study on its own, similar to the systematic review on experiments by  Sjøberg et al. (2005) . Further, examples are used both to illustrate good practices and lack thereof.",
        "This paper is outlined as follows. We first define a set of terms in the field of empirical research, which we use throughout the paper (Section 2.1), set case study research into the context of other research methodologies (Section 2.2) and discuss the motivations for software engineering case studies (Section 2.3). We define a case study research process (Section 2.4) and terminology (Section 2.5), which are used for the rest of the paper. Section 3 discusses the design of a case study and planning for data collection. Section 4 describes the process of data collection. In Section 5 issues on data analysis are treated, and reporting is discussed in Section 6. Section 7 discusses reading and reviewing case study report, and Section 8 summarizes the paper. Checklists for conducting and reading case study research are linked to each step in the case study process, and summarized in the Appendix.",
        "Throughout the paper, we use three different case study examples to illustrate the methods. The examples are selected from the authors' publications, representing a variety of approaches within case study research. They illustrate solutions or identify problems in case study research, i.e. are not always compliant with the guidelines in this paper. The examples are presented in a format like this and they are denoted study XP, RE and QA after their research area on agile methods (extreme programming), requirements engineering and quality assurance, respectively. More information about the studies can be found in the original publications  (Karlström and Runeson 2005; 2006)  (XP),  (Regnell et al. 2001"
      ],
      "Methods": [
        "In order to set the scope for the type of empirical studies we address in this paper, we put case studies into the context of other research methodologies and refer to general definitions of the term case study according to  Robson (2002 ), Yin (2003)  and  Benbasat et al. (1987)  respectively.",
        "The three definitions agree on that case study is an empirical method aimed at investigating contemporary phenomena in their context. Robson calls it a research strategy and stresses the use of multiple sources of evidence, Yin denotes it an inquiry and remarks that the boundary between the phenomenon and its context may be unclear, while Benbasat et al. make the definitions somewhat more specific, mentioning information gathering from few entities (people, groups, organizations), and the lack of experimental control.",
        "There are three other major research methodologies which are related to case studies:",
        "& Survey, which is the \"collection of standardized information from a specific population,",
        "or some sample from one, usually, but not necessarily by means of a questionnaire or interview\"  (Robson 2002) . & Experiment, or controlled experiment, which is characterized by \"measuring the effects of manipulating one variable on another variable\"  (Robson 2002)  and that \"subjects are assigned to treatments by random.\"  (Wohlin et al. 2000) . Quasi-experiments are similar to controlled experiments, except that subjects are not randomly assigned to treatments. Quasi-experiments conducted in an industry setting may have many characteristics in common with case studies. & Action research, with its purpose to \"influence or change some aspect of whatever is the focus of the research\"  (Robson 2002) , is closely related to case study. More strictly, a case study is purely observational while action research is focused on and involved in the change process. In software process improvement  (Dittrich et al. 2008; Iversen et al. 2004 ) and technology transfer studies  (Gorschek et al. 2006 ), the research method should be characterized as action research. However, when studying the effects of a change, e.g. in pre-and post-event studies, we classify the methodology as case study. In IS, where action research is widely used, there is a discussion on finding the balance between action and research, see e.g.  (Avison et al. 2001; Baskerville and Wood-Harper 1996) . For the research part of action research, these guidelines apply as well.  Easterbrook et al. (2008)  also count ethnographic studies among the major research methodologies. We prefer to consider ethnographic studies as a specialized type of case studies with focus on cultural practices  (Easterbrook et al. 2008)  or long duration studies with large amounts of participant-observer data  (Klein and Myers 1999) . Zelkowitz and Wallace define four different \"observational methods\" in software engineering (Zelkowitz and Wallace 1998); project monitoring, case study, assertion and field study. Our guidelines apply to all these, except assertion which is not considered a proper research method. In general, the borderline between the types of study is not always distinct. We prefer to see project monitoring as a part of a case study and field studies as multiple case studies. Robson summarizes his view, which seems functional in software engineering as well: \"Many flexible design studies, although not explicitly labeled as such, can be usefully viewed as case studies.\"  (Robson 2002)  p 185.",
        "Finally, a case study may contain elements of other research methods, e.g. a survey may be conducted within a case study, literature search often precede a case study and archival analyses may be a part of its data collection. Ethnographic methods, like interviews and observations are mostly used for data collection in case studies.",
        "Different research methodologies serve different purposes; one type of research methodology does not fit all purposes. We distinguish between four types of purposes for research based on  Robson's (2002)   Case study methodology was originally used primarily for exploratory purposes, and some researchers still limit case studies for this purpose, as discussed by  Flyvbjerg (2007) . However, it is also used for descriptive purposes, if the generality of the situation or phenomenon is of secondary importance. Case studies may be used for explanatory purposes, e.g. in interrupted time series design (pre-and post-event studies) although the isolation of factors may be a problem. This involves testing of existing theories in confirmatory studies. Finally, as indicated above, case studies in the software engineering discipline often take an improvement approach, similar to action research; see e.g. the QA study  (Andersson and Runeson 2007b) .",
        "Klein and Myers define three types of case study depending on the research perspective, positivist, critical and interpretive  (Klein and Myers 1999) . A positivist case study searches evidence for formal propositions, measures variables, tests hypotheses and draws inferences from a sample to a stated population, i.e. is close to the natural science research model  (Lee 1989 ) and related to Robson's explanatory category. A critical case study aims at social critique and at being emancipatory, i.e. identifying different forms of social, cultural and political domination that may hinder human ability. Improving case studies may have a character of being critical. An interpretive case study attempts to understand phenomena through the participants' interpretation of their context, which is similar to Robson's exploratory and descriptive types. Software engineering case studies tend to lean towards a positivist perspective, especially for explanatory type studies.",
        "Conducting research on real world issues implies a trade-off between level of control and degree of realism. The realistic situation is often complex and non-deterministic, which hinders the understanding of what is happening, especially for studies with explanatory purposes. On the other hand, increasing the control reduces the degree of realism, sometimes leading to the real influential factors being set outside the scope of the study.",
        "Case studies are by definition conducted in real world settings, and thus have a high degree of realism, mostly at the expense of the level of control.",
        "The data collected in an empirical study may be quantitative or qualitative. Quantitative data involves numbers and classes, while qualitative data involves words, descriptions, pictures, diagrams etc. Quantitative data is analyzed using statistics, while qualitative data is analyzed using categorization and sorting. Case studies tend mostly to be based on qualitative data, as these provide a richer and deeper description. However, a combination of qualitative and quantitative data often provides better understanding of the studied phenomenon  (Seaman 1999), i.e . what is sometimes called \"mixed methods\"  (Robson 2002) .",
        "The research process may be characterized as fixed or flexible according to  Anastas and MacDonald (1994)  and  Robson (2002) . In a fixed design process, all parameters are defined at the launch of the study, while in a flexible design process key parameters of the study may be changed during the course of the study. Case studies are typically flexible design studies, while experiments and surveys are fixed design studies. Other literature use the terms quantitative and qualitative design studies, for fixed and flexible design studies respectively. We prefer to adhere to the fixed/flexible terminology since it reduces the risk for confusion that a study with qualitative design may collect both qualitative and quantitative data. Otherwise it may be unclear whether the term qualitative refers to the data or the design of the study,",
        "Triangulation is important to increase the precision of empirical research. Triangulation means taking different angles towards the studied object and thus providing a broader picture. The need for triangulation is obvious when relying primarily on qualitative data, which is broader and richer, but less precise than quantitative data. However, it is relevant also for quantitative data, e.g. to compensate for measurement or modeling errors. Four different types of triangulation may be applied  (Stake 1995) :",
        "& Data (source) triangulation-using more than one data source or collecting the same data at different occasions. & Observer triangulation-using more than one observer in the study. & Methodological triangulation-combining different types of data collection methods, e.g. qualitative and quantitative methods. & Theory triangulation-using alternative theories or viewpoints.",
        "Table  1  shows an overview of the primary characteristics of the above discussed research methodologies Yin adds specifically to the characteristics of a case study that it (Yin 2003):",
        "& \"copes with the technically distinctive situation in which there will be many more variables than data points, and as one result & relies on multiple sources of evidence, with data needing to converge in a triangulating fashion, and as another result & benefits from the prior development of theoretical propositions to guide data collection and analysis.\" Hence, a case study will never provide conclusions with statistical significance. On the contrary, many different kinds of evidence, figures, statements, documents, are linked together to support a strong and relevant conclusion.",
        "Perry et al. define similar criteria for a case study  (Perry et al. 2005) . It is expected that a case study:",
        "& \"Has research questions set out from the beginning of the study & Data is collected in a planned and consistent manner & Inferences are made from the data to answer the research question & Explores a phenomenon, or produces an explanation, description, or causal analysis of it & Threats to validity are addressed in a systematic way.\"",
        "In summary, the key characteristics of a case study are that 1) it is of flexible type, coping with the complex and dynamic characteristics of real world phenomena, like software engineering, 2) its conclusions are based on a clear chain of evidence, whether qualitative or quantitative, collected from multiple sources in a planned and consistent manner, and 3) it adds to existing knowledge by being based on previously established theory, if such exist, or by building theory."
      ],
      "Discussion": [
        "The principal decisions on methods for data collection are defined at design time for the case study, although detailed decisions on data collection procedures are taken later.  Lethbridge et al. (2005)  define three categories of methods: direct (e.g. interviews), indirect (e.g. tool instrumentation) and independent (e.g. documentation analysis). These are further elaborated in Section 4.",
        "In case studies, the case and the units of analysis should be selected intentionally. This is in contrast to surveys and experiments, where subjects are sampled from a population to which the results are intended to be generalized. The purpose of the selection may be to study a case that is expected to be \"typical\", \"critical\", \"revelatory\" or \"unique\" in some respect  (Benbasat et al. 1987) , and the case is selected accordingly. Flyvbjerg defines four variants of information-oriented case study selections: \"extreme/deviant\", \"maximum variation\", \"critical\" and \"paradigmatic\" (Flyvbjerg 2007). In a comparative case study, the units of analysis must be selected to have the variation in properties that the study intends to compare. However, in practice, many cases are selected based on availability  (Benbasat et al. 1987)  as is the case for many experiments  (Sjøberg et al. 2005) .",
        "Case selection is particularly important when replicating case studies. A case study may be literally replicated, i.e. the case is selected to predict similar results, or it is theoretically replicated, i.e. the case is selected to predict contrasting results for predictable reasons (Yin 2003).",
        "There were different objectives of the three example cases. The objective of study XP was to investigate how an agile process can coexist with a stage-gate management organization. The objective of study RE was to evaluate a method for prioritization of requirements, and the objective of study QA was to find quantitative prediction models and procedures for defect data. Study XP is considered an embedded case study with two units of analysis from two different companies, although it might be seen as two holistic case studies, as denoted above. RE is a holistic case study with one unit of analysis, while QA is an embedded case study in one company with three different projects as units of analysis. All the companies were selected based on existing academia-industry relations, while the units of analysis were selected to fit the specific case study purposes. Concerning the frame of reference, no explicit theories are referred to in studies XP and RE. However, the investigated approaches are based on existing methods that, to some extent, already have been investigated. Earlier studies thereby affected the designs of the studies. Study QA was partly a replication, which means that the original study formed a frame of reference from which theories on, for example, the Pareto principle and fault persistence between test phases were used when hypotheses were defined. Data were primarily collected using interviews in the XP case. In the RE case, questionnaires constituted the major source of data, while in the QA case, defect metrics from a company was the major data source.",
        "Detailed description of data analysis procedures, including data schemas, priori codes etc. Appendix A Template letter to invite participants.",
        "& A text clearly stating that the participation is voluntary, and that collected data will be anonymous. & A list of known risks. & A list of benefits for the participants, in this case for example experience from using a new technique and feedback effectiveness. & A description of how confidentiality will be assured. This includes a description of how collected material will be coded and identified in the study. & Information about approvals from review board. & Date and signatures from participant and researchers.",
        "If the researchers intend to use the data for other, not yet defined purposes, this should be signed separately to allow participants to choose if their contribution is for the current study only, or for possible future studies.",
        "Issues on confidentiality and publication should also be regulated in a contract between the researcher and the studied organization. However, not only can information be sensitive when leaking outside a company. Data collected from and opinions stated by individual employees may be sensitive if presented e.g. to their managers  (Singer and Vinson 2002) . The researchers must have the right to keep their integrity and adhere to agreed procedures in this kind of cases. Companies may not know academic practices for publication and dissemination, and must hence be explicitly informed about those. From a publication point of view, the relevant data to publish is rarely sensitive to the company since data may be made anonymous. However, it is important to remember that it is not always sufficient to remove names of companies or individuals. They may be identified by their characteristics if they are selected from a small set of people or companies.",
        "Results may be sensitive to a company, e.g. by revealing deficiencies in their software engineering practices, or if their product comes out last in a comparison (Amschler  Andrews and Pradhan 2001) . The chance that this may occur must be discussed upfront and made clear to the participants of the case study. In case violations of the law are identified during the case study, these must be reported, even though \"whistle-blowers\" rarely are rewarded.",
        "The inducements for individuals and organizations to participate in a case study vary, but there are always some kinds of incentives, tangible or intangible. It is preferable to make the inducements explicit, i.e. specify what the incentives are for the participants. Thereby the inducement's role in threatening the validity of the study may also be analyzed.",
        "Giving feedback to the participants of a study is critical for the long term trust and for the validity of the research. Firstly, transcript of interviews and observations should be sent back to the participants to enable correction of raw data. Secondly, analyses should be presented to them in order to maintain their trust in the research. Participants must not necessarily agree in the outcome of the analysis, but feeding back the analysis results increases the validity of the study.",
        "In all three example studies issues of confidentiality were handled through Non-Disclosure Agreements and general project cooperation agreements between the companies and the university, lasting longer than one case study. These agreements state that the university researchers are obliged to have publications approved by representatives of the companies before they are published, and that raw data must not be spread to any but those signing the contract. The researchers are not obliged to report their sources of facts to management, unless it is found that a law is violated. In order to ensure that interviewees were not cited wrongly, it was agreed that the transcribed interviews were sent back to them for review in the XP study. In the beginning of each interview, interviewees were informed about their rights in the study. In study QA, feedback meetings for analysis and interpretation were explicitly a part of the methodology ((Andersson and Runeson 2007b) Fig.  1 ). When negotiating publication of data, we were explicitly told that raw numbers of defects could not be published, but percentages over phases could, which was acceptable for the research purposes. All the three studies were conducted in Sweden, where only studies in medicine are explicitly regulated by law; hence there was no approval of the studies by a review board beforehand.",
        "Data analysis is conducted differently for quantitative and qualitative data. For quantitative data, the analysis typically includes analysis of descriptive statistics, correlation analysis, development of predictive models, and hypothesis testing. All of these activities are relevant in case study research.",
        "Descriptive statistics, such as mean values, standard deviations, histograms and scatter plots, are used to get an understanding of the data that has been collected. Correlation analysis and development of predictive models are conducted in order to describe how a measurement from a later process activity is related to an earlier process measurement. Hypothesis testing is conducted in order to determine if there is a significant effect of one or several variables (independent variables) on one or several other variables (dependent variables).",
        "It should be noticed that methods for quantitative analysis assume a fixed research design. For example, if a question with a quantitative answer is changed halfway in a series of interviews, this makes it impossible to interpret the mean value of the answers. Further, quantitative data sets from single cases tend to be very small, due to the number of respondents or measurement points, which causes special concerns in the analysis.",
        "Quantitative analysis is not covered any further in this paper, since it is extensively covered in other texts. The rest of this chapter covers qualitative analysis. For more information about quantitative analysis, refer for example to  (Wohlin et al. 2000; Wohlin and Höst 2001; Kitchenham et al. 2002) .",
        "In study RE and study QC the main analyses were conducted with quantitative methods, mainly through analysis of correlation and descriptive statistics, such as scatter plots. In the QC case, the quantitative data acted as a trigger for deeper understanding. Patterns in the data, and lack thereof generated questions in the feedback session. The answers lead to changes in the data analysis, e.g. filtering out some data sources, and to identification of real patterns in the data. In study XP, the main analysis was conducted with qualitative methods, but this was combined with a limited quantitative analysis of number of defects found during different years in one of the organizations. However, there would probably have been possibilities to conduct more complementary analyses in order to corroborate or develop the results from the qualitative analysis.",
        "Since case study research is a flexible research method, qualitative data analysis methods  (Seaman 1999 ) are commonly used. The basic objective of the analysis is to derive conclusions from the data, keeping a clear chain of evidence. The chain of evidence means that a reader should be able to follow the derivation of results and conclusions from the collected data  (Yin 2003) . This means that sufficient information from each step of the study and every decision taken by the researcher must be presented.",
        "In addition to the need to keep a clear chain of evidence in mind, analysis of qualitative research is characterized by having analysis carried out in parallel with the data collection and the need for systematic analysis techniques. Analysis must be carried out in parallel with the data collection since the approach is flexible and that new insights are found during the analysis. In order to investigate these insights, new data must often be collected, and instrumentation such as interview questionnaires must be updated. The need to be systematic is a direct result of that the data collection techniques can be constantly updated, while the same time being required to maintain a chain of evidence.",
        "In order to reduce bias by individual researchers, the analysis benefits from being conducted by multiple researchers. The preliminary results from each individual researcher is merged into a common analysis result in a second step. Keeping track and reporting the cooperation scheme helps increasing the validity of the study.",
        "There are two different parts of data analysis of qualitative data, hypothesis generating techniques and hypothesis confirmation techniques  (Seaman 1999) , which can be used for exploratory and explanatory case studies, respectively.",
        "Hypothesis generation is intended to find hypotheses from the data. When using these kinds of techniques, there should not be too many hypotheses defined before the analysis is conducted. Instead the researcher should try to be unbiased and open for whatever hypotheses are to be found in the data. The results of these techniques are the hypotheses as such. Examples of hypotheses generating techniques are \"constant comparisons\" and \"cross-case analysis\"  (Seaman 1999) . Hypothesis confirmation techniques denote techniques that can be used to confirm that a hypothesis is really true, e.g. through analysis of more data. Triangulation and replication are examples of approaches for hypothesis confirmation  (Seaman 1999) . Negative case analysis tries to find alternative explanations that reject the hypotheses. These basic types of techniques are used iteratively and in combination. First hypotheses are generated and then they are confirmed. Hypothesis generation may take place within one cycle of a case study, or with data from one unit of analysis, and hypothesis confirmation may be done with data from another cycle or unit of analysis  (Andersson and Runeson 2007b) .",
        "This means that analysis of qualitative data is conducted in a series of steps (based on  (Robson 2002), p. 459) . First the data is coded, which means that parts of the text can be given a code representing a certain theme, area, construct, etc. One code is usually assigned to many pieces of text, and one piece of text can be assigned more than one code. Codes can form a hierarchy of codes and sub-codes. The coded material can be combined with comments and reflections by the researcher (i.e. \"memos\"). When this has been done, the researcher can go through the material to identify a first set of hypotheses. This can, for example, be phrases that are similar in different parts of the material, patterns in the data, differences between sub-groups of subjects, etc. The identified hypotheses can then be used when further data collection is conducted in the field, i.e. resulting in an iterative approach where data collection and analysis is conducted in parallel as described above. During the iterative process a small set of generalizations can be formulated, eventually resulting in a formalized body of knowledge, which is the final result of the research attempt. This is, of course, not a simple sequence of steps. Instead, they are executed iteratively and they affect each other.",
        "The activity where hypotheses are identified requires some more information. This is in no way a simple step that can be carried out by following a detailed, mechanical, approach. Instead it requires ability to generalize, innovative thinking, etc. from the researcher. This can be compared to quantitative analysis, where the majority of the innovative and analytical work of the researcher is in the planning phase (i.e. deciding design, statistical tests, etc). There is, of course, also a need for innovative work in the analysis of quantitative data, but it is not as clear as in the planning phase. In qualitative analysis there are major needs for innovative and analytical work in both phases.",
        "One example of a useful technique for analysis is tabulation, where the coded data is arranged in tables, which makes it possible to get an overview of the data. The data can, for example be organized in a table where the rows represent codes of interest and the columns represent interview subjects. However, how to do this must be decided for every case study.",
        "There are specialized software tools available to support qualitative data analysis, e.g. NVivo and Atlas. However, in some cases standard tools such as word processors and spreadsheet tools are useful when managing the textual data.",
        "In study XP, the transcribed interviews were initially analyzed by one of the researchers. A preliminary set of codes were derived from the informal notes and applied to the transcripts. The preliminary set of codes was: project model, communication, planning, follow-up, quality, technical issues and attitudes. Each statement in the transcribed interviews was given a unique identification, and classified by two researchers. The transcribed data was then filled into tables, allowing for analysis of patterns in the data by sorting issues found by, for example, interviewee role or company. The chain of evidence is illustrated with the figure below (from  Karlström and Runeson 2006)  In study RE and QA the main analysis was quantitative, although some qualitative analysis was conducted on the information that was gathered in feedback sessions. However, this analysis would probably benefit from being conducted in a more structured way, e.g. by recording, transcribing, and coding feedback data before analysis."
      ]
    }
  },
  {
    "paperId": "5e80e7131e77fecfc1cf6648f0f6b25e364c7e4d",
    "title": "Human Computer Interaction",
    "sections": {}
  },
  {
    "paperId": "420af69e5ae3686b709c14a8cec7dc9f90a85681",
    "title": "ChatGPT: perspectives from human–computer interaction and psychology",
    "sections": {
      "Introduction": [
        "ChatGPT, developed by OpenAI, is a conversational system based on the large language model GPT (Generative Pre-trained Transformer). This product aims to achieve smooth, natural conversations with human users through natural language processing technology. ChatGPT has broad applications in customer service, educational tutoring, entertainment interactions, and more  (Kocoń et al., 2023) .",
        "ChatGPT can understand complex queries and commands, producing fluid, coherent natural language responses. The system can remember conversation history and comprehend contextual information, thus providing accurate and relevant responses across multiple rounds of dialog  (Nah et al., 2023) . Additionally, it supports multiple languages, serving global users.",
        "ChatGPT not only understands and responds to factual questions but also simulates emotional interactions, offering a more humanized communication experience. By continuously learning from user feedback and dialog data, ChatGPT can self-optimize to enhance dialog quality and user experience.",
        "ChatGPT has been widely applied in various fields, becoming a revolutionary tool. In customer service, it significantly improves response speed and efficiency by automatically answering common questions. In the education sector, ChatGPT acts as an intelligent tutoring assistant, offering personalized learning advice and materials to help students enhance their learning efficiency. Moreover, it assists in content creation, helping users write articles, reports, and creative writing. In entertainment and social media, ChatGPT generates creatively interactive content, bringing a new interactive experience to users. These application scenarios demonstrate ChatGPT's powerful capabilities in understanding and generating natural language, as well as its enormous potential in improving human-computer interaction, boosting work efficiency, and enriching people's lives.",
        "The exploration of ChatGPT's impacts has been extensive, yet existing research predominantly focuses on technical aspects and societal implications, leaving a notable gap in understanding its effects on human-computer interaction and user psychology. While prior studies have elucidated the technical architecture of ChatGPT and its broader societal implications, there remains a scarcity of research examining its nuanced effects on the dynamics of human-computer interaction and the psychological responses of users.",
        "Based on the above background, this paper comprehensively analyzes the impact of ChatGPT in the fields of HCI, psychology, and society. Section 2 elaborates on the research purpose, detailing the research objectives and key research questions: How does ChatGPT impact human-computer interaction? What are the psychological effects of interacting with ChatGPT? Section 3, Methodology, describes the research methods employed, including data collection techniques and the inclusion and exclusion criteria used to ensure the relevance and quality of the selected studies. In Section 4, Technical Description, we explore the technical evolution of ChatGPT and its breakthroughs in the HCI field, highlighting its development history and the changes and impacts it has brought to natural language interaction. Section 5, Psychological Implications, examines the effects of ChatGPT on psychological support, emotional regulation, and social relationships, analyzing its influence on both human-to-human and humanmachine interactions. Section 6, Social Implications, discusses the opportunities presented by ChatGPT in education, healthcare, and scientific research, as well as the challenges and risks such as the dissemination of false information, employment impact, and data privacy security. Section 7, Business Implications, addresses the impact of ChatGPT on various business domains. This section explores the opportunities and challenges in business environments such as intelligent customer service and digital marketing to understand the potential benefits and pitfalls of integrating ChatGPT into business operations. In Section 8, the Future Outlook discusses the directions for optimizing ChatGPT, including enhancing understanding and generation capabilities, multimodal interaction, and personalized language generation. This section also considers potential changes in social relationships and strategies to avoid social and ethical issues."
      ],
      "Discussion": [
        "Due to its proficiency in understanding and effectively processing human language, the ChatGPT platform demonstrates significant advantages in the field of customer sentiment analysis  (Harahap et al., 2023) . Trained on comprehensive and diverse datasets, it can understand the context, nuances, and emotional aspects embedded in customer texts. In sentiment analysis, ChatGPT can identify whether the textual input provided by customers contains positive, negative, or neutral emotions  (Sutrisno et al., 2023) .  Sudirjo et al. (2023)  indicate that utilizing ChatGPT has significant potential in improving customer sentiment analysis for commercial enterprises. It can aid in understanding and addressing customer requirements, tendencies, and satisfaction levels. However, it is important to understand that ChatGPT should not be the sole source of information and the analysis results need to be interpreted judiciously by humans.",
        "Real business cases already utilize NLP for customer sentiment analysis. Brandwatch is a social media monitoring and analysis tool that uses NLP to analyze brand mentions on social media. With the help of AI, this company identifies the emotional tendencies of consumers toward brands, products, and services. This sentiment analysis provides brands with real-time insights into public sentiment, helping them adjust their marketing strategies. These cases demonstrate that the capabilities of GPT-3 and its derivatives indeed provide strong support for sentiment analysis. By understanding complex language patterns and emotional distinctions, these technologies give businesses deeper insights when processing customer feedback and social media interactions. As technology advances and its business applications expand, more enterprises are expected to directly utilize ChatGPT or similar models for sentiment analysis and customer insights."
      ],
      "Conclusion": [
        "This article explores ChatGPT's impact on all aspects of human life and technology in detail, highlighting its infrastructure, including its innovative Transformer model and reinforcement learning (RLHF) processes from human feedback. These advances in technology enable ChatGPT to generate responses that are not only context-relevant, but also human-resonant, thus making significant progress in conversational interfaces.",
        "From a human-computer interaction perspective, the author analyzes how ChatGPT can enhance the user experience by providing sophisticated conversation capabilities that push the boundaries of traditional computer-mediated communication. From a psychological perspective, this paper weighs the potential of ChatGPT as a support tool against the risk of fostering dependence and reducing interpersonal connection. On the social side, this paper investigates its applications in customer service and education, acknowledging both the efficiencies it brings and the challenges it brings, such as privacy concerns.",
        "The review also makes predictions and recommendations for the future development of ChatGPT, in particular its role in shaping social relationships and its ethical implications. We believe that while ChatGPT presents numerous opportunities for progress, it also requires careful and ethical considerations to reach its full potential."
      ]
    }
  },
  {
    "paperId": "fd495d6cf7c3169bc58550fdf32be6e16e2800f8",
    "title": "Bioconductor: open software development for computational biology and bioinformatics",
    "sections": {
      "Introduction": [
        "The Bioconductor project  [1]  is an initiative for the collaborative creation of extensible software for computational biology and bioinformatics (CBB). Biology, molecular biology in particular, is undergoing two related transformations. First, there is a growing awareness of the computational nature of many biological processes and that computational and statistical models can be used to great benefit. Second, developments in high-throughput data acquisition produce requirements for computational and statistical sophistication at each stage of the biological research pipeline. The main goal of the Bioconductor project is creation of a durable and flexible software development and deployment environment that meets these new conceptual, computational and inferential challenges. We strive to reduce barriers to entry to research in CBB. A key aim is simplification of the processes by which statistical researchers can explore and interact fruitfully with data resources and algorithms of CBB, and by which working biologists obtain access to and use of state-of-the-art statistical methods for accurate inference in CBB.",
        "Among the many challenges that arise for both statisticians and biologists are tasks of data acquisition, data management, data transformation, data modeling, combining different data sources, making use of evolving machine learning methods, and developing new modeling strategies suitable to CBB. We have emphasized transparency, reproducibility, and efficiency of development in our response to these challenges. Fundamental to all these tasks is the need for software; ideas alone cannot solve the substantial problems that arise.",
        "The primary motivations for an open-source computing environment for statistical genomics are transparency, pursuit of reproducibility and efficiency of development."
      ],
      "Methods": [
        "The software development strategy we have adopted has several precedents. In the mid-1980s Richard Stallman started the Free Software Foundation and the GNU project  [2]  as an attempt to provide a free and open implementation of the Unix operating system. One of the major motivations for the project was the idea that for researchers in computational sciences \"their creations/discoveries (software) should be available for everyone to test, justify, replicate and work on to boost further scientific innovation\"  [3] . Together with the Linux kernel, the GNU/Linux combination sparked the huge open-source movement we know today. Open-source software is no longer viewed with prejudice, it has been adopted by major information technology companies and has changed the way we think about computational sciences. A large body of literature exists on how to manage open-source software projects: see Hill  [4]  for a good introduction and a comprehensive bibliography.",
        "One of the key success factors of the Linux kernel is its modular design, which allows for independent and parallel development of code  [5]  in a virtual decentralized network  [3] . Developers are not managed within the hierarchy of a company, but are directly responsible for parts of the project and interact directly (where necessary) to build a complex system  [6] . Our organization and development model has attempted to follow these principles, as well as those that have evolved from the R project  [7, 8] .",
        "In this section, we review seven topics important to establishment of a scientific open source software project and discuss them from a CBB point of view: language selection, infrastructure resources, design strategies and commitments, distributed development and recruitment of developers, reuse of exogenous resources, publication and licensure of code, and documentation.",
        "To ensure that the code is open to public scrutiny and comment To provide full access to algorithms and their implementation To provide to users the ability to fix bugs without waiting for the developer, and to extend and improve the supplied software in other languages such as Perl [39] and Python  [40] . All that is needed is some platform-independent format for binding together the data, software and scripts defining the analysis, and a document that can be rendered automatically to a conveniently readable account of the analysis steps and their outcomes. If the format is an R package, this package then constitutes a single distributable software element that embodies the computational science being published. This is precisely the compendium concept espoused in  [36] ."
      ],
      "Conclusion": [
        "We have detailed the approach to software development taken by the Bioconductor project. Bioconductor has been operational for about three years now and in that time it has become a prominent software project for CBB. We argue that the success of the project is due to many factors. These include the choice of R as the main development language, the adoption of standard practices of software design and a belief that the creation of software infrastructure is an important and essential component of a successful project of this size.",
        "The group dynamic has also been an important factor in the success of Bioconductor. A willingness to work together, to see that cooperation and coordination in software development yields substantial benefits for the developers and the users and encouraging others to join and contribute to the project are also major factors in our success.",
        "To date the project provides the following resources: an online repository for obtaining software, data and metadata, papers, and training materials; a development team that coordinates the discussion of software strategies and development; a user community that provides software testing, suggested improvements and self-help; more than 80 software packages, hundreds of metadata packages and a number of experimental data packages.",
        "At this point it is worth considering the future. While many of the packages we have developed have been aimed at particular problems, there have been others that were designed to support future developments. And that future seems very interesting. Many of the new problems we are encountering in CBB are not easily addressed by technology transfer, but rather require new statistical methods and software tools. We hope that we can encourage more statisticians to become involved in this area of research and to orient themselves and their research to the mixture of methodology and software development that is necessary in this field.",
        "In conclusion we would like to note that the Bioconductor Project has many developers, not all of whom are authors of this paper, and all have their own objectives and goals. The views presented here are not intended to be comprehensive nor prescriptive but rather to present our collective experiences and the authors' shared goals. In a very simplified version these can be summarized in the view that coordinated cooperative software development is the appropriate mechanism for fostering good research in CBB.",
        "Hypergeometric analysis of molecular function enrichment of genes selected in the analysis described in Figure  1  Figure  3  Hypergeometric analysis of molecular function enrichment of genes selected in the analysis described in Figure  1 .  > mf  [1]  \"MHC class II receptor activity\"  [2]  \"MHC class I receptor activity\""
      ]
    }
  },
  {
    "paperId": "90485e0ce54c1ad12a2d01362a007ab107d71063",
    "title": "Biopython: freely available Python tools for computational molecular biology and bioinformatics",
    "sections": {
      "Introduction": [
        "Python (www.python.org) and Biopython are freely available open source tools, available for all the major operating systems. Python is a very high-level programming language, in widespread commercial and academic use. It features an easy to learn syntax, objectoriented programming capabilities and a wide array of libraries. Python can interface to optimized code written in C, C++ or even FORTRAN, and together with the Numerical Python project numpy  (Oliphant, 2006) , makes a good choice for scientific programming  (Oliphant, 2007) . Python has even been used in the numerically demanding field of molecular dynamics  (Hinsen, 2000) . There are also high-quality plotting libraries such as matplotlib (matplotlib.sourceforge.net) available.",
        "Since its founding in 1999  (Chapman and Chang, 2000) , Biopython has grown into a large collection of modules, described briefly below, intended for computational biology or bioinformatics programmers to use in scripts or incorporate into their own software. Our web site lists over 100 publications using or citing Biopython.",
        "The Open Bioinformatics Foundation (OBF, www.open-bio.org) hosts our web site, source code repository, bug tracking database and email mailing lists, and also supports the related BioPerl  (Stajich et al., 2002) , BioJava  (Holland et al., 2008) , BioRuby (www.bioruby.org) and BioSQL (www.biosql.org) projects."
      ]
    }
  },
  {
    "paperId": "f004ac1ae5fc4509295c743f1b1573e3c2e5732a",
    "title": "Computational Linguistics",
    "sections": {
      "Conclusion": [
        "Computational linguistics is now an active sub-discipline in applied linguistics. It is a field of data science that powers chatbots, search engines, and more. Applications of computational linguistics techniques range from those minimally dependent on linguistic structure and meaning to those that attain some level of competence in comprehending and using language. At Amazon, computational linguists and language engineers work on Alexa. At Apple, computational linguists and speech engineers develop Siri.",
        "It has been shown that languages can be learned with a combination of simple input presented incrementally as the child develops better memory and longer attention span. Researchers have created a system which not only predicts future linguistic evolution but also gives insight into the evolutionary history of modern-day languages  [6] . More information about computational linguistics can be found in the books in  and the following related journals:  Computational Linguistics  International Journal of Computational Linguistics"
      ]
    }
  },
  {
    "paperId": "ca4e88fb357a83bb3f9761963dcc753e095403e4",
    "title": "LFTK: Handcrafted Features in Computational Linguistics",
    "sections": {
      "Introduction": [
        "Handcrafted linguistic features have long been inseparable from natural language processing (NLP) research. Even though automatically-generated features (e.g., Word2Vec, BERT embeddings) have recently been mainstream focus due to fewer manual efforts required, handcrafted features (e.g., type-token ratio) are still actively found in currently literature trend  (Weiss and Meurers, 2022; Campillo-Ageitos et al., 2021; Chatzipanagiotidis et al., 2021; Kamyab et al., 2021; Qin et al., 2021; Esmaeilzadeh and Taghva, 2021) . Therefore, it is evident that there is a constant demand for both 3 Core contributor the identification of new handcrafted features and utilization of existing handcrafted features.",
        "After reviewing the recent research, we observed that most research on automatically-generated features tends to focus on creating deeper semantic representations of natural language. On the other hand, researchers use handcrafted features to create wider numerical representations, encompassing syntax, discourse, and others. An interesting new trend is that these handcrafted features are often used to assist auto-generated features in creating wide and deep representations for applications like English readability assessment  (Lee et al., 2021)  and automatic essay scoring  (Uto et al., 2020) .",
        "The trend was observed across various tasks and languages. For example, there are Arabic speech synthesis  (Amrouche et al., 2022) , Burmese translation  (Hlaing et al., 2022) , English-French term alignment  (Repar et al., 2022) , German readability assessment  (Blaneck et al., 2022) , Italian pre-trained language model analysis  (Miaschi et al., 2020) , Korean news quality prediction  (Choi et al., 2021) , and Spanish hate-speech detection  (García-Díaz et al., 2022)  systems.",
        "Though using handcrafted features seems to benefit multiple research fields, current feature extraction practices suffer from critical weaknesses. One is the inconsistent implementations of the same handcrafted feature across research works. For example, the exact implementation of the average words per sentence feature can be different in  Lee et al. (2021)  and  Pitler and Nenkova (2008)  even though both works deal with text readability. Also, there have been no standards for categorizing these handcrafted features, which furthers the confusion.",
        "In addition, no open-source feature extraction system works multilingual, though handcrafted features are increasingly used in non-English applications. The handcrafted linguistic features can be critical resources for understudied or lowresource languages because they often lack highperformance textual encoding models like BERT. In such cases, handcrafted features can be useful in creating text embeddings for machine learning studies  (Zhang et al., 2022; Kruse et al., 2021; Maamuujav et al., 2021) . In this paper, we make two contributions to address the shortcomings in the current handcrafted feature extraction practices.",
        "1. We systematically categorize an extensive set of reported handcrafted features and create a feature extraction toolkit. The main contribution of this paper is that we collect more than 200 handcrafted features from diverse NLP research, like text readability assessment, and categorize them. We take a systematic approach for easiness in future expansion. Notably, we designed the system so that a fixed set of foundation features can build up to various derivation features. We then categorize the implemented features into four linguistic branches and 12 linguistic families, considering the original author's intention. The linguistic features are also labeled with available language, depending on whether our system can extract the feature in a language-agnostic manner. LFTK (Linguistic Feature ToolKit) is built on top of another opensource library, spaCy 1  , to ensure high-performance parsing, multilingualism, and future reproducibility by citing a specific version. Our feature extraction software aims to cover most of the generally found handcrafted linguistic features in recent research.",
        "2. We report basic correlation analysis on various task-specific datasets. Due to the nature of the tasks, most handcrafted features are from text readability assessment or linguistic analysis studies with educational applications in mind. The broader applications of these handcrafted features to other fields, like text simplification or machine translation corpus generation, have been only reported fairly recently  (Brunato et al., 2022; Yuksel et al., 2022) . Along with the feature extraction software, we report the predictive abilities of these handcrafted features on four NLP tasks by performing a baseline correlation analysis. As we do so, we identify some interesting correlations that have not been previously reported. We believe our preliminary study can serve as a basis for future in-depth studies.",
        "In a way, we aim to address the recent concern about the lack of ready-to-use code artifacts for handcrafted features  (Vajjala, 2022) . Through this work, we hope to improve the general efficiency of identifying and implementing handcrafted features for researchers in related fields."
      ],
      "Conclusion": [
        "In this paper, we have reported our open-source, large-scale handcrafted feature extraction system. Though our extraction system covers a large set of pre-implemented features, newer, task-specific features are constantly developed. For example, URLs count is used for Twitter bot detection  (Gilani et al., 2017)  and grammatical error count is used for automated essay scoring  (Attali and Burstein, 2006) . These features, too, fall under our definition (Figure  2 ) of handcrafted linguistic features.",
        "Our open-source script is easily expandable, making creating a modified, research-specific version of our extraction program more convenient. With various foundation features to build from, our extraction program will be a good starting point.",
        "Another potential user group of our extraction library is those looking to improve a neural or nonneural model's performance by incorporating more features. Performance-wise, the breadth of linguistic coverage is often as important as selection  (Lee et al., 2021; Yaneva et al., 2021; Klebanov and Madnani, 2020; Horbach et al., 2013) . Our current work has various implemented features, and we believe the extraction system can be a good starting point for many research works.",
        "Compared to other historically important code artifacts like the Coh-Metrix  (Graesser et al., 2004)  and L2 Syntactic Complexity Analyzer  (Lu, 2017) , our extraction system is comparable or larger in size. To the best of our knowledge, this research is the first attempt to create a \"general-purpose\" handcrafted feature extraction system. That is, we wanted to build a system that can be widely used across NLP tasks. To do so, we have considered expandability and multilingualism from architecture design. And such consideration is grounded in the systematic categorization of popular handcrafted linguistic features into the attributes like domain and family. With the open-source release of our system, we hope that the current problems in feature extraction practices (section 1) can be alleviated."
      ]
    }
  },
  {
    "paperId": "d89f6b942c4a8c0b09945462688290484493ed6b",
    "title": "The ACL OCL Corpus: advancing Open science in Computational Linguistics",
    "sections": {
      "Introduction": [
        "Building scholarly corpora for open research accelerates scientific progress and promotes reproducibility in research by providing researchers with accessible and standardized data resources. Driven by advancements in natural language processing and machine learning technologies, the computational linguistics (CL) discipline has experienced rapid growth in recent years. This rapid growth underpins the importance of having a scholarly corpus in the CL domain for ensuring sustainable progress.",
        "The ACL Anthology 2 is an important resource that digitally archives all scientific papers in the CL domain, including metadata, PDF files, and supplementary materials. Previous scholarly corpora built on it, such as the Anthology Reference Corpus (ARC;  Bird et al., 2008)   Author Network  (AAN; Radev et al., 2009) , extend its utility by providing full texts, citation and collaboration networks. However, both are becoming obsolete due to their outdated text extraction methods and insufficient updates.",
        "We present the ACL OCL (or OCL for short), an enriched and contemporary scholarly corpus that builds upon the strengths of its predecessors while addressing their limitations. The OCL corpus includes 73,285 papers hosted by the ACL Anthology published from 1952 to September 2022. The OCL further provides higher-quality structured full texts for all papers, instead of previous stringformatted ones, enabling richer textual analyses. For instance, higher-quality full texts better foster the development of document-level information extraction tasks  (Jain et al., 2020; Das et al., 2022) . The structured information in full texts, such as sections and paragraphs, facilitates section-wise tasks such as related work generation (Hoang and  Kan, 2010)  and enables fine-grained linguistic analyses Name #Doc. Text Type Linked KG Fig.  Peer  Source Domain RefSeer  (Huang et al., 2015)  1.0M string CiteSeerX × partial WWW multi S2ORC  (Lo et al., 2020)  8.1M structured S2AG × partial multi multi CSL  (Li et al., 2022)  396K -self × all CCJ multi unarXive  (Saier et al., 2023)  1.9M structured MAG × partial arXiv multi ACL ARC  (Bird et al., 2008)     (Jiang et al., 2020a) .",
        "In addition, to advance multimodal research such as figure caption generation  (Hsu et al., 2021) , OCL extracts 210K figures from its source documents. Furthermore, we link OCL to large-scale scientific knowledge graphs to enrich OCL with external information. In particular, we consider information such as citation data from a larger scholarly database (e.g., Semantic Scholar) and linkage to other platforms (e.g., arXiv).",
        "To showcase the scientific value of the OCL corpus, we illustrate its utility through a downstream application of temporal topic trends  (Hall et al., 2008; Gollapalli and Li, 2015)  within the CL domain. We first train a pre-trained language model (PLM) based topic classification method on a subset of 2,545 scientific papers with ground truth topic labels. We then extrapolate and integrate the model predictions as silver-labeled topic information in the OCL.",
        "The contributions of this paper are as follows:",
        "• We construct the ACL OCL, which augments the source ACL Anthology. The OCL provides structured full text for 73.3K CL papers, enriches metadata originated from Anthology by linking to an external knowledge graph, and extracts 210K figures. We also analyze the OCL, disclosing its statistics and the quality of full texts.",
        "• We conduct a case study on OCL's temporal topic trends, showing the emergence of new topics like \"Ethics\" and witnessing the past glory of \"Machine Translation\". We validate the importance of supervised data in topic classification. Model-predicted silver topic labels are released together with OCL."
      ],
      "Related Work": [
        "Scholarly datasets typically fall into two categories: task-specific and open research-oriented. The former, designed to serve one task, includes selective information of scientific papers such as abstract and citation strings, paired with task-specific outputs such as entities  (Hou et al., 2021) , summaries  (Cachola et al., 2020)  and citation intent labels  (Cohan et al., 2019)    1 . The OCL distinguishes itself from other corpora by the target domain, focusing on the computational linguistic domain same as ARC and AAN 3  . Their common source (i.e., ACL Anthology) provides higher-quality scientific papers, which are all peerreviewed by domain experts. In contrast to them, the enriched and updated OCL corpus includes more papers and information.",
        "Inspired by and following S2ORC, the OCL provides structured full texts with the scientific documents' discourse structure (i.e., sections), which enables more extensive textual analysis. In contrast to corpora that rely solely on internal papers for cita-tion networks and thus limit their completeness, the OCL is linked to a large knowledge graph to overcome the constraints. Furthermore, multi-modal features such as figures are extracted to foster research in document layout and multi-modality."
      ],
      "Discussion": [
        "We present statistical analyses of the OCL corpus, including the distribution of papers across years and linguistic distribution. We further highlight the quality of full texts and citation graph analysis.",
        "We analyze the trend of model-predicted research topics in OCL starting from 2000 to 2021. Figure  4  presents the popularity (estimated by publication percentage) of all topics across years, subgrouped into recognizable trend patterns. We first introduce waning topics in Figure  4a , including both pre-dominant ones like \"Syntax\" and \"Resource\" and underrepresented ones such as \"Discourse\" and \"Speech\". Another predominant topic in CL \"MT\", shown in Figure  4b , which peaks (around 19%) in the 2010s and declines in the latter years. From all the remaining underrepresented topics, we further classify them into three types, only two types are shown in Figure  4 . Figure  4c  shows resurgent topics including \"Dialogue\", \"Interpret\", \"ML\" and \"NLG\", which have declining/low interests before 2015 but increased afterward. Figure  4d  shows topics including \"CompSocial\", \"IE\", \"Sentiment\" and \"SenSem\", which are underrepresented historically and become noticeable later. Among them, \"Ethics\" is a very new and small topic starting in 2016. In contrast, \"QA\", \"LexSem\", \"Summ\" and \"IR\" (not shown) are relatively stable research topics with mild corrections in recent years. Among all topics, the popularity of \"Syntax\" drops the most (20%→2%) while \"CompSocial\" increases the most (1%→10%)."
      ],
      "Methods": [
        "Given the absence of large-scale topic-labeled data, our initial investigation focuses on zero-shot document classification methods.  Yin et al. (2019)  fine-tuned BART  (Lewis et al., 2020)  on natural language inference datasets, thus achieving zeroshot prediction of many tasks including document classification. We follow their work and use a variant model BART-large-MNLI 11  to model the topic classification task as an inference task. To identify the topic label of a document d, the BART-large-MNLI model predicts the probability p(l|d) that the hypothesis label l is entailed from the premise document d. We denote this unsupervised method as BART-NLI-0shot.",
        "Inspired by the label-partially-seen experimental settings in  Yin et al. (2019) , we establish a semisupervised setup leveraging the limited labeled data ( §6) for improved performance. Specifically, we fine-tuned the BART-large-MNLI model with the labeled data, which is tailored to fit the NLI task. We refer to this semi-supervised method as BART-NLI-FT.",
        "After obtaining over 2000 documents with groundtruth topic labels, we train a supervised model for topic classification. As salient information of documents, keywords are shown to be helpful for many tasks such as classification  (Zhang et al., 2021) , clustering  (Chiu et al., 2020) , summarization  (Litvak and Last, 2008; Liu et al., 2021)  the most suitable topic. We explore different keyword extraction methods including TF-IDF and Yake!  (Campos et al., 2020) , both of which are simple and efficient.",
        "Given the proven success of pre-trained language models (PLMs) in multiple NLP tasks, particularly with training data in a small scale, we utilize a PLM-based classification framework for our task.",
        "The framework employs a pre-trained language model to encode the input document and a softmax classification layer atop it for topic label prediction. In addition, we consider pre-trained language models trained from scientific documents, namely SciBERT  (Maheshwari et al., 2021)  and SPECTER  (Cohan et al., 2020) , to take advantage of their strong encoding power of domain-specific documents."
      ],
      "Results": [
        "Topic Data Curation. We crawl published papers of several online held CL conferences (e.g., ACL 2020, EACL 2021) between 2020 and 2022, together with their topics from those websites. After aligning those papers with the data in the ACL OCL, we obtained 2545 documents classified in 21 topics in total, present in Table  4 . These documents together with their topics are used as our training and testing data. We use 5-fold cross-validation across all experiments, randomly selecting 2,036 (80%) papers balanced in each topic as our training set, and the remaining 509 (20%) as test. We use Macro F1, Weighted F1, and Accuracy (aka. Micro F1) as evaluation metrics for the multi-class topic classification. We rely on Accuracy to compare systems' performances."
      ],
      "Conclusion": [
        "We introduce ACL OCL, a scholarly corpus aiming to advance open research in the computational linguistics domain. The structured full texts, enriched metadata from Semantic Scholar Academic Graph, and figures provided by OCL can benefit existing research tasks as well as enable more opportunities.",
        "We highlight the utility of OCL in temporal topic trends analysis in the CL domain. The topics are generated by a trained neural model with a small yet effective scientific topic dataset. By analyzing the topics' popularity, we ask for more attention on the emerging new topics such as \"Ethics of NLP\" and the declining underrepresented topics like \"Discourse and Pragmatics\" and \"Phonology, Morphology and Word Segmentation\".",
        "In the future, we will work on the data currency of OCL, which aims to keep OCL up-to-date with the ACL Anthology data. We plan to update OCL by year to keep it alive. The ultimate solution is to provide full-text extraction and information extraction APIs to ACL Anthology, thus hosting the OCL data on ACL Anthology itself.",
        "The OCL corpus is a small-scale collection of documents specifically focusing on peer-reviewed and open-access CL papers. As a result, it is not a comprehensive corpus like S2ORC  (Lo et al., 2020) , since it does not include any other sources beyond the ACL Anthology. In the future, the OCL could be expanded by incorporating CL papers on arXiv (e.g., cs.CL), which is related to unarXive  (Saier et al., 2023) . The challenge is how to filter out arXiv papers of low quality.",
        "To ensure the extraction of high-quality full texts from the provided PDF files, the OCL corpus utilizes the most advanced open-sourced PDF2text toolkit, GROBID. Due to constraints on budget, only open-source toolkits are considered, although it is acknowledged that some paid PDF2text services might yield higher-quality full texts. In addition, previous work such as unarXive use L A T E X files as source documents to avoid PDF2text.",
        "The OCL corpus is an unlabeled resource, lacking annotations for specific tasks that require labels. Given the demonstrated capabilities of large lan-guage models (LLMs), we suggest that LLMs can play an instrumental role in generating high-quality, large-scale silver labels  (Yu et al., 2023) . Moreover, human-AI collaborative annotations  (Liu et al., 2022)  provide an effective strategy for complex tasks like natural language inference."
      ]
    }
  },
  {
    "paperId": "7c9975fedb81929f0115f1fb1f7b7535121f8c4e",
    "title": "Reproducibility in Computational Linguistics: Is Source Code Enough?",
    "sections": {
      "Introduction": [
        "Modern natural language processing and computational linguistics research progresses at a rapid pace, making it expedient to build upon the groundwork of earlier research rather than reinventing solutions from scratch. This is complicated by the field's reliance in recent years on deep learning models that are difficult to interpret and sensitive to small changes in architecture and environment. These distinct characteristics hinder their reproducibility, as do the substantial computing resources often required to replicate them. Thus, many strong NLP models fall short on two crucial parameters: accessibility and reproducibility.",
        "Although deep learning models today allow for effective processing in highly complex search spaces and in most cases outperform solutions from the past, researchers must consider the risks and potential ethical implications associated with their use alongside the performance benefits. There have been numerous calls and attempts  (Dodge et al., 2019; Rogers et al., 2021)  to push the community into taking more responsibility for the reproducibility of their research. Several venues have introduced and adapted reproducibility checklists and standards  (Pineau, 2019; Stojnic, 2022)  into their submission process  (Nature, 2022; NeurIPS, 2022; AAAI, 2022; ACM, 2022; ACL, 2022a; Deutsch et al., 2022) , and others have organized reproducibility challenges  (Sinha et al., 2021; Belz et al., 2021)  to encourage the community to reproduce published research. Although this is undeniably a step forward, it is unclear how large of a step it has been. There is also some concern that asking reviewers to evaluate reproducibility burdens them with another time-consuming task that might extend beyond their expertise.",
        "In this work, we analyze and report on the state of reproducibility in NLP. We investigate the extent to which content necessary for reproducing research is currently available, and study the influence of reproducibility checklists on this availability over the last seven years. We also conduct an eight-paper reproducibility case study to develop a deeper understanding of current strengths and weaknesses in research reproducibility in NLP. Our key contributions include the following:",
        "• We scrape the ACL Anthology for data associated with all papers published at major venues in the last seven years. Then, we investigate the impact of the introduction of reproducibility checklists into the paper submission process, by analyzing trends in the source code availability of the accepted papers.",
        "• We randomly select eight papers from the 2021 Conference on Empirical Methods in Natural Language Processing  (Moens et al., 2021a , EMNLP 2021 ) and attempt to reproduce their reported results. We find that despite the recent progress towards reproducibility, most released artifacts are of low quality. We make the artifacts from our own repro-ductions publicly available as self-contained Docker containers.",
        "• We propose four recommendations to address major issues affecting reproducibility.",
        "Our recommendations include incorporating small-scale experiments in papers to increase accessibility, creating well-documented scripts to reproduce reported results, publishing executable selfcontained artifacts, and embedding artifact evaluation in the publication pipeline. We elaborate on our study process and findings in the remainder of this paper. Our hope is that this paper serves as a call to action for systemic improvements to research reproducibility in NLP.",
        "Numerous studies have assessed the reproducibility of scientific publications. This task often involves attempting to achieve results close enough to the ones reported in the paper with little to no reliance on the released software artifacts, if available.  Raff (2019)  attempts to quantify the reproducibility ratio of 255 papers published from 1984 to 2017. He selects different thresholds for a minimal acceptable error for algorithmic and empirical claims, ultimately reporting a 63% reproducibility ratio. In a similar study,  Wieling et al. (2018)  survey 395 papers presented at the ACL 2011 and 2016 conferences and identify whether links to data and code were provided. Then, they attempt to reproduce the results of ten papers using provided code and data. They ultimately find results close to those reported for six papers.  Olorisade et al. (2017)  attempt to independently investigate the claims of six studies on text mining for citation screening. In the authors' words, 27% of machine learning papers lack the necessary information required for achieving reproducible results; hence, they introduce a checklist to help mitigate this issue. The challenge of dealing with missing information has also been brought up by  Gundersen and Kjensmo (2018) . Utilizing checklists and guiding authors towards better standards during the paper submission process has become a common practice amongst several venues  (Nature, 2022; AAAI, 2022; ACM, 2022; ACL, 2022b; Pineau et al., 2021) . Aside from guidelines, communities have organized reproducibility challenges  (Sinha et al., 2021; Belz et al., 2021 ) that attempt to promote improved reproducibility across the field.",
        "While this increased attention towards openness and availability is a welcome change, the lack of consensus on terms and definitions has diminished progress.",
        "There are a variety of definitions of and perspectives on reproducibility.  Rougier et al. (2017)  define reproducing as running the same software on the same input data and obtaining the same results. Replicating then is limited to running new software and achieving results judged as similar enough by an expert in the field. The Association for Computing Machinery (ACM) (ACM, 2022) considers the team and the experimental setup as contributing factors; furthermore, they add another term, repeatability, to the glossary of definitions.  Whitaker (2017)  and Schloss (2018) introduced two additional concepts known as robustness and generalisability to cover other missing dimensions.",
        "These definitions attempt to cover an open-ended number of dimensions. Therefore, they often do not age well and become obsolete upon the arrival of a more comprehensive definition  (Belz et al., 2022) .  Belz et al. (2022)  suggest the community should adapt the definitions provided by the International Vocabulary of Metrology  (JCGM, 2012, VIM) . VIM defines reproducibility as a measurement precision under reproducibility conditions of measurement. These conditions must be known and recorded and include but are not limited to the source code, hyperparameters, dependencies, and runtime environment. As a result, this framework enables the use of precision as a measure of statistical variability to quantify how close results are to one another. Doing so will provide far more information than just binary assessments of whether research is reproducible or irreproducible. a virtual machine (e.g., VMWare or Virtual-Box) or within a container (e.g., Docker) so that they run \"out-of-the-box\" on any machine. Thus, research artifacts are much more than simply releasing the source code for the prototype on a public website (i.e., GitHub).   et al., 2022, ASPLOS) ) allow authors to submit artifacts, typically after the paper is accepted. The quality of the submitted artifacts is then assessed by the artifact evaluation committee (AEC), who run the artifacts following the authors' instructions to judge whether the artifact reproduces all the results reported in the paper, and whether the artifact can be reused for future research on the same subject. At the recommendation of the AEC, published papers then get a set of badges on the first page. As a result, reproducing and building on research papers with an AEC badge should be straightforward: users should simply need to download the artifact and follow the instructions.",
        "Conferences in the broad areas of NLP and machine learning have recently adopted reproducibility checklists  (AAAI 2022 (AAAI, 2022 ), NeurIPS 2022 (NeurIPS, 2022) ) which require authors to provide an appendix with the source code used in the experiments. Additionally, they recommend the inclusion of dependency specifications, training and evaluation code, pre-trained models, and a README file containing the information required to achieve the results in the paper. Unfortunately, as we show in the following sections, reproducing publications that follow these good practices is not easy. It requires a considerable amount of engineering involving fixing compilation errors, \"guessing\" configurations not documented, figuring out which code to execute for which experiment, manually processing experimental data, and dealing with obsolete tools or libraries.",
        "Given the numerous challenges one faces to re-produce a research paper's results, there has been a recent surge of venues solely focused on reproducibility (e.g., the ML Reproducibility Challenge 2021  (Sinha et al., 2021)  and ReproGen 2021  (Belz et al., 2021) ), in which participants attempt to reproduce results of a selected published paper. Unlike the research artifacts described above, these reproducibility efforts do not lead to a self-contained and self-executable artifact. Self-contained and self-executable artifacts should be capable of reproducing the results in short order while avoiding vulnerability to \"bit rotting\" as time passes and widely available tools and libraries grow obsolete."
      ],
      "Methods": [
        "We adapt the Metrology-based Reproducibility Assessment originally proposed by  Belz et al. (2022)  to achieve our reported results. In this framework, reproducibility is a measurement of precision and is directly connected to the conditions in which the measurement is being recorded. In the context of machine learning and natural language processing, these conditions include but are not limited to source code, trained models, evaluation methods, and datasets. Furthermore, repeatability is defined as a special case of reproducibility where the conditions across different measurements are the same. This formulation enables the use of common terms used in reporting precision. We report and focus on coefficient of variation (CV * ), defined as the unbiased sample standard deviation over the mean.",
        "Ultimately, the gold standard for scientific artifacts is to have the highest level of reproducibility precision (or low variability). However, this standard is not practical. An alternative approach is to gradually push the community towards improving existing reproducibility standards. Existing reproducibility tracks are a great example of this effort. One of the primary objectives of the reproducibility movement is to increase availability of data and source code. Source code is an integral part of the reproducibility process since it allows other researchers to implement or execute conditions and details that may have been omitted in the publication itself. Here, we investigate trends in source code availability for scientific papers published in major conferences of the Association for Computational Linguistics (ACL) in recent years, using information from the ACL Anthology. Observing these trends is especially interesting since not all ACL conferences have introduced a reproducibil-ity track in their submission process. We expected to observe a higher code availability ratio among venues that actively promote reproducibility.",
        "Following this, we switch our focus and investigate whether the released source code is enough to reproduce the reported results. Empirical results have become harder to reproduce over time for known and unknown reasons (several of these reasons are discussed in Section 5.2). Although we quantify reproducibility using CV * , we consider a reproducibility attempt successful if we are able to achieve any results. We understand that this process is not comprehensive-we cannot determine absolute reproducibility, since one may be able to spend days debugging a specific source code to ultimately achieve the reported results. However, absolute reproducibility and its time cost is not feasible at scale or for many researchers. In our case study, we clearly explain all steps taken by following instructions provided by the authors. We raise any questions or issues we face along the way, opening direct lines of communication with authors to overcome challenges and improve the reproducibility of the released source code as part of our process. We hypothesize that despite the increased availability of source code for papers at large NLP conferences, achieving reproducible results may still be an extremely challenging task.",
        "For our case study, we randomly selected eight papers from the 2021 Conference on Empirical Methods in Natural Language Processing  (Moens et al., 2021a) . We tried to reproduce their results using the code and instructions provided. Out of over 1300 accepted papers at EMNLP 2021, 723 had URLs to a repository containing the code required to run their experiments. Our selected papers are provided below. To save space, we refer to each paper by its associated number in this list. While random selection of the papers avoids inserting selection bias into our findings, it may increase the difficulty of achieving reproducibility due to lower familiarity with certain concepts. We allotted fixed, limited time and computation resources for each paper, and report whether we were able to reproduce the findings of the paper within our time and resource budget."
      ],
      "Results": [
        "In this section we describe the results obtained, and how they answer the following research questions (RQs):",
        "RQ1 Has code availability for published NLP literature improved in recent years?",
        "RQ2 Is code availability enough for reproducibility?",
        "RQ3 What new guidelines can be used to support reproducibility?  For perspective, Table  1  shows the total number of accepted papers at each of these conferences over the same time span. RQ1: Figure  1  shows a positive trend in source code availability. Moreover, EMNLP, ACL, and NAACL appear to have the highest code submission ratio, which suggests that highlighting the importance of reproducibility throughout the submission process does lead to increased availability of source code. The surge of the number of published papers with code is especially noticeable for EMNLP 2020 and ACL 2021."
      ],
      "Discussion": [
        "It is encouraging to observe the recent upward trend in releasing source code across the broad NLP community, thanks to the recent focus on reproducibility. Unsurprisingly, conferences not promoting reproducibility standards have fewer submissions with included code. Therefore, we encourage all conferences to include such standards in their submission process. Unfortunately, our results suggest that submitting the code alone does not seem to be enough, as the released code does not meet a minimum requirement for reproducibility, defined as achieving the results reported in the paper using the provided source code. We believe it is time to improve reproducibility standards to address the concerns we raise in regards to quality of the released source code. In particular, we strongly suggest shifting the focus from source code to research artifacts which include (1) a self-contained runtime environment (e.g., a Docker container or a VM image) with scripts for achieving every single result reported in the paper, (2) smaller experiments to quickly validate the integrity of the artifact, and (3) extensive documentation explaining how to run the code. We took a first step in this direction by packaging all the results in the paper in their own artifact, and releasing it with this paper. These artifacts are available through Zenodo  (Arvan et al., 2022) .",
        "Reproducibility is a desired attribute for solutions in natural language processing, but it comes with a cost. There may exist cases in which the required conditions for reproducing the results are not practical. Defining what is practical, of course, depends on the problem at hand. For example, reproducing a named entity classifier that requires using the same hardware may not be considered practical. This phenomenon is quite similar to the bias-variance tradeoff. Bias-variance, a property of statistical and machine learning models, suggests that the variance of the parameter estimated across samples can be reduced by increasing the bias. A dilemma exists when trying to minimize these two sources of error simultaneously. We have a dilemma when it comes to assessing the reproducibility of results. Many attempts have focused on controlling all the variables. Yet, while they have their use cases, their complexity makes them less viable. Perhaps a better alternative is to reduce the emphasis on the top-performing results and utilize techniques that attempt to aggregate and report the results of a set of experiments."
      ],
      "Conclusion": [
        "In this paper, we examined the trends of source code availability at major NLP conferences in recent years. We also performed a reproducibility study on eight randomly selected papers. After achieving a 25% success rate, we draw attention to the primary causes of irreproducibility of the selected papers. Based on the findings from our case study, we close by providing recommendations for improving the state of reproducibility in NLP. Researchers concerned about the added workload should come to terms with the fact that accessibility and reproducibility are fundamental, rather than auxiliary, cornerstones of strong research. It is also advantageous to researchers: following the recommendations proposed in this paper is anticipated to prolong and future-proof work, minimizing the required support in the long term.",
        "We have done our best to provide logical and stepby-step reasoning to describe our work. However, we have also identified a few limitations. First, some papers may have released their source code but not included it in their submission process, leading to inaccuracies in the trends computed based on data scraped from the ACL Anthology portal. Additionally, our reproducibility analysis only examines a small number of recent papers. To avoid the risk of selection bias, we selected the papers randomly. We believe our results were obtained from a representative sample. We have included additional information for each selected paper in Appendix A.",
        "We failed to reproduce the results for six papers (75%), which may be attributed to our own lack of expertise. We allotted a similar amount of effort and time to each paper. We used this time to fix the issues and contact the authors in case we were not able to do so. In theory, we could have devoted a large enough amount of effort to each paper to reproduce it successfully. However, in practice and without help from the authors, it is unclear how long this would take, and we believe that such an approach would amount to a complete reimplementation of the original paper, which is outside of the scope of this work.",
        "Finally, in our recommendations we emphasized the importance of self-contained environments. However, this may not be achievable in every case. For instance, in the healthcare domain, datasets often contain private and sensitive information. Releasing such datasets is not an option, and thus achieving the same degree of reproducibility is not possible in those fields."
      ]
    }
  },
  {
    "paperId": "284fcc96d0e51902b466ba15ac76973086a5841d",
    "title": "Computational linguistics and discourse complexology: Paradigms and research methods",
    "sections": {
      "Introduction": [
        "The article addresses modern trends in computational linguistics, language and discourse complexity. It also provides a brief overview of the articles in the issue.",
        "Computational linguistics (hereinafter CL), as the name implies, is an interdisciplinary science at the intersection of linguistics and computer sciences. It explores the problems of automatic processing of linguistic information. Another commonly used name for this discipline, that is synonymous with the term \"computational linguistics\", is Natural Language Processing (NLP). In a number of research works these concepts are separated, considering that CL is more of a theoretical discipline, and NLP is of a more applied nature. CL began to develop in the early 1950s, almost immediately after the advent of computers. Its first task was development of machine translation, and translation of journals from Russian into English in particular. The initial stage of CL development is comprehensively presented in J.  Hutchins (1999) . It surely was beyond the capacity of researchers to solve the problems of machine translation very quickly, and the initial optimism turned out to be groundless, although in recent years it has become possible to obtain translations of acceptable quality. However, within 70 years of development, CL has achieved significant success in solving many urgent practical problems, which made it one of the most dynamically developing and important research areas in both linguistics and computer science. In our opinion, the best monographs on CL are  (Clark et al., Indurkhya & Damerau 2010) . The latest review, including also an analysis of the prospects for its development, can be found in the article by  Church and Liberman (2021) .",
        "In the review of computational linguistics trends, we focus on the following aspects of research: application-oriented tasks, methods, resources, contribution of theoretical linguistics to computer linguistics, and application of deep learning neural networks. The latter appeared about 10 years ago  (Schmidhuber 2015)  and revolutionized research of artificial intelligence, including many areas of CL. Artificial neural networks constitute a formal model of biological networks of neurons. Their most important feature is the ability to learn; in case of an error, the neural network is modified in a certain way. Although neural networks were proposed as early as 1943, a breakthrough in their use was made only a few years ago. It is associated with the three following factors: the emergence of new, more advanced 'self-learning', unsupervised training algorithms, improved performance of computers, and Internet database increase. Advances in NLP in the late 2018 were mainly related to BERT  (Devlin et al. 2018) , a neural network pre-trained on a corpus of texts. Currently, BERT and its enhanced models show better performance on many NLP problems (see  Lauriola, Lavelli & Aiolli 2022) ."
      ],
      "Methods": [
        "All CL methods can be divided into two large classes: a class based on dictionaries and rules (templates) and a class based on machine learning. These two classes are fundamentally different in their approaches. Dictionaries and rules use accumulated knowledge about the language, as well as results of highly professional manual labor, and therefore they are extremely expensive. Machine learning is implemented on a large number of examples, presented in annotated corpora which function as training sets. The algorithm implies analyzing training sets, identifying the existing patterns and then offering solutions to the problems set. Modern machine learning systems vary in their functions and applications, although deep learning neural networks have proved to be the most efficient. At an input node of a neural network, any language data is fed in encoded forms as tokens: letters, bigrams, short high-frequency morphemes, and words.",
        "Application of this approach depends on a large body of annotated texts at a researcher's disposal: the larger the training set, the better the neural network will learn. At the same time, annotation is quite simple and its implementation does not necessarily involve professional linguists as researchers can refer to services of native speakers.",
        "In this article, we will focus on the basic methods of CL and refer readers to the above-mentioned monographs for a detailed review of the area (cf.  Clark et al. 2013 , Indurkhya & Damerau 2010) .",
        "Automatic text analysis usually begins with its pre-processing which includes text segmentation, i.e. segmentation into words and sentences. Though it may seem like a simple task, since words are separated from each other by spaces and sentences begin with a capital letter and end with a period (rarely, exclamation marks, question marks, ellipsis) followed by a space. The most typical example of the rule or pattern is the following: a period -space -capital letter. However, it is not that simple. A period can be in the middle of a sentence after the first initial, followed by a space and then a capitalized second initial. Here, the period does not explicitly indicate the division of the text into sentences. As an example, we can refer to the following sentence: \"Lukashevich N.V., Levchik A.V. Creation of a lexicon of evaluative words of the Russian language RuCentilex // Proceedings of the OSTIS-2016 conference. pp. 377-382\". Despite all the difficulties, the segmentation problem is considered to be practically solved.  In 1989 , Riley (1989)  managed to achieve a 99.8% accuracy rate for splitting texts into sentences. To achieve this result, the researcher developed a complex system of rules taking into account the following features: length of the word before the dot, length of the word after the dot, presence of a word before the dot in the dictionary of abbreviations, etc.",
        "The next step in the course of text analysis is morphological. Consider, as an example, a language with complex morphology -Russian. For the Russian language, morphological analysis is performed by a number of analyzers: MyStem, Natasha, pymorphy2, SpaCy, etc. In CL, morphological analysis, the purpose of which is to determine the morphological characteristics of a word, is based on a detailed description of inflectional paradigms. For the Russian language, a reference book of this kind is Zaliznyak  (1977) , which presents paradigm indices of almost 100,000 lemmas of the Russian language. The presence of such a directory made it possible to generate about 3 mln Word forms for the registered lemmas of the Russian language. Automatic text analysis finds a lemma corresponding to any word form and a complete list of morphological characteristics. The main challenge for the existing analyzers is homonymy, which the available parsers have not solved yet. And in situations when users require not all parsing options but one, analyzers produce the variant of morphological parsing of the highest frequency, still ignoring senses of the word in the context.",
        "Another problem is parsing of the so-called \"off-list\" words, i.e. words not registered in the dictionary. Given that the average number of such words is about 3%, their morphological analysis requires developing special algorithms. The simplest solution foreseen is the following: based on the analysis of its flexion, the off-list word is assigned its morphological paradigm.",
        "Syntactic parsing, or parsing, is much more complex. The result of syntactic parsing of a sentence is a dependency tree that presents a sentence structure either in the formalism of a generative grammar or in the formalism of a dependency grammar (cf.  Tesnière 2015) . Parsing requires a detailed description of the syntax of the language. The most successful analyzer for the Russian language is ETAP developed by the Laboratory of Computational Linguistics of the Institute for Information Transmission Problems of the Russian Academy of Sciences as a result of over 40 years of research. Its latest version, ETAP-4, is available at (ENA, June 6, 2020) 3  . ETAP parser is based on the well-known model \"Meaning ⇔ Text\"  (Melchuk 1974) , its formalized version is described in the monograph by  Apresyan (1989) .",
        "In the recent decade, parsing has also been performed by neural networks (cf. Chen & Manning 2014) trained on syntactically annotated corpora. English Penn Treebank (ENA, June 6, 2022) 4  is used for English. For the Russian language, one can use SynTagRus (ENA, June 6, 2022) 5  , developed by the Laboratory of Computational Linguistics at the Institute for Information Transmission Problems RAS.",
        "The task of semantic analysis is even more difficult. However, if we want the computer to \"understand\" the meaning, it is necessary to formalize semantics of words and sentences. The problem is solved in two classical ways. The first was initiated by C.  Fillmore (1968) , who introduced concepts of semantic cases or roles of noun phrases in a sentence. The correct establishing of semantic roles is an important step towards sentence comprehension. Fillmore's original ideas were realized in FrameNet lexical database (ENA, June 6, 2022)  6  .",
        "The second approach was implemented in an electronic thesaurus, or lexical ontology, WordNet  (Fellbaum 1998)  which was originally designed for the English language. Subsequently its analogues were developed for many languages. There are numerous analogues of WordNet for the Russian language, the most effective and being widely used is RuWordNet thesaurus (ENA, June 6, 2022) 7  , (cf.  Loukachevitch & Lashevich 2016) , comprising over 130,000 words. WordNetlike thesauri explicate semantic relationships between words (concepts) including synonymy, hyponymy, hypernymy, etc., and their systemic parameters partially define their semantics. WordNet has been successfully implemented in a large number of both linguistic and computer research.",
        "The idea of vector representation of semantics, i.e. word embeddings, has been proposed recently. Its core is constituted by the distributive hypothesis: linguistic units occurring in similar contexts have similar meanings  (Sahlgren 2008) . This hypothesis has been confirmed in numerous studies aimed at defining frequency vectors of words registered in large text corpora. There are multiple refinements and computer implementations of the idea, the most popular of which is word2vec  (Mikolov et al. 2013)  available in Gensim library (ENA, June 6, 2022) 8  . RusVectores system  (Kutuzov & Kuzmenko 2017) , available at (ENA, June 6, 2022) 9  identifies vector semantics for Russian words. Specifically, RusVectores evaluates semantic similarity of words.",
        "Obviously, the most important tool for research in CL, as indeed in all modern linguistics, are text corpora. The first corpus compiled in the 1960s was Brown Corpus which when released contained one million words. Since then, corpora size requirements have increased dramatically. For the Russian language, the most well known is the National Corpus of the Russian Language (NCRL, ENA, June 6, 2022 10  ). Created in 2004, it is being constantly updated and currently includes over 600 mln words. In 2009, Google compiled and uploaded a very interesting multilingual resource, i.e. Google Books Ngram (ENA, June 6, 2022) 11  , containing 500 bln words, 67 bln words of which constitute the Russian sub-corpus (cf.  Michel 2011) .",
        "Another important problem is corpus annotation or tagging, which in difficult cases is done manually. The work is usually carried out by several annotators and their performance consistency is closely monitored (Pons & Aliaga 2021). Despite the fact that corpora have become an integral part of linguistic research, there have been ongoing disputes on their representativeness, balance, differential completeness, subject and genre relatedness, as well as data correctness (cf.  Solovyev, Bochkarev & Akhtyamova 2020) .",
        "Thus, thanks to CL, researchers fully implement numerous services including information retrieval, automatic error correction, etc. This became possible due to fundamentally important accomplishments not only in computer science, but also in linguistics. CL uses extensive dictionaries and thesauri, detailed syntax models, and giant corpora of texts. Automatic morphological analysis in its modern form would not exist without A. A. Zaliznyak's \"Dictionary of the Russian Language Grammar\"  (1977) . Multiple studies in CL are based on manually created WordNet and RuWordNet thesauri. Computer technologies, in turn, contribute to the development of linguistics. Text corpora and statistical methods have already become commonplace; without them serious linguistic research would be impossible.",
        "All key CL technologies are publicly available, e.g. (ENA, June 6, 2022) 12  houses programs to solve numerous basic tasks for numerous languages.",
        "It is not really feasible to cover all the topics of CL, a vast and rapidly developing field of linguistics, in one article. Many important questions have been left beyond. We refer readers interested in the topics of co-reference resolution, disambiguation, topic modeling, etc. to the above-mentioned publications."
      ],
      "Conclusion": [
        "The recent successes of computational linguistics have largely ensured accomplishments in discourse complexology and allowed scientists not only to automate a number of linguistic analysis operations, but also create user-friendly text profilers. Tools such as ReaderBench, Coh-Metrix, and RuMOR (cf. the current issue) are capable of solving both research and practical tasks: selecting texts for target audiences, editing and shortening texts, analyzing cognitive causes of errors, and even suggesting verbal strategies. The algorithms of automatic text profilers are based on classical and machine learning methods, including deep learning neural networks, one of the latest systems of which is BERT. At present, and this is well shown in a number of articles of the special issue, researchers are successfully combining methods of machine learning and the so-called \"parametric approach\".",
        "However, the most important feature of modern research is a vast expansion of research problems and accuracy increase resulting from the abilities of artificial neural networks to learn and modify. Artificial intelligence breakthroughs are attributable to the three main factors: new advanced self-learning algorithms, high computer speeds, and a significant increase in training data. Modern databases, as well as dictionaries and tools for the Russian language developed in recent years, allowed the authors of the special issue to address and successfully solve a number of problems of text complexity.",
        "A solid foundation for success in discourse complexity were findings of cognitive scientists at the beginning of our century which completely changed complexology paradigm. If the main achievement of the XXth century complexology was the idea that \"different types of texts are complex in different ways\", the discourse complexology of the XXIst century proposed and verified complexity predictors for various types of texts and developed toolkits for assessing relative complexity of texts in various communicative situations. With cognitive methods in its arsenal, complexology acquired two additional variables: linguistic personality of the reader and reading environment.",
        "The new research paradigm of linguistic complexology is manifested in those articles of the special issue which are aimed at defining new criteria for text complexity: expert evaluation, comprehension tests and reading speed tests have been replaced by new methods, which allow scholars to identify discourse units affecting text comprehension.",
        "The studies published in the special issue also highlighted the main problems facing Russian linguistic complexology: creating a complexity matrix for texts of various types and genres, expanding the list of complexity predictors, validating new complexity criteria, and expanding databases for the Russian language."
      ]
    }
  },
  {
    "paperId": "44a5d4b7ab16910c101755b0d7d78594f6c58bb3",
    "title": "Computational Linguistics: 16th International Conference of the Pacific Association for Computational Linguistics, PACLING 2019, Hanoi, Vietnam, October 11–13, 2019, Revised Selected Papers",
    "sections": {
      "Introduction": [
        "In this paper, we argue that Higher-Order Unification (HOU) provides a linguistically adequate tool for modeling the semantics of focus. Building up on  (Pulman, 1995) , we develop a unificationbased analysis of focus which we show favourably compares with two prominent theories of focus, Rooth's Alternative Semantics and Krifka's Structured Meanings theory. For data which is generally viewed as a test-bed for focus theory (utterances with multiple focus operators and second occurrence expressions), we show that contrary to Rooth's and Krifka's theories, the HOU treatment yields a transparent analysis while avoiding under-and over-generation."
      ],
      "Discussion": [
        "For computing the Focus Semantic Value, we propose to use Higher-Order Unification. More specifically, given (part of) an utterance U with semantic representation Sem and foci F 1 . . . F n , we require that the following equation, the ground equation, be solved: Sem = Gd(F 1 ) . . . (F n ) Assuming the typed λ-calculus as our semantic representation language, this equation can be solved by Huet's algorithm (cf.  (Huet, 1975) ), thus assigning a value to Gd. On the basis of this value, we can then define the FSV, written Gd, as follows:",
        "Definition 3.1 (Focus Semantic Value) Let Gd be of type α = β k → t and n be the number of foci (n ≤ k), then the Focus Semantic Value derivable from Gd, written Gd, is {Gd(t",
        "As mentioned before, this yields a focus semantic value which is in essence Rooth's Alternative Set 1 .",
        "1 Though in fact, our definition is more syntactic than Rooth. In Rooth's approach, the FSV definition is purely semantic whereas in our approach the FSV is indirectly defined by solving equations and the value thus obtained (i.e. the value of Gd) is a term, that is, a syntactic object. Hence, our FSV can be more accurately compared to Kratzer's presupposition skeleton,. This means that our approach inherits the advantages of Kratzer's approach (cf.  (Kratzer, 1991) ). In par-Finally, we assume as in  (Pulman, 1995) , that foci are stored and discharged nondeterministically as the need arises, thus contributing to the definition of the ground equation. Furthermore, equations are set up at the level at which there are needed e.g. at the VP level in the case of a pre-verbal focus operator.",
        "To illustrate the workings of our approach, we now run through a simple example. Consider (1a). To determine the meaning of only likes MARY, the FSV of the VP must be known. Hence the following equation must be solved: λx.l(x, m) = Gd(m) By HOU, the value of Gd is then 2 : Gd = λyλx.l(x, y) And by definition (3.1), the FSV is:",
        "Assuming the semantic of only given above, the semantic representation of (1a) is then:",
        "In short, we obtain a reading similar to that of Rooth, the difference being in the way the FSV is determined: by HOU in our approach, by means of a semantic definition in Rooth's.",
        "Under the HOU approach, (3b) is analysed as follows. First, the meaning of only 1 read the letters that SUE 2 sent to PAUL 1 is derived. To determine the FSV of the VP, the ground equation (4b) must be solved for which (4c) is a solution. Applying the semantics of only given in section 2, the semantics of (4a) is then as given in (4d) 4  .",
        "(4) a. only 1 read the letters that SUE 2 sent to PAUL 1 b. G 1 (p) = λx.read(x, l(s, p)) c. G 1 = λy.λx.read(x, l(s, y)) d. λz.∀P [P ∈ λyλx.read(x, l(s, y)) ∧ P (z) → P = λx.read(x, l(s, p))] Analysis then proceeds further and the ground equation Comparison with Rooth and Krifka As mentioned in section 4.1, under the Alternative Semantics approach, a focus operator necessarily associates with any focus occurring in its scope. Furthermore in (3b), the scope of only 1 is the whole VP read the letters that SUE 2 sent to PAUL 1 . Hence, if no quantifier raising occurs, only 1 associates with both SUE 2 and PAUL 1 . Thus in order to generate the desired reading, SUE 2 must be moved out of the scope of only 1 . However, since the NP the letters that SUE 2 sent to PAUL 1 is a scope island, quantifier raising is impossible. Hence, the desired reading cannot be generated  5  .",
        "Recall that in the Structured Meanings approach, the right-sibling of a focus operator must contain all and only the focus this operator associates with (cf. section 4.1). Hence, to generate the desired reading in (3b), there must exist a syntactic constituent which is right-adjacent to only 1 and which contains PAUL 1 but not SUE 2 6  ; similarly, there must exist a syntactic constituent which is right-adjacent to also and which contains SUE 2 but not PAUL 1 . Given standard assumptions about syntax, such constituents do not exist so that the desired interpretation cannot be generated.",
        "Our proposal is to analyse SOEs as involving a deaccented anaphor which consists of the repeated material, and is subject to the condition that its semantic representation must unify with the semantic representation of its antecedent. This is modeled as follows.",
        "Let SSem and T Sem be the semantic representation of the source (i.e. antecedent) and target (i.e. anaphoric) clause respectively, and T P 1 . . . T P n , SP 1 . . . SP n be the target and source parallel elements 7  , then the interpretation of an SOE must respect the following equations:",
        "Intuitively, these two equations require that target and source clause share a common semantics An, the semantics of the deaccented anaphor.",
        "Given this proposal, the analysis of (5a) involves three equations:",
        "Since neither Gd nor F ocus are initially given, the third equation above is untyped and cannot be solved by Huet's algorithm 8  . In that situation, we can either assume some delaying mechanism or some extension of Huet's algorithm that can cope with type variables (cf.  (Dougherty, 1993; Hustadt, 1991) ). Resolution of the first equation yields the following solution: An = λy∀P [P ∈ {λx.l(x, y) | y ∈ wff e } ∧ P (z) → P = λx.l(x, m)] By applying An to p, the left-hand side of the second equation is then determined so that the second equation becomes",
        "and the value of Gd is identified as being Gd = λyλx.l(x, y) (Note further, that the third equation can now be solved thus yielding the value m for the focus F .) That is, the HOU approach to SOEs allows us to correctly capture that fact that an SOE can inherit its FSV from its source clause (by unification). In  (Gardent et al., 1996) , we show in more detail how the analysis accounts for the interaction of focus with anaphora and definiteness in the case of a particular instantiation of SOEs, namely corrections."
      ],
      "Conclusion": [
        "In this paper, we have argued that Higher-Order Unification provides an adequate tool for computing Focus Semantic Values. To this end, we have considered data which is viewed as a testbed for focus theory and shown that, whilst existing theories either under-generate, over-generate or are methodologically unsatisfactory, the HOU approach yields a simple and transparent analysis. There appear to be two main reasons for this.",
        "First, the HOU analysis makes minimal assumptions about the role syntax is called to play in determining the FSV. It is defined on a purely semantic level in the sense that unification operates on semantic representations, and relies neither on quantifier raising, nor on a rule-to-rule definition of the FSV. As we have seen, this type of approach is a plausible way to avoid undergeneration.",
        "Second, the HOU approach permits an equational analysis which can naturally be further constrained by additional equations. The interest of such an approach was illustrated in our treatment of SOEs which we characterise as involving two phenomena: the computation of an FSV, and the resolution of a deaccented anaphor. Not only did we show that this analysis is methodologically and empirically sound, we also showed that it finds a natural realisation in the equational framework of HOU: each linguistic phenomena is characterised by some equation(s) and the equations may mutually constrain each other. For instance, in the case of SOEs, we saw that the equations characterising the deaccented anaphor help determine the unidentified FSV of the utterance containing the unmarked focus.",
        "Clearly, our approach extends to cases of adverbial quantification. For lack of space we could not develop the theory here; let us just point out that von Fintel's criticism  (von Fintel, 1995)  of semantic approaches to focus, also applies to Krifka's Structured Meanings analysis, but not to the HOU approach presented here. Von Fintel points out that in certain cases of adverbial quantification, a focus operator associates with an unmarked focus and does not associate with a marked focus occurring in its scope -as should be clear from this article, this is unproblematic for our analysis.",
        "Of course, there are still many open issues. First, how does the proposed analysis interact with quantification? Second, how does it extend to a dynamic semantics (e.g. Discourse Representation Theory)?"
      ]
    }
  },
  {
    "paperId": "f87abe4dc1257f86fd919d13b72ec0b98d64d7ed",
    "title": "Distributed Systems",
    "sections": {}
  },
  {
    "paperId": "b87a87e4394dd3fbd0e5167d06afdc9ce00ca42a",
    "title": "A bibliometric analysis of publications in computer networking research",
    "sections": {
      "Introduction": [
        "Bibliometric analysis of a literature is a crucially important source of objective knowledge and information about the quantity and quality of scientific work  (Narin et al. 1994) . In this work we perform a bibliometric analysis of the the literature of the field of computer networking, which is a major research domain in electrical and computer engineering and science. This breadth-wise knowledge saves ample amount of time for researchers to get started with the research of a domain and helps inform about the major trends observed in computer networking publications.",
        "There are several article genres in computer networking, such as conference articles, letters, editorials, surveys, and empirical studies. To keep the scope of this study to manageable proportions, we have focused on journal publications principally (survey and empirical studies) but have also compared journal publications to conference publications in this area. We have selected four exemplar venues that represent the highest standard of research in the field of computer networking-namely, IEEE Communications Surveys and Tutorials (COMST), IEEE/ACM Transactions on Networking (TON), ACM Special Interest Group on Data Communications (SIGCOMM), and IEEE International Conference on Computer Communications  (INFOCOM) . COMST and TON are among the top ranked journals in the field of computer networking while SIGCOMM and INFOCOM represent the top ranked conferences of the field.",
        "Towards this end, we statistically analyze 18 years of accepted articles published in the two journals (IEEE TON and COMST), explore various bibliometric questions, and examine the publication behaviors of several research entities and how these are affected by the elements of articles. We also analyze popular topics in periodicals on computer networking and the effects of several parameters on the citations of an article. We also compare and contrast the publication standards and practices of the two journals (TON and COMST) and the two conferences (INFOCOM and SIGCOMM) that we consider. We believe that a deep study of the articles published in these venues can not only provide insight into current publication practice, but can also inform about the temporal evolution of the publishing trends in these venues.",
        "We structure our work around three major comparisons. First, we directly compare publication trends in TON vs. COMST, to understand how these two distinct publication types differ. Second, we compare trends over time to understand how they have evolved. Thirdly, we compare trends from TON and COMST with INFOCOM and SIGCOMM to map out the differences between the trends of top conferences and journals.",
        "Our aim is to investigate changes in publication behavior and collaboration patterns of distinctive authors, institutes and countries in the various computer networking publications, and the distribution of various mathematical and graphical elements (figures, tables, and equations) within them. Our goal is therefore to provide generalized insights into the publication trends in the field of networking. We also aim to answers questions such as the following: Which topics are popular in which regions of the world? What are the topics discussed by the top authors in their articles in the various publications? Which parameters affect the citations of an article?",
        "The key contribution of this article is to develop a methodology and framework for performing a comprehensive bibliometric analysis on computer networking research and the public release of a comprehensive dataset. To the best of our knowledge, no such comprehensive study has been undertaken to study the publication trends in the field of computer networking. To facilitate future research in this area, we have publicly released our dataset including metadata, content, and citation related data for the articles published in IEEE COMST, TON, ACM SIGCOMM, and IEEE INFOCOM from 2000 to 2017 1  .",
        "The rest of this article is structured as follows. In section 2, we discuss related previous research work. The bulk of our investigations focus on the publication trends in computer networking journal publications in COMST and TON (Sections 3-6), but to make our analysis complete we also compare these trends with those observed in top ranked conferences  (INFOCOM and SIGCOMM)  in the area (Section 7). In Section 3, our dataset is described and our methodology is broadly outlined. A detailed bibliographic focused on comparison of TON and COMST is presented in Sections 4, 5, 6 in which metadata analyses, content-based analyses, citation-based analyses are presented respectively. A detailed comparison of publication trends in top networking journals and conferences (TON/COMST vs. INFOCOM/SIGCOMM) is presented in Section 7. We discuss future directions of this study in section 8. The paper is finally concluded in Section 9."
      ],
      "Related Work": [
        "In this section, we present related work and highlight the novelty of this article. Bibliometrics is an established field in which the major trends of research fields are studied rigorously. A number of bibliometrics studies have been conducted in various fields to gain useful insights through the analysis of authorship and publication trends of different research outlets and areas  (Nobre and Tavares 2017; Fernandes and Monteiro 2017; Serenko et al. 2009; Chiu and Fu 2010; Rajendran et al. 2011; Nattar 2009; Yin and Zhi 2017) . These bibliometric analyses are not confined to the authorship based meta-data analysis of venues. Some authors have also undertaken quantitative analysis on the top ACM conferences. The purpose of these studies is to determine the genre of the article and to understand the publication culture of these conferences  (Flittner et al. 2018) . These related studies do not explain which factors of the article affect the productivity parameters and the information about the correlation between important parameters required to analyze the productivity of different entities. Many previous works have performed an analysis on the content of various research areas using topic modeling  (Paul and Girju 2009)  and keyword-based analysis  (Choi et al. 2011) .",
        "A number of studies have used social networking analysis for social sciences and medical science research to find the most significant collaborating entities  (Savić et al. 2017; Wagner et al. 2017; Didegah and Thelwall 2018; Borgatti et al. 2009; Waheed et al. 2018) , using social network analysis on generally social media data and altmetric data  (Hassan et al. 2017b) . Social media analysis has not been used to determine the communities in computer networking research due to which we do not yet have complete insights into the collaborating patterns that exist in computer networking research.",
        "Limited work has focused on using bibliometric or scientometric techniques to analyze the publication mores of the field of computer networks.  Chiu et al. (Chiu and Fu 2010)  have performned an analysis of author productivity in computer networking venues in 2010. Our work is different in that we perform a detailed bibliometric analysis on the computer networking literature including an analysis of the effects of various features of article (such as the graphical and mathematical elements and the numbers of references) on the article's productivity metrics as defined in the field of bibliometrics.",
        "Bibliometric analyses can also be utilized to see the extent of the incorporation of related research. Reference count in a article is the simplest way to observe the inclusion of related research and literature review. Different researchers analyzed referencing patterns in research articles to identify incorporation of the latest studies relating to a research article  (Heilig and Voß 2014)  and citation analysis of the productivity of various research entities  (Hamadicharef 2012; Bartneck and Hu 2009) . These studies do not explain how the references are affected by the type of article venue."
      ],
      "Methods": [
        "We start by describing our data collection methodology. There are several article genres in the field of computer networking, including conference articles, letters, editorials, survey articles, and empirical studies. To capture a broad swathe of these, we sample from 4 different well known publication outlets."
      ],
      "Discussion": [
        "We start our analysis by exploring the key metadata attributes associated with the publications. Specifically, we focus on metadata associated with publications authors and their respective institutes, before inspecting the structural elements of the articles (e.g., presence of figures). In this section we focus on comparing these observations across the two journals under study.",
        "First, we investigate the most important authors of the two journals. There are many parameters to analyze the significance of a researcher's published work. A simple measure would be publication count is listed in Figure  1 . The h-index is also another widely used metric where h tells us that h articles of a researcher have h citations  (Hirsch 2005) . Using the h-index of only COMST and TON, we can observe which authors are publishing highly cited research in COMST and TON.",
        "Figure  2  shows the authors in COMST and TON with the highest h-index, and how the top five highest publication counts are from the top ten authors Fig.  1  Most-published authors during 2000-2017, according to article count. Interestingly, there is no overlap at all in the top 10 list, supporting a \"horses for courses\" hypothesis implying that it's rare to find an author who is extremely prolific in both these genres. with the highest h-index in COMST and TON. The data confirms that the top authors (measured by publication count) are the ones who have significant research contributions in terms of publication count as well as citation count.",
        "In a research domain, some countries play a pivotal role in driving the ongoing advancements in that field. Figure  3  shows the distribution of published articles in COMST and TON from different countries using a global heat map. As expected, the United States is in the highest position in COMST and TON in terms of publication count. Other top countries include Canada, China, France, and the United Kingdom in COMST. In TON, top countries remained the same but Italy replaced the United Kingdom in the list of top countries.  The differences in a country's publications in the two journals can partly be attributed to different publication cultures arising from different incentives for faculty promotion/assessment. Some countries in North America and parts of Europe (e.g., USA and UK) give more weight to top-tier conferences (like Sigcomm, NSDI, Infocomm, etc.) in their assessment criteria while many others in parts of Asia and southern Europe (e.g., Pakistan, Malaysia, France, Spain, Italy) emphasize journal publications. In many cases, extended versions of conference papers in the networking domain are published in journals such as TON. COMST, due to its focus on tutorial/survey papers, is more specialized and therefore not relevant for conference paper extensions. Figure  4  shows the rank of different countries in COMST and TON based on published articles using a global heat map. Rank of some countries has significantly changed in both journals. Israel was on Rank 7 out of 33 in TON as compared to 27 out of 28 in COMST. Similarly, Pakistan was on Rank 16 out of 28 in COMST as compared to 33 out of 33 in TON. Many countries have not published a single paper in TON but published many papers in COMST. These countries include Ghana, South Africa, Iceland and many more. We next inspect the collaborations that took place between these countries. Figure  5  shows the co-authorship network of top countries in COMST and TON. In COMST, the top three countries have significant co-authorship activities among themselves, thus they are clustered in a single group. The same pattern is followed by the fourth and fifth most influential countries, which are clustered in one group. In TON, all the major contributing countries are clustered in a single node due to the great publication contribution of the United States. The United States contributed 1,667 of the 2,439 articles in TON. With the advancement of information and communication technologies, researchers from various countries now have new ways to work with each other. Top countries enjoy the share of publication from their authors, and in addition a contribution from authors from collaborating countries. We next proceed to inspect the productivity rates among countries, specifically in terms of publication and citation count. By using these features, we propose a simple mathematical model for determining the rank of a country in a venue. We kept the highest measurement in each feature as a reference point for the calculation of the ranking score. Normalized Rank Score (NRS) for each country can be calculated by using equation 1 where  We calculated ranking scores of top countries in COMST and TON using equation 1. Figure  6  shows the ranking of different countries in COMST and TON where it is seen that the USA has the maximum ranking score in both the venues. Both the venues are dominated by more or less the same countries with some exceptions-e.g., Israel is among the top-ranked countries publishing in TON but it is not a prominent contributor to COMST. This indicates that different countries can (for various socioeconomic reasons) have incentives to target particular journals. Table  4  shows the impact of the top countries in COMST and TON. For both publication venues, the United States is the highest-ranked contributor with the average citation count per document being higher in TON than in COMST.",
        "The structural elements of an article consist of the mathematical and graphical parts and the references cited. The mathematical and graphical elements help authors to convey the results related to an article, to discuss problems more precisely and concisely, and the references help readers to find research relating to the article. This sub-section addresses many important bibliometric questions on the structural elements of a research article. These include the distribution of the references in different genres of articles; the relationship between higher numbers of references and the author count of an article; the relationship between the number of references and the number of mathematical and graphical elements; and what kind of graphical and mathematical elements are found more in survey articles than experimental studies, and vice versa.",
        "Different kinds of articles have varying numbers of references. For instance, survey-based articles have a high number by their nature that requires coverage of a broad area. Figure  12  shows that articles from a particular number of authors have higher numbers of median references in COMST than in TON. Figure  12  shows the results for COMST and TON data, where the number of references in COMST and TON goes up with the increasing number of authors. Similar results are reported by Saeed et al.,  Valenzuela et al. and Zhu et al.  in their studies  (Hassan et al. 2017a; Valenzuela et al. 2015; Zhu et al. 2015) . Figure  12  also shows that with the increasing number of authors, number of references from the last ten years in a paper also increase in COMST and TON. The data also has some outliers in terms of the number of references and references from the last ten years in a paper. Therefore, we have used median references for analysis because mean is more susceptible to outliers than median  (Leys et al. 2013) . Different types of research articles have different types of structural elements. For example, a survey-based article might have a higher number of graphical elements than mathematical equations, because tutorials can explain topics best using figures and tables. Figure  13  presents a breakdown of the average numbers of artifacts per year. In both journals, tables are the least frequently used. COMST has a high number of figures each year, and TON has a high number of equations. This is not surprising, considering the contrasting nature of these two journals.",
        "We also note that the number of references in an article increases with the number of authors. Over time this trend is increasing, with the numbers of authors per article growing for both COMST and TON. Moreover, the number of references is higher in COMST articles than in TON articles. This is to be expected, as COMST focus on review and survey articles. Similar trends are send with graphical elements, where COMST exceeds TON. In contrast, TON has more mathematical elements which, again, is to be expected as TON tends to contain experiment-based publications.",
        "This section contains two types of analysis of COMST and TON: (A) keywordbased analysis, based on index keywords; and (B) readability-based analysis.",
        "Investigating the popular topics is considered to be one of the best ways of studying the paradigm shifts in any research field. It is helpful in describing the research trends of a field. In this sub-section, we use COMST and TON data to analyze the popular topics in the field of computer networking. We have described the top 10 popular topics discussed in survey-based and experimental studies-based articles in computer networks. This approach provides a holistic overview of research trends in computer networking since it covers both original and survey-based articles. Figure  14  represents the most popular topics in computer networking, according to the COMST and TON dataset. COMST contains survey articles and, from 2000 to 2017, it published surveys relating to wireless and mobile communication systems, QoS, and Internet. By contrast, during this period most of the articles published in TON discuss algorithmic and optimization problems relating to computer networking. Table  5  shows the change over time of popular topics in the field of computer networking, using the COMST and TON datasets. Popular topics mentioned in Table  5  give the approximate overall research trends in the field of computer networking. While there is a lot of stability in the keywords ('wireless networks' is common in COMST and 'optimization' and 'algorithms' is common in TON, we see over time new topics emerging such as 'complex networks' in the last three years of TON publications).",
        "Table  6  Using LDA-based topic modeling to determine 10 most popular topics in COMST and TON. We see different (more coherent) results using LDA-based topic modeling compared to the keywords-based results in Table  5 . One limitation of the analysis above is that it is based on stipulated keywords, which may exclude pertinent topics. Hence, we use Latent Dirichlet Allocation (LDA) to identify important themes within the article's body. LDA takes raw text, the number of topics and a dictionary of words as the input, and outputs the most significant topics with words from the raw data  (Blei et al. 2003) . We kept the number of latent output topics to 10 and iterated our algorithms 400 times on our dataset in order to achieve converged results. Table  6  shows the results of LDA on the COMST and TON datasets. It can be seen that the results are different from those of the results for keywords in Table  5  and refer to different topics such as smart grid, sensor networks, cognitive radios for COMST and optimization algorithms, congestion control solutions, approximation algorithms for TON.",
        "Keyword co-occurrence analysis helps researchers to find a publication venue's most common topics. These analyses also help researchers to find topics and domains that are strongly related to each other. Figure  15  is the term cooccurrence map for COMST and TON. There is limited overlap in the keywords used in the top-cited articles in TON and COMST. The keywords in COMST are biased towards problems and those in TON towards techniques/solutions.",
        "Terms in a larger font size have a higher co-occurrence than other keywords in the graphs. In COMST, frequently co-occurring terms are \"Wireless Telecommunication Systems\", \"Wireless Networks\", \"Quality of Service\", \"Energy Efficiency\", \"Mobile Telecommunication Systems\", and so on. In TON, the most frequently co-occurring terms are \"Optimization\", \"Algorithms\", \"Wireless Networks\", \"Scheduling\", and so on. Top keywords (measured on publication count) in both the venues are clustered in the same groups and have stronger links with each other than with unpopular keywords. This trend shows that in both venues, there are only some top keywords (measured on publication count) which are discussed in most of the articles. The results also show that in most of the articles in COMST and TON, top keywords co-occur with each other.",
        "We also observe several other trends that are noteworthy. For example, in COMST, authors mostly discuss network configurations (e.g. WSN) and problems (e.g. scheduling, energy efficiency), whereas in TON it is the techniques (such as optimization, algorithms) that are emphasized. We note that the Keyword co-occurrence-based analysis also helps researchers to establish the topics and domains that are strongly related to each other. Our findings are that the most popular keyword terms in COMST and TON relate to problems (quality of service, energy efficiency etc.) and techniques (optimization, algorithms etc.), respectively.",
        "Citations are used to investigate the contributions of an author, organization, country or publication venue. Citation analysis is an effective tool to rank the productivity of various research bodies. In this section, we address some important bibliometric questions using citation data from COMST and TON articles, such as who are the most-cited authors in COMST and TON; whether they have the same h-index as the most-published authors in COMST; whether increasing the number of authors affects the number of citations of an article; the most-cited keywords in COMST and TON; and whether a larger number of mathematical and graphical elements in an article increases its citation count.",
        "In computer networking, some authors play more significant roles in advancements of the field than others. It is worth observing the impact and usability of their research. Figure  16  shows the most-cited authors in COMST and TON Fig.  16  Most-cited authors. We see that the most-cited TON articles tend to have more citations even though COMST on average are cited more; cf. Table  I , which shows that COMST (TON) on average has 67 (37) citations.",
        "from 2000 to 2017. From Figure  16  and Figure  1 , it can be observed that the top most-published authors and the top most-cited authors in COMST and TON are entirely different. Citations do not entirely represent the significance of the research undertaken by a researcher. There are many parameters to analyze its significance, but the h-index is the most widely used, and it is a better measure of an author's significance in a field than a simple citation count.",
        "Figure  2  shows the authors in COMST and TON with the highest h-index, and how the top ten highest publication counts are from the top ten authors with the highest h-index in COMST and TON. The data confirms that the top authors (measured by publication count) are the ones who have significant research contributions in terms of publication count as well as citation count.",
        "Figure  17  shows the impact of the top countries in COMST and TON. For both publication venues, the United States is the most prominent contributor. Figure  18  presents the citation counts for each journal based on how many authors are on the article. We see that TON articles tend to have higher citation counts than survey-based articles when we consider the top-cited articles but on average COMST articles are cited more (see Table  I , in which it is shown than COMST have on average 67 citations compared to 37 for TON). The higher citations of COMST articles on average likely stems from their citations in many topic-specific articles as a general resource.  The feature ranking of parameters can be performed by various methods such as PCA, SVD, and Random Forest. To measure the impact of these parameters on the citation count in our dataset, we used the Extremely Randomized Trees classifier, which is a variant of Random Forest. It computes the importance of a feature using Gini or average decay in impurity, which gives the impact of a feature on the label of a dataset. A higher value from the ExtraTree Classifier for a feature indicates greater importance for that feature with respect to the dependent variable (class label)  (Geurts et al. 2006 ). Table  7  shows the impact of each feature on a dependent variable (class label). Results from Table  7  show that citations of the papers are more dependent on structural elements of paper as compared to the author based elements of the paper.  TON is one of the most reputed journals in computer networking and many authors extend their work, published in different conferences, to publish in TON. Figure  20  shows the number of articles published in TON whose prequel work is published in either INFOCOM and SIGCOMM. We found out that 269 out of 2410 ( 10%) articles of TON have their prequel work published in INFOCOM. Similarly, 69 out of 2410 articles of TON are the sequel of the work published in SIGCOMM. There is no overlap between SIGCOMM and INFOCOM. Similarly, COMST has no intersection with any of the other venues.",
        "We have explored the changing trends in co-authorship in SIGCOMM and INFOCOM over the period 2000 to 2017 and compared them with discussed journals. We explore how the distribution of collaborating authors changes over time. Figure  21  shows the distribution of the number of authors per article in COMST per year. It is clear that the tendency for co-authorship is increasing; in 2000 the median number of authors is 2 for COMST and 3 for TON, compared to 4 and 4 in 2017. Perhaps most noteworthy is the spread of authorship numbers across articles, with a standard deviation of 0.87 in 2000 vs. 1.69 in 2017 for TON (similar trends of COMST). Similarly, in SIGCOMM and INFOCOM, the tendency for co-authorship is increasing by the passage of time; in 2000 the median number of authors is 3 for SIGCOMM and 3 for INFOCOM, compared to 4 and 4 in 2017. Again, one of the most worth observing trends is the spread of authorship across time duration with a standard deviation of 1.94 in 2000 vs. 2.52 in 2017 for SIGCOMM (standard deviation of 0.95 in 2000 vs. 1.65 in 2017 for INFOCOM). One more surprising fact is the comparison between the spread of authorship of journals and conferences. Top conferences in computer networking show the higher spread of authorship across the years as compared to journals.",
        "Each venue in every domain has a handful of common authors and this trend is also similar in computer science. Figure  22  shows the number common authors among all of the venues during 2000-2017. From results present in this figure, we can observe that SIGCOMM and TON have the highest percentage of common authors among all of the venues.  In a research domain, some countries play a pivotal role in driving the ongoing advancements in that field. Figure  23     interesting results. Surprisingly, in journals (COMST and TON), the top 10 most-published list and the top 10 authors with the highest h-index are almost identical but in conferences (SIGCOMM and INFOCOM), this trend is not true as the most-published authors and authors with the highest h-index are not same. For top conferences, this data shows that the authors with top publication count are not the ones with a balanced contribution of publication count and citation count.",
        "Figure  27  presents the citation counts for each journal and conference based on how many authors are on the article. We see that TON articles tend to have higher citation counts than survey-based articles when we consider the top-cited articles but on average COMST articles are cited more (see Table  I , in which it is shown that COMST has on average 67 citations compared to 37 for TON). The higher citations of COMST articles on average likely stems from their citations in many topic-specific articles as a general resource.",
        "Similarly, INFOCOM articles tend to have higher citation count across the time duration.",
        "Investigating popular topics is considered to be one of the best ways of studying the paradigm shifts in any research field. It is helpful in describing the research trends of a field. In this section, we investigate such paradigm shift in journals and conferences and analyze the overlapping between those two genres. To perform keyword-based analysis, we use Latent Dirichlet Allocation (LDA). LDA takes raw text, the number of topics and a dictionary of words as the input, and outputs the most significant topics with words from the raw data.",
        "We kept the number of latent output topics to 10 and iterated our algorithms 400 times on our dataset in order to achieve converged results. Furthermore, we categorized the top latent topics extracted from all datasets into 11 main categories. Top topics in all of these four venues are discussed mainly from these categories. Figure  28  shows the overlap between these categories in all of the four venues. Table  8  shows the results of LDA on the COMST, TON, SIGCOMM and INFOCOM datasets. Table  8  Using LDA-based topic modeling to determine 10 most popular topics in all four venues. We see coherent results using LDA-based topic modeling as it reveals the topics hidden in actual text.",
        "8 Future Directions",
        "Our study provides a methodology and framework for performing a comprehensive bibliometric analysis on computer networking research and the public release of a comprehensive dataset. Future research of this study can be extended in several directions, some of which we highlight below:",
        "-This work can be followed up with a more comprehensive analysis on a larger set of related journals and conferences in the field of computer networking; -Future researchers can also explore using data from, and integrating with, popular conference management systems (EDAS, HotCRP, EasyChair, etc.) -This study can be extended by work that finds correlation of publications in computer networking literature with the priorities defined by major global research funding agencies; -A comparison of computer networking with other fields (e.g. machine learning, artificial intelligence, network science) can be performed and differences in publication trends (such as citations, h-index) can be identified."
      ],
      "Conclusion": [
        "In this paper, we have performed an in-depth bibliometric study of the publication trends in computer networking literature using article content and metadata of four important computer networking periodicals-IEEE Communications Surveys and Tutorials (COMST), IEEE/ACM Transactions on Networking (TON), ACM Special Interest Group on Data Communications (SIGCOMM), and IEEE International Conference on Computer Communications (INFOCOM)-gathered over the time period 2000-2017. Our work extends the state of the art in bibliometric analysis of computer networking literature by presented comprehensive analyses that shed light on the publication patterns in these journals including which kinds of articles are published where; how are journal and conference publications different in this area; and which different authors, institutes, and countries have been successful in these venues (and how). Although we cannot make strong claims about causality or the parameters responsible for the acceptance/rejection of an article since we did not have access to missing data (rejected articles), we believe that our analyses provide an insightful look into the publication culture in the networking community and can help develop a more nuanced understanding of this research field especially in the light of the limited existing bibliometric work that focused on the computer networking community. In this regard, we have also publicly shared our dataset that includes content, metadata, and citation-related information related to the articles published from 2000 to 2017 in COMST, TON, SIGCOMM, and INFOCOM as our contribution to the research community. 13"
      ]
    }
  },
  {
    "paperId": "0fe2636446cd686830da3d971b31a004d6094b3c",
    "title": "CodeBERT: A Pre-Trained Model for Programming and Natural Languages",
    "sections": {
      "Introduction": [
        "Large pre-trained models such as ELMo  (Peters et al., 2018) , GPT  (Radford et al., 2018) , BERT  (Devlin et al., 2018) , XLNet  (Yang et al., 2019)  and RoBERTa  (Liu et al., 2019)  have dramatically improved the state-of-the-art on a variety of natural language processing (NLP) tasks. These pre-trained models learn effective contextual representations from massive unlabeled text optimized by self-supervised objectives, such as masked language modeling, which predicts the original masked word from an artificially masked input sequence. The success of pre-trained models in NLP also drives a surge of multi-modal pre-trained models, such as ViLBERT  (Lu et al., 2019)  for language-image and VideoBERT  (Sun et al., 2019)  for language-video, which are learned from bimodal data such as language-image pairs with bimodal self-supervised objectives.",
        "In this work, we present CodeBERT, a bimodal pre-trained model for natural language (NL) and programming language (PL) like Python, Java, JavaScript, etc. CodeBERT captures the semantic connection between natural language and programming language, and produces general-purpose representations that can broadly support NL-PL understanding tasks (e.g. natural language code search) and generation tasks (e.g. code documentation generation). It is developed with the multilayer Transformer  (Vaswani et al., 2017) , which is adopted in a majority of large pre-trained models. In order to make use of both bimodal instances of NL-PL pairs and large amount of available unimodal codes, we train CodeBERT with a hybrid objective function, including standard masked language modeling  (Devlin et al., 2018)  and replaced token detection  (Clark et al., 2020) , where unimodal codes help to learn better generators for producing better alternative tokens for the latter objective.",
        "We train CodeBERT from Github code reposito-ries in 6 programming languages, where bimodal datapoints are codes that pair with function-level natural language documentations  (Husain et al., 2019) . Training is conducted in a setting similar to that of multilingual BERT  (Pires et al., 2019) , in which case one pre-trained model is learned for 6 programming languages with no explicit markers used to denote the input programming language. We evaluate CodeBERT on two downstream NL-PL tasks, including natural language code search and code documentation generation.",
        "Results show that fine-tuning the parameters of CodeBERT achieves state-of-the-art performance on both tasks. To further investigate what type of knowledge is learned in CodeBERT, we construct a dataset for NL-PL probing, and test CodeBERT in a zero-shot scenario, i.e. without fine-tuning the parameters of CodeBERT. We find that CodeBERT consistently outperforms RoBERTa, a purely natural language-based pre-trained model. The contributions of this work are as follows:",
        "• CodeBERT is the first large NL-PL pretrained model for multiple programming languages.",
        "• Empirical results show that CodeBERT is effective in both code search and code-to-text generation tasks.",
        "• We further created a dataset which is the first one to investigate the probing ability of the code-based pre-trained models.",
        "2.1 Pre-Trained Models in NLP Large pre-trained models  (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2018; Yang et al., 2019; Liu et al., 2019; Raffel et al., 2019)  have brought dramatic empirical improvements on almost every NLP task in the past few years. Successful approaches train deep neural networks on large-scale plain texts with self-supervised learning objectives. One of the most representative neural architectures is the Transformer  (Vaswani et al., 2017) , which is also the one used in this work. It contains multiple self-attention layers, and can be conventionally learned with gradient decent in an end-to-end manner as every component is differentiable. The terminology \"self-supervised\" means that supervisions used for pre-training are automatically collected from raw data without manual annotation. Dominant learning objectives are language modeling and its variations. For example, in GPT  (Radford et al., 2018) , the learning objective is language modeling, namely predicting the next word w k given the preceding context words {w 1 , w 2 , ..., w k-1 }. As the ultimate goal of pretraining is not to train a good language model, it is desirable to consider both preceding and following contexts to learn better general-purpose contextual representations. This leads us to the masked language modeling objective used in BERT  (Devlin et al., 2018) , which learns to predict the masked words of a randomly masked word sequence given surrounding contexts. Masked language modeling is also used as one of the two learning objectives for training CodeBERT."
      ],
      "Conclusion": [
        "In this paper, we present CodeBERT, which to the best of our knowledge is the first large bimodal pre-trained model for natural language and programming language. We train CodeBERT on both bimodal and unimodal data, and show that finetuning CodeBERT achieves state-of-the-art performance on downstream tasks including natural language code search and code-to-documentation generation. To further investigate the knowledge embodied in pre-trained models, we formulate the task of NL-PL probing and create a dataset for probing. We regard the probing task as a cloze-style answer selection problem, and curate distractors for both NL and PL parts. Results show that, with model parameters fixed, CodeBERT performs better than RoBERTa and a continuously trained model using codes only.",
        "There are many potential directions for further research on this field. First, one could learn better generators with bimodal evidence or more complicated neural architecture to improve the replaced token detection objective. Second, the loss functions of CodeBERT mainly target on NL-PL understanding tasks. Although CodeBERT achieves strong BLEU scores on code-to-documentation generation, the CodeBERT itself could be further improved by generation-related learning objectives.",
        "How to successfully incorporate AST into the pretraining step is also an attractive direction. Third, we plan to apply CodeBERT to more NL-PL related tasks, and extend it to more programming languages. Flexible and powerful domain/language adaptation methods are necessary to generalize well."
      ]
    }
  },
  {
    "paperId": "0dac0e73dc0d6f0ebbbd45ea2e3bc60d437200e1",
    "title": "CIRCLE: continual repair across programming languages",
    "sections": {
      "Introduction": [
        "DL-based APRs have achieved state-of-the-art performance on program repair task  [9, 11, 45, 76, 88] . Most of them treat repairing as a neural machine translation task and optimize an encoderdecoder model on a set of bug-fix pairs to learn latent patterns based on supervised learning. The inputs and neural model architectures of DL-based APRs are various. For example, CoCoNut  [45]  separately encodes context and buggy codes by CNN networks. SequenceR  [11]  abstracts the buggy context and takes it as input together with buggy lines. Recently, pre-trained models are also used in DL-based APRs. CURE  [25]  employs GPT as token embedding layer. Mashhadi et al.  [49]  utilize CodeBERT to fix Java simple bugs.",
        "However, to the best of our knowledge, repairing multiple programming languages' defects via a single model is still underexplored. In light of this, we propose to employ the recent pre-trained model T5 as the skeleton and build a repair model that can fix bugs across languages."
      ],
      "Methods": [
        "In this section, we introduce the experimental design, including the research questions we studied, the training datasets, evaluation benchmarks, and implementation details in the experiments.",
        "To evaluate the performance of CIRCLE, we compare it with stateof-the-art techniques, including the traditional and DL-based ones. We adopt the final trained CIRCLE in RQ1 to perform repair tasks for five bug benchmarks across four programming languages. Table  2  shows the repair performance of a single CIRCLE model and the all selected baselines. In Table  2 , each cell is represented as 𝑥/𝑦, where 𝑥 is the number of correct patches and 𝑦 is the number of produced plausible patches. The results show that a single CIRCLE model achieves state-of-the-art performance in different languages.",
        "As shown in Table  2 , CIRCLE fixes 120 bugs for all bug benchmarks in four programming languages. For Java benchmark, CIR-CLE fixes 19 bugs on QuickBugs, outperforming CoCoNut and is"
      ],
      "Results": [
        "In this section, we evaluate CIRCLE and answer to four research questions based on experimental results."
      ],
      "Related Work": [
        "Over the past decade, researchers have proposed a variety of techniques to generate patches based on different hypotheses  [17, 56] . Following recent work  [6, 39, 92] , we categorize them into four main categories: heuristic-based  [32, 47, 90] , constraint-based  [14, 52, 86] , template-based  [30, 37, 38]  and DL-based repair techniques  [34, 45, 93] .",
        "Recently, DL-based repair techniques, which attempt to fix bugs enhanced by machine learning techniques, is getting growing atten-tionRecently, due to the large available open-source code. Tufano et al.  [77]  extensively evaluate the ability of adopting neural machine translation techniques to generate patches from bug-fixes commits in the wild. Li et al.  [34]  adopt a tree-based RNN encoder-decoder model (i.e., DLFix) to learn code contexts and transformations from previous bug fixes.  Lutellier et al. [45]  propose a new context-aware NMT architecture (i.e., CoCoNut) that represents the buggy source code and its surrounding context separately, to automatically fix bugs in multiple programming languages. Jiang et al.  [25]  propose a novel approach (i.e., CURE) combines a program language model, a code-aware search strategy, and a subword tokenization technique. The results demonstrate CURE can outperform all existing techniques on two popular benchmark (i.e., Defects4J and QuixBugs) when published. Recently, Zhu et al.  [93]  use a syntax-guided edit decoder (i.e., Recoder) with provider/decider architecture to ensure accurate patch generation. Compared to existing work, CIRCLE is the first work that aims to address the the generalizability issue of APR by repairing multiple languages in a lifelong learning scenario."
      ],
      "Conclusion": [
        "In this paper, we propose CIRCLE, an automatic program repairing framework that can continually learn to fix bugs crossing various programming languages. Specifically, CIRCLE consists of five components: a prompt-based representation, a T5-based model, a difficulty-based example replay, an EWC-based regularization, and a re-repairing mechanism. The T5-based model is the skeleton of APR model. The prompt-based representation converts program repairing to fill-in-the-blank task, filling the gap between T5's pretrained task and APR task. The difficulty-based replay and EWCbased regularization are two lifelong strategies, enabling CIRCLE to continually update its parameters according to the incremental task requirements. Finally, a simple yet effective re-repairing method is applied to eliminate the form error caused by multiple languages repairing. To the best of our knowledge, it is the first time to construct an APR model simultaneously addressing multiple programming languages based on continual learning approaches. We conduct extensive experiments with 4 programming languages on 5 benchmarks to demonstrate the effectiveness of our CIRCLE. Experimental results show that our CIRCLE (1) can continually learn bug fixing crossing languages; (2) achieves state-of-the-art performance on all benchmarks using a single model."
      ]
    }
  },
  {
    "paperId": "ae17c900f9cd235f71d8e6dc13b52142f4a54fd5",
    "title": "Ranking programming languages by energy efficiency",
    "sections": {
      "Introduction": [
        "Software language engineering provides powerful techniques and tools to design, implement and evolve software languages. Such techniques aim at improving programmers productivity -by incorporating advanced features in the language design, like for instance powerful modular and type systems -and at efficiently executing such software -by developing, for example, aggressive compiler optimizations. Indeed, most techniques were developed with the main goal of helping software developers in producing faster programs.",
        "More recently, this reality is quickly changing and software energy consumption is becoming a key concern for computer manufacturers, software language engineers, programmers, and even regular computer users. Nowadays, even mobile phone (which are powerful computers) users tend to avoid using CPU intensive applications just to save battery/energy. While the concern on the computers' energy efficiency started by the hardware manufacturers, it quickly became a concern for software developers too  [1] . In fact, this is a recent and intensive area of research where several techniques to analyze and optimize the energy consumption of software systems are being developed. Such techniques already provide knowledge on the energy efficiency of data structures  [2] [3] [4] , the energy impact of different programming practices both in mobile  [5] [6] [7]  and desktop applications  [8, 9] , the energy efficiency of applications within the same scope  [10, 11] , or even on how to predict energy consumption in several software systems  [12, 13] , among several other works.",
        "An interesting question that frequently arises in the software energy efficiency area is whether a faster program is also an energy efficient program, or not. In other words, does a faster program consume less energy due to performing the task quicker?. If the answer is yes, then optimizing a program for speed also means optimizing it for energy, and this is exactly what the compiler construction community has been doing since the very beginning of software languages. However, energy consumption does not depends only on execution time, as shown in the equation E nerg y = T ime × P ower . In fact, there are several research works showing different results regarding this subject  [14] [15] [16] [17] 2, 18] .",
        "A similar question arises when comparing software languages: is a faster language, a more energy efficient one? Comparing software languages, however, is an extremely complex task, since the performance of a language is influenced by the quality of its compiler, virtual machine, garbage collector, available libraries, etc. Indeed, a software program may become faster by improving its source code, but also by \"just\" optimizing its libraries and/or its compiler.",
        "Such questions arise by both researchers and programmers as there is still a large difficulty of how to analyze, interpret, and optimize the energy consumption in software. In fact, studies  [19, 1]  have shown that programmers are very concerned with the energy consumption of their software, and many times seek help. There are many misconceptions within the programming community as to what causes high energy consumption and in what ways they can be solved  [20] . Recent works  [21, 22]  argue that there are two main roadblocks in regards to energy efficiency software development: the lack of tools and lack of knowledge. The research work presented in this paper aims at helping with the lack of knowledge for energy efficient software development by tackling one of the initial steps in software development and providing further useful information: what programming language should be chosen?",
        "In previous works  [23, 24] , we have made coherent and consistent efforts to assess and compare the performance of (a total of) 27 of the most widely used software languages. We considered (a total of) ten different programming problems that are expressed in each of the languages, following the exact same algorithm, as defined in the Computer Language Benchmark Game (CLBG)  [25] . We compiled/executed such programs using the state-of-the-art compilers, virtual machines, interpreters, and libraries for each language. Afterwards, we analyzed the performance of the different implementations considering runtime performance, i.e., execution time and peak memory consumption, and energy consumption. Moreover, we analyzed those results according to the languages' execution type (compiled, virtual machine and interpreted), and programming paradigm (imperative, functional, object oriented, scripting) used. For each of the execution types and programming paradigms, we compiled a software language ranking according to each objective individually considered (e.g., time or energy consumption). We have also proposed global rankings for all the possible combinations of objectives (e.g., time and energy consumption). While the study was performed on a server based system, there is clear evidence that such results can be mapped to embedded systems  [26] .",
        "Additionally, our previous work was openly welcomed by the community of researchers and industrial practitioners with excitement and interest in our findings, sparking countless amounts of discussions. This result is expected, as our previously mentioned studies have shown that the programming community is very much concerned with the energy consumption of their software and seek ways to improve it, and insights tackling the lack of knowledge in the field is very appreciated. In fact, this can be clearly seen by practitioners' responses and discussions through online news pages, 1  social-media, 2  external teaching material,  3  and Reddit 4 discussions. Such information has helped many rethink (and even publicly change) their programming language of choice as energy efficiency is a concern of theirs. For language developers, such information is also important in order to compare language performance (energy and time) against competitors. In fact, the competition in the computing world, from both software and hardware developers, drives and motivates further evolution. Language developers become excited to see that their language is competing to be a very efficient one, for example as shown through the Rust language newsletter.  5 This paper extends our previous work in two ways. First, we have considered an alternative dimension within our earlier work. Indeed, one of the objectives we considered was peak memory usage, which did not prove to be correlated with memory energy consumption. Now, we are presenting total memory usage, or the accumulative amount of memory used through the application's lifecycle, as another possibility for analyzing memory behavior. Finally, we also present statistical correlation tests between energy, runtime, and memory.",
        "Second, we present a second large study in order to provide a validation of our previous energy ranking that uses a more idiomatic and day-to-day code example base. Indeed, we consider a chrestomathy repository, Rosetta Code  [27] , of alternative solutions to programming problems that is maintained with the main goal of assisting programmers in understanding syntactic or semantic aspects of programming languages outside their domain of expertise. Thus, the solutions that are gathered have a clarity and pedagogical concern, which is essentially different when compared to CLBG, whose solutions are strictly performance-oriented. To validate, we considered 9 tasks from Rosetta Code, and their solutions in (up to) the 27 programming languages that we have previously considered. With this, we are also able to study the energy efficiency of program solutions from a performance-oriented source (CLBG) and an educational source (Rosetta), allowing us to analyze how performance vs. comprehensibility affects energy consumption.",
        "With the proposition of a secondary ranking serving as a validation, we are interested in finding efficiency trends that confirm or contradict our earlier findings with respect to the efficiency of programming languages, and the representativeness of our benchmarks. This is aligned with our perspective that the insights provided by one ranking, if considered in isolation, are more subject to imprecise systematization, and might indeed benefit from complementary perspectives provided by different rankings. We believe that this is actually an idea that generalizes to traditional rankings, e.g., when considering the prestigious of worldwide Universities, and the multiple rankings that attempt to analyze it. Comparable rankings between the study and validation can solidify the results we have presented, while at the same time allowing us to understand how normal and more representative day-to-day programming styles and tendencies (Rosetta Code) compare to those focused on pure performance (CLBG).",
        "As we have previously mentioned, the work presented in this paper extends previous work  [23, 24] . While this work has been previously introduced, we go back to fully describing the methodology and results as to provide readers the full-picture and complete comprehensive overview of our context. This provides a fully self contained look at our presented research work. Thus, in this paper we present and answer the following research questions:",
        "• RQ1: Can we compare the energy efficiency of software languages? This will allow us to have results in which we can in fact compare the energy efficiency of popular programming languages. In having these results, we can also explore the relations between energy consumption, execution time, and memory usage.",
        "• RQ2: Is the faster language always the most energy efficient? Properly understanding this will not only address if energy efficiency is purely a performance problem, but also allow developers to have a greater understanding of how energy and time relates in a language, and between languages.",
        "• RQ3: How does memory usage relate to energy consumption? Insight on how peak memory (highest amount of used memory in a given instance) and total memory (the accumulative amount of memory used) affects energy consumption will allow developers to better understand how to manage memory if their concern is energy consumption.",
        "• RQ4: Can we automatically decide what is the best programming language considering energy, time, and memory usage? Often times developers are concerned with more than one (possibly limited) resource. For example, both energy and time, time and memory space, energy and memory space or all three. Analyzing these trade-offs will allow developers to know which programming languages are best in specific scenarios.",
        "• RQ5: How do the results of our energy consumption analysis of programming languages gathered from rigorous performance benchmarking solutions compare to results of average day-to-day solutions? As the results and ranking we gathered are based off a competitive benchmarking structure for the performance of programming languages, the solutions may be very specific to the problem at hand and stray away from a typical and representative idiomatic style of programming for an everyday user. It is important to understand how such results compare to the day-to-day programming styles, which follow more flexible solutions and are written by more everyday users. This will allow us to conclude if our results are representative and generalizable to a degree, and understand in what cases and why differences occur.",
        "The remainder of this paper is organized as follows: Section 2 details previous work which was used as the basis of this paper, which includes the steps of our rigorous and strict methodology to measure and compare the energy efficiency in software languages; this section also includes the description of our data set from CLBG and the study's results. Section 3 presents a discussion and ranking on the energy efficiency of each programming language based on the results. We describe in Section 4, how we structured a new validating study based on our previous methodology in order to produce a secondary ranking using the Rosetta Code chrestomathy repository of representative programs in order to validate our prior one and understand if our results are generalizable. In Section 5 we discuss the threats that may affect the validity of the insights we are drawing. Section 6 presents the related work, and finally, in Section 7 we present the conclusions of our work."
      ],
      "Results": [
        "The results from our study are partially shown in this section, with all the remaining benchmark results (including means, standard deviation, and boxplot data) shown in the online appendix for this paper 7 . Table  3 , and the left most tables under Results -A. Data Tables in the appendix, contains the measured data from different benchmark solutions. We only show the results for binary-trees, fannkuch-redux, and fasta within the paper, which are the first 3 ordered alphabetically. Each row in a table represents one of the 27 programming languages which were measured.",
        "The 4 rightmost columns, from left to right, represent the average values for the Energy consumed (Joules), Time of execution (milliseconds), Ratio between Energy and Time, and the amount of peak memory usage in Mb. The Energy value is the sum of CPU and DRAM energy consumption. Additionally, the Ratio can also be seen as the average Power, expressed in Kilowatts (kW). The rows are ordered according to the programming language's energy consumption, from lowest to highest. Finally, the right most tables under Results -A. Data Tables contain the standard deviation and average values for our measured CPU, DRAM, and Time, allowing us to understand the variance.",
        "The first column states the name of the programming languages, preceded by either a (c), (i), or (v) classifying them as either a compiled, interpreted, or virtual-machine language, respectively. In some cases, the programming language name will be followed with a ↑ x /↓ y and/or ⇑ x /⇓ y symbol. The first set of arrows indicates that the language would go up by x positions (↑ x ) or down by y positions (↓ y ) if ordered by execution time. For example in Table  3 , for the fasta benchmark, Fortran is the second most energy efficient language, but falls off 6 positions down if ordered by execution time. The second set of arrows states that the language would go up by x positions (⇑ x ) or down by y positions (⇓ y ) if ordered according to their peak memory usage. Looking at the same example benchmark, Rust, while the most energy efficient, would drop 9 positions if ordered by peak memory usage.",
        "Table  4  shows the global results (on average) for Energy, Time, and Mb normalized to the most efficient language in that category. Since the pidigits benchmark solutions only contained less than half of the languages covered, we did not consider this one for the global results. The base values are as follows: Energy for C is 57.86J, Time for C is 2019.26 ms, and Mb for Pascal is 65.96Mb. For instance, Lisp, on average, consumes 2.27x more energy (131.34J) than C, while taking 2.44x more time to execute (4926.99 ms), and 1.92x more memory (126.64Mb) needed when compared to Pascal.",
        "In order to better assess and interpret the differences among the considered languages in terms of elapsed time, memory and energy usage considering statistical evidences, a hierarchical clustering method was used. The graphical result of such a method is presented in the form of dendrograms, a format typically used to observe hierarchical relationship between objects, where the objects are joined together in a hierarchical manner from the closest grouping to the furthest. In these kinds of diagrams, it is crucial to look at the heights in which any two objects are joined together, as they reflect the distance between the clusters. In order to generate such clusters, we selected the 4 benchmarks (fannkuch-redux, fasta, nbody, spectral-norm) where all 27 languages were represented, as to maintain the 100% code coverage requirement needed to run such an analysis. Next, we used the Python package plotly  10  to generate the distinct clusters of languages for Energy consumption (CPU + DRAM), Time, and Memory. Figs. 1, 3, 5 show the generated dendogram for energy, time and memory for all considered languages. The X-axis in each Figure detail the different programming languages under analysis. The Y-axis represents the total amount of energy consumed in Joules (J), the total execution time in milliseconds (ms), and the peak memory in Megabytes (Mb) for Figs.  1, 3 , and 5 respectively. Looking at these diagrams, it is clearly noticeable that there are languages significantly distant from the group at the center of the graph (the most efficient languages in the given metric category). Given the distance between these groups of languages, and in order to obtain a more visually informative dendrogram, the same method of hierarchical clustering was applied, but now only considering the innermost languages. The results of this new procedure are shown    The execution time is represented by the line chart, with the right y-axis representing average time in milliseconds. The joining of these two charts allow us to better understand the relationship between energy and time. Finally, a scatter plot on top of both represents the ratio between energy consumed and execution time.",
        "The ratio plot allows us to understand if the relationship between energy and time is consistent across languages. A variation in these values indicates that energy consumed is not directly proportional to time, but dependent on the language and/or benchmark solution.",
        "The second set, Figs.   In order to evaluate the relationship between energy consumption, runtime and memory usage with statistical support, we first applied both the methods of Shapiro  [35]  and Anderson-Darling  [36] , and verified that our data was non-parametric. As such, we used the Spearman correlation coefficient. The Spearman's correlation coefficient is a statistical measure of the strength of a monotonic relationship between paired data. In this way, we evaluated the level of correlation between the Energy-Memory, Energy-Time and Memory-Time pairs, in order to be able to evaluate how these metrics are related between the different languages and whether high/low values of one of these metrics have significant consequences in one of the pairs. Since the Spearman method returns a ρ value between -1 and 1, indicating a negative and positive correlation, we also applied the of Rea and Parker  [37]  method to obtain a nominal classification of the correlation value module. This value can be interpreted as a measure of effect size between the paired data. The classification assigned to each range of values according to Rea and Parker is as follows:",
        "• 0.00 < 0.10 -Negligible   Finally, Table  6  summarizes the results of the DRAM energy consumption (Joules), peak memory usage (Mb), and total memory usage (Mb)."
      ],
      "Discussion": [
        "By turning to the CLBG, we were able to use a large set of software programming languages which solve various different programming problems with similar solutions. This allowed us to obtain a comparable, representative, and extensive set of programs, written in several of the most popular languages, along with the compilation/execution options, and compiler versions. With these joined together with our energy measurement framework, which uses the accurate Intel RAPL tool, we were able to measure, analyze, and compare the energy consumption, and in turn the energy efficiency, of software languages, thus answering RQ1 as shown with our results. Additionally, we were also able to measure the execution time and the peak and total memory usage, allowing us to analyze how these two relate with energy consumption.",
        "In the following subsections, we will present an analysis and discussion on the results of our study. While our main focus is on understanding the energy efficiency in languages, we will also try to understand how energy, time, and memory relate. Additionally, in this section we will try to answer the following three research questions, each with their own designated subsection.",
        "This section presents the results of energy consumption and runtime execution for each of the nine Rosetta Code tasks that we selected.",
        "For each task, we include a table ordering the languages by the energy consumption from lowest (more energy efficient) to highest (less energy efficient). We recall that both the energy, presented in Joules, and the execution time, presented in milliseconds, for each task, is the average of ten measurements.  Tables 10, 11  and 12 contain such results.",
        "In the remainder of this section, we analyze, one by one and in detail, the results we believe have the most profound impact when compared to our earlier ranking based on the CLBG, and try to understand why such differences occurred.",
        "Looking at the results for the sorting algorithms (merge and quicksort, presented in Table  10 ) we can see that Java is not performing as well as before. In fact, while most imperative implementations use the same array as the data structure to store the original and the sorted list of integer numbers (which is obtained by changing elements among positions), the Java implementations in this repository use a more OO-based approach: they use (List) collections, and build new structures which are dynamically populated with sorted elements using add methods. This overhead does influence the performance of Java. We can also see surprising differences between different sorting algorithm implementations: both Pascal and PHP solutions are very efficient performing quicksort, which is not replicated by the merge sort implementations. For these two languages, the merge sort implementations use additional temporary arrays for merging.",
        "For the (exponential) Fibonacci problem, whose results are presented in Table  10 , and although we were careful defining test cases so that all implementations would execute in a timely manner, there is one language -Python -that could not terminate (within a 24 hour timeout!) for the defined input. While there are small differences between this specific ranking and the overall CLBG one, the four most efficient solutions -Ada, Rust, C, C++ -are the same and do conclude the task very quickly and efficiently.",
        "The results of the four tasks shown in Table  11  also generally follow the CLBG-based ranking. The most energy inefficient languages in our earlier ranking -Ruby, Python, Perl -also appear in the bottom of the individual rankings. This also occurs in the other individual rankings in Tables  10  and 12 . C wins in three of these four tasks, and ranks third in the Removeduplicates task. The Remove-duplicates task, however, does not require the sorting of the resulting elements. Thus, most For the Sieve of Eratosthenes, the results presented in Table  12  are also aligned with the results obtained with CLBG. A remarkable outlier, however, is observed for the Chapel implementation: although it is very well ranked based on CLBG, it is the most inefficient language for this task! In spite of our best effort in trying to understand this corner case, we believe it deserves a more detailed study of its own, that we leave for future reference, and ideally with the involvement of an expert of Chapel. Naturally, we have confirmed that the algorithm implemented in Chapel is the correct one, and so, the result it produces is also correct.",
        "When globally considering all tasks, we can see that the C programming language is yet again generally the most energy efficient language and also the fastest. As shown in the CLBG-based ranking, the compiled languages are also the best performing ones, whilst the interpreted ones are handicapted by their execution mechanisms. In fact, for most of the tasks, the languages follow the CLBG ranking.",
        "The differences, which are most often seen through a language being placed further down in the ranking, are also very much explainable. These cases are due to specific implementations/languages which are penalized by the usage of poor implemented solutions (used in Rosetta Code) and also by chosen algorithm (for example the overhead of sorting the final results) and extra or inefficient data structure (for example the overuse of auxiliary structures).",
        "Having produced individual energy-sorted rankings for each of the 9 tasks we considered, we now wish to produce an overall language ranking so that we can compare the ranks of languages in a performance-tailored program corpus (the CLBG) to one more oriented to program comprehension (the Rosetta Code). To produce such overall ranking we use the Schulze method  [39]  to aggregate the results of the individual rankings in Tables 10, 11 and 12 into a combined one. We needed to use a different method to produce this ranking, compared to the CLBG one, because the range of values is very large and the number of implementation differ much more between tasks. Table  13  shows the Rosetta Code overall ranking that we obtained. This Rosetta Code based ranking is similar to the our earlier CLBG ranking. The top six languages in CLBG continue to be in the top five this new ranking, with the exception of Java. As we discussed before the Rosetta Code implementations in Java rely on the widely used Java Collection Framework, which require more work when compared to imperative-based solutions that use static arrays. We can also see that the Chapel language also dropped in our Rosetta Code based ranking. These results also show that interpreted languages like PHP, Lua, Ruby, Perl, Python continue at the bottom being the least energy efficient software languages."
      ],
      "Conclusion": [
        "As expected, the fact that one specific solution uses a different, more efficient approach to solve a task did influence the results of the Rosetta Code study and the ranking of the different languages. This occurs in two situations: i) the requirements for a task on Rosetta Code are not completely defined; thus, there are solutions that perform work that is not specified (for example, sorting the list after removing duplicates); and ii) some solutions that use additional temporary data structure which also force additional computational work to be performed.",
        "In this new study to validate our previous work, we use the most natural and understandable solutions available in Rosetta Code, and we did not change the program's repository: as discussed in previous sections only strictly necessary editions (such as adding test cases or main functions) were performed on programs. If we were forcing the different implementations for a task to perform exactly the same algorithm, we were essentially re-doing the CLBG-based study. This could easily be done, for example in the sorting tasks, by adding a solution in C that sort dynamically linked lists, instead of sorting the original array, but diverges from our intentions here.",
        "In regards to RQ5: How do the results of our energy consumption analysis of programming languages gathered from rigorous performance benchmarking solutions compare to results of average day-to-day solutions?, we have seen that the results of our validation study (Rosetta Code) against our original study (CLBG) are very much comparable. As expected, the results show many similarities, even though one repository source is to help learn and comprehend programs in various languages and the other is tailored to analyze the performance of languages and the other. Carefully analyzing the cases where the rankings would differ, we were able to quickly localize and explain such occurrences down to improper or inefficient use of the programming language.",
        "If wanting to understand how programming languages compare in terms of energy efficiency, and are written by general everyday programmers (through very simple and direct solutions), Table  13  shows such results. On the other hand, Table  4  details the results of the potential each language has on reducing their energy consumption if programmed by more experienced programmers or those taking into consideration basic algorithmic optimizations. With this validation study, we have shown that our results based off the rigorous benchmarking of highly optimized programs from CLBG (presented in Section 3) are representative and detail a good overall look at the energy efficiency of programming languages written by either expert or non-expert programmers.",
        "This first category describes threats which may influence our capacity to draw correct conclusions  [41] .",
        "Fishing is a possible threat as one may be searching for particular results, thus making the analysis not independent  [41] . In the case of our study, we are not evaluating a programming language(PL) that we may have proposed and hence have no particular interest in the outcome. Thus, we are not searching for a particular result, and as such, this threat does not apply to our study.",
        "A common threat is the reliability of measures. In our case, when measuring the energy consumption of the various different programming languages, other factors alongside the different implementations and actual languages themselves may contribute to variations, i.e. specific versions of an interpreter or virtual machine. To avoid this, we executed every language and benchmark solution equally. In each, we measured the energy consumption (CPU and DRAM), execution time, and peak and total memory 10 times, removed the lowest and highest 20% outliers, and calculated the median, mean, standard deviation, min, and max values. This allowed us to minimize the particular states of the tested machine, including uncontrollable system processes and software. However, the measured results are quite consistent, and thus reliable. In addition, the used energy measurement tool has also been proven to be very accurate.",
        "Another common threat is the reliability of treatment implementation. The implementations used to evaluate the PLs were produced by external developers. We simply reused the settings from CLBG which were also applied to Rosetta tasks. Thus, these implementations are independent from this study and are the best available as the CLBG is a running contest of the performance of.",
        "Regarding random heterogeneity of subjects, we used all the available languages in the CLBG, that is, 27 different PLs. Although there are hundreds of languages, this set includes many popular languages and also more academic ones, thus covering a vast set of PLs. In fact, several communities from news pages, to social-media, to Reddit have found our work broad enough to be interesting.",
        "Internal validity This category concerns itself with what factors may interfere with the results of our study, that is, that may influence the relationship between the treatment and the outcome  [41] .",
        "Instrumentation is one of the possible causes of internal validity  [41] . This refers to the artifacts used during the experiment. In our case, we used scripts to collect the energy, time and memory used during the execution of the programs. However, these are simple scripts used to call RAPL for measurement during the execution of programs. They were previously validated and tested  [23, 24]  and are also publicly available in the paper's online appendix.",
        "Construct validity This category concerns the generalization of the results to the concept or theory behind the experiment  [41] .",
        "Inadequate preoperational explication of constructs is a possible issue related to the constructs not being well defined prior to being measured  [41] . In our case we evaluated the energy, time and memory used by programs, and thus the measurements were obvious, making this issue minor or nonexistent in our study.",
        "Another possible issue is the mono-operation bias concerned with the under-representation of a construct. We have used about 10 programs to evaluate each PL. These programs were proposed by others to evaluate the performance of PLs and thus were designed to stress the languages within the context of a contest. Thus, they seem to represent an interesting way of evaluating the PLs.",
        "Regarding the mono-method bias, we have indeed used just a single tool to measure energy and time (RAPL), and another tool for memory (the Unix-based time tool). However, both known to be very precise for measuring energy, time, and memory, thus their results are reliable. The interaction of different treatments is also a possible issue. However, we have used different and independent programs to evaluate the languages. Between each measuring execution (as common practice in measuring energy consumption), there was a two minute idle time rest to allow the system to cool-down, as to reduce over heating (which may affect energy measurements), and to allow the system to treat garbage collecting.",
        "External validity This type of threat is concerned with the generalization of the results to an industrial setting  [41] .",
        "A common threat is termed interaction of selection and treatment meaning the population chosen is not representative, in our case the PLs  [41] . In the first study we analyzed 27 different programming languages. These PLs include popular languages among industry such as C/C++, Java, C#, JavaScript, Ruby, PHP or Python.  15  Thus, our study applies also to an industrial setting, at least regarding the PLs used.",
        "Another external threat is the interaction of setting and treatment, that is, the experimental setting might not represent the industrial setting. Each PL was evaluated with roughly 10 solutions to the proposed problems, totaling out to almost 270 different cases. The implementation solutions we measured were developed by external experts in each of the programming languages, with the main goal of \"winning\" by producing the best solution for performance time. While the different languages contain different implementations, they were written under the same rules, all produced the same exact output, and were implemented to be the fastest and most efficient as possible. Having these different yet efficient solutions for the same scenarios allows us to compare the different programming languages in a quite just manner as they were all placed against the same problems. Moreover, the compilers and computers used are recent and thus in line with nowadays industry. For the Rosetta the solutions are not so curated. In any case, the authors have reviewed and used solutions that were correct thus solving the underlying problem. While our benchmarking system is server based, studies have shown that there is no statistical difference between server platforms and embedded systems in regards to energy based readings  [26] , thus the results can be generalized directly to embedded systems. In regards to the generalization to mobile systems, this can not be completely assured as results differ slightly between server/embedded based systems and mobile, and sometimes even between independent studies in mobile, if analyzing on a small scale. Overall however, the results seem to maintain their tendencies  [17, 4]  In general, in this category of threats it is paramount to report the characteristics of the experiment in order to understand its applicability to other contexts  [41] . The actual approach and methodology we used also favors easy replications. This can be attributed to the CLBG containing most of the important information needed to run the experiments, these being: the source code, compiler version, and compilation/execution options. Moreover, all the material used and produced is publicly available at https://sites .google .com /view /energy-efficiency-languages. Thus we believe these results can be further generalized, and other researchers and industry can replicate our methodology for future work.",
        "In this paper, we present an extended work of a series of systematic comparisons and rankings over the energy efficiency of 27 well-known software languages. These comparisons take as their original code base programs from a popular programming language benchmarking competition, The Computer Language Benchmarks Game (CLBG), where experts in different languages compete to produce the most performance efficient solution.",
        "We were able to show which were the most energy efficient software languages, execution types, and paradigms across 10 different benchmark problems. We were also able to relate execution time and memory consumption to energy con-sumption to understand not only how memory usage affects energy consumption, but also how time and energy relate. This allowed us to understand if a faster language is always the most energy efficient. As we saw, this is not always the case. Additionally, we have created rankings, group clusterings, and correlation tables of programming languages based on their energy consumption, execution time, and memory usage. In addition, we further analyzed the correlation between energy consumption and memory usage.",
        "As often times developers have limited resources and may be concerned with more than one objective, or efficiency characteristic, we established rankings of the best/worst languages according to a combination of different objectives: limited battery, limited time, and limited memory capacity.",
        "In order to properly assess our original findings, we revisited this study and presented a new empirical study based on a chrestomathy repository, Rosetta Code  [27]  in order to validate our original study. This allows us to also understand if the analyzed performance-oriented solutions, of which we based our results on, are representative of day-to-day programming and by non-expert programmers.",
        "This new validation considered 9 tasks from Rosetta Code, and their solutions in the programming languages that we have previously considered. These results showed many similarities when compared to the strict and rigorous performanceoriented benchmarks used to produce our programming language rankings. Additionally, albeit very few (and highly explainable) differences, we have concluded the originally presented rankings and results are representative of both expert and non-expert programmers.",
        "Our work helps contribute another stepping stone in bringing more information to developers to allow them to become more energy-aware when programming."
      ],
      "Related Work": [
        "The work presented in this paper extends previous work in  [23]  and  [24] . In this extended version, an analysis on total memory usage was performed to better understand the relationship between continuous memory usage and DRAM energy consumption. Additionally, we replicated our study on a different repository, the Rosetta Code chrestomathy repository. This not only allowed us to validate our previous programming language energy ranking using the CLBG, but also to understand how different are the results of programs on a repository for performance based benchmarking and a repository for learning and comprehensibility.",
        "The CLBG benchmark solutions have already been used for validation purpose by several research works. Among other examples, CLGB was used to study dynamic behavior of non-Java JVM languages  [42] , to analyze dynamic scripting languages  [43]  and compiler optimizations  [44] , or even to benchmark a JIT compiler for PHP  [45] . At the best of our knowledge, CLGB was only used once for energy consumption analysis. In  [17] , the authors used the provided Haskell implementations, among other benchmarks, to analyze the energy efficiency of Haskell programs from strictness and concurrency perspectives, while also analyzing the energy influence of small implementation changes.",
        "A similar study using the Rosetta Code repository was performed  [26] , where the authors looked at the energy-delay implications on 14 programming languages, on three different computing platforms (embedded, laptop, and server. They too produced very similar results to ours across the three platforms, and found that there is no statistical differences between server platforms and embedded systems. Thus, our server based benchmarking system can easily be generalized for embedded devices. They further explored  [46]  the energy-delay implications within inter-process communication systems and observed how energy consumption and run-time performance can very significantly across different programming language implementations.",
        "While several works have shown indications that a more time efficient approach does not always lead to the most energy efficient solution  [17, 15, 16, 18, 2, 4] , these results were not the intended focus nor main contribution, but more of a side observation per se. We focused on trying to understand and directly answer this question of how energy efficiency and time relate.",
        "Nevertheless, the energy efficiency in software problem has been growing in interest in the past few years. In fact, studies have emerged with different goals and in different areas, with the common vision of understanding how development aspects affect the energy consumption in diversified software systems. For instance, for mobile applications, there are works focused on analyzing the energy efficiency of code blocks  [47] [48] [49] [50] , or just monitoring how energy consumption evolves over time  [51] . Other studies aimed at a more extensive energy consumption analysis, by comparing the energy efficiency of similar programs in specific usage scenarios  [13, 11] , or by providing conclusions on the energy impact of different implementation decisions  [52] . Several other works have shown that several factors, such as different design patterns  [6, 7] , Android keyboards  [53]  and energy footprints  [54] , coding practices  [15, 55, 17, 56, 8, 9, 49] , technical energy debt  [57] , and data structures  [2, 31, 3, 19, 58, 59] , actually have a significant influence in the software's energy efficiency.",
        "In the context of Android, a particular line of work relates to ours. The authors of  [60, 4]  also used CLBG and the Rosetta Code repository to compare JavaScript, Java, and C/C++ in a setting specifically targeted for Android systems. Using programs from these sources they concluded that most of the times JavaScript consumes less energy, but there is no overall winner. Moreover, they also advocate that faster programs are not always the ones that consume less energy."
      ]
    }
  },
  {
    "paperId": "ea11192b7f351071f1efaf6ce37f47bc9af6dfb4",
    "title": "Relating Natural Language Aptitude to Individual Differences in Learning Programming Languages",
    "sections": {
      "Results": [
        "Python learning outcomes. As expected, large individual differences were observed in each of the Python learning outcomes. For example, the fastest learner moved through the lessons two-and-a-half times as quickly as did the slowest learner (Fig.  1A : mean learning rate = 1.25, range = 0.81-2.0, sd = 0.24). Similar variability was also observed in the two post-test measures: programming accuracy (mean = 0.57, range = 0.01-0.92, sd = 0.19) and declarative knowledge test (mean = 0.73, range = 0.46-0.92, sd = 0.10). The three outcome measures were highly positively intercorrelated: learning rate and programming accuracy [r(34) = 0.79, p < 0.001]; learning rate and declarative knowledge [r(34) = 0.74, p < 0.001]; and programming accuracy and declarative knowledge [r(34) = 0.71, p < 0.001]. This provides evidence that people who moved through the program more quickly were not sacrificing speed for learning accuracy.",
        "Behavioral predictors of Python learning outcomes. Language aptitude. Consistent with our hypothesis, language aptitude, as assessed by the Modern Language Aptitude Test (MLAT)  22  , was a robust predictor of all of the Python learning outcomes. Specifically, learning rate [Fig.  1B : r(34) = 0.56, p < 0.001], programming accuracy [r(34) = 0.54, p = 0.001], and declarative knowledge [r(34) = 0.45, p = 0.006] were all positively correlated with MLAT percentile. These correlations remained significant when applying False Discovery Rate (FDR) corrections for multiple comparisons (ps <0.05).",
        "Numeracy. Numeracy, as measured by the abbreviated numeracy scale  23  , was also a significant predictor of all Python learning outcomes. Specifically, learning rate [Fig.  1C : r(29) = 0.52, p = 0.003], programming accuracy [r(29) = 0.54, p = 0.002], and declarative knowledge [r(29) = 0.42, p = 0.019] were all positively correlated with numeracy scores. Correlations between numeracy and learning rate and programming accuracy remained significant when FDR corrections for multiple comparisons were applied (ps < 0.05).",
        "General cognitive abilities. Fluid reasoning, working memory updating, working memory span, and inhibitory control, were also significant predictors of learning to program in Python. Specifically learning rate [Fig.  1D : r(34) = 0.66, p < 0.001], programming accuracy [r(34) = 0.71, p < 0.001], and declarative knowledge [r(34) = 0.55, p = 0.001] were all strongly positively correlated with fluid reasoning. Similarly, learning rate [r(34) = 0.45, p < 0.005], programming accuracy [r(34) = 0.54, p = 0.001], and declarative knowledge [r(34) = 0.41, p = 0.013] were all positively correlated with working memory updating. In contrast, working memory span only correlated with learning rate [r(34) = 0.43, p = 0.01] and programming accuracy [r(34) = 0.38, p = 0.02], and inhibitory control only correlated with declarative knowledge [r(34) = 0.44, p = 0.008]. Figure  1  depicts individual differences in rate of learning to program in Python at the group level (A), along with scatterplots relating these differences to language aptitude (B), numeracy (C), and fluid reasoning (D). The complete list of bivariate correlations between behavioral predictors and Python learning outcomes with False Discovery Rate (FDR) corrections applied is included in Supplementary Table  S1 . Intercorrelations between behavioral predictor variables are included in Supplementary Table  S2 .",
        "Resting-state EEG predictors of Python learning outcomes. Our results also provide the first evidence that measures of intrinsic network connectivity obtained from resting-state (rs)EEG can be used to predict Python learning outcomes. The predictors investigated were defined a priori, constrained to a set of oscillation-based features of rsEEG that have previously predicted natural language learning  20, 21  . The complete set of rsEEG predictors, along with references to the papers in which they relate to natural language learning, are included in Supplementary Tables  S3  and S4 .   S3 . rsEEG coherence. Only programming accuracy was predicted by rsEEG coherence measures. Specifically, less coherence within the left posterior network was associated with higher programming accuracy across both theta [r(34) = -0.37, p = 0.031] and beta [r(34) = -0.38, p = 0.024] frequency bands. The complete set of bivariate correlations between rsEEG power and Python learning outcomes, which did not survive FDR corrections for multiple comparisons, are included in Supplementary Table  S4 .",
        "Stepwise regression analyses. To better understand how the cognitive and neural indices investigated combine to predict facile programming in high-aptitude learners, we entered each of the predictors identified by bivariate correlations into separate stepwise regression analyses aimed at explaining the three outcome variables.",
        "Learning rate. When the six predictors of Python learning rate (language aptitude, numeracy, fluid reasoning, working memory span, working memory updating, and right fronto-temporal beta power) competed to explain variance, the best fitting model included four predictors: language aptitude, fluid reasoning (RAPM), right fronto-temporal beta power, and numeracy. This model was highly significant [F(4,28) = 15.44, p < 0.001], and explained 72% of the total variance in Python learning rate. Language aptitude was the strongest predictor, explaining 43.1% of the variance, followed by fluid reasoning, which contributed an additional 12.8% of the variance, right fronto-temporal beta power, which explained 10%, and numeracy scores, which explained 6.1% of the variance.",
        "Programming accuracy. By comparison, when the seven predictors of programming accuracy (language aptitude, numeracy, fluid reasoning, working memory span, working memory updating, left posterior theta coherence, left posterior beta coherence) competed for variance, the best fitting model included three predictors: fluid reasoning, language aptitude, and working memory updating. This model was also highly significant [F(3,24) = 15.93, p < 0.001], and explained 66.7% of the total variance in Python programming accuracy. Fluid intelligence was the strongest predictor of programming accuracy, explaining 50.1% of the total variance, followed by language aptitude, which explained an additional 8.7%, and working memory updating, which explained 7.8% of the variance.",
        "Declarative knowledge. Finally, when the eight predictors of post-test declarative knowledge (language aptitude, numeracy, fluid reasoning, working memory updating, inhibitory control, vocabulary, right fronto-temporal low-gamma power, and right posterior low-gamma power) were entered into a stepwise regression analysis, the best fitting model included only two predictors: fluid reasoning and right fronto-temporal-low-gamma power. This model was highly significant [F(2,25) = 13.13, p < 0.001], and explained 51.2% of the total variance in post-test declarative knowledge scores. Fluid reasoning was also the best predictor of declarative knowledge, explaining 30.9% of the total variance, with right fronto-temporal-low-gamma power explaining an additional 20.3% of the variance.",
        "The results of these regression analyses are summarized visually in Fig.  3 , which categorizes the predictor variables into four types: (1) language aptitude; (2) general cognitive; (3) neuropsychometrics; and (4) numeracy.",
        "Full regression tables with fit and uncertainty statistics are included in the supplementary materials. The data and regression scripts, along with readme files that describe them, can be downloaded at https://github.com/ UWCCDL/ComputerWhisperers."
      ],
      "Discussion": [
        "The research reported herein describes the first investigation of the neurocognitive predictors of learning to program in Python. The results of this research demonstrate the utility of adopting natural language learning in adulthood as a model for understanding individual differences in the ability to learn modern programming languages. Specifically, using the combination of neural and behavioral measures that have previously been associated with natural language learning, we were able to explain up to 70% of the variability in Python learning outcomes. As depicted in Fig.  3 , behavioral language aptitude (salmon) explained an average of over 17% of the variance in Python outcomes (rightmost column). Importantly, either the behavioral indices of language aptitude (salmon), the neural predictors of natural language learning (beige), or both, explained unique variance in Python learning outcomes, even when robust predictors such as fluid reasoning and working memory capacity, which also relate to language learning, were accounted for.",
        "In comparison, numeracy only explained unique variance in Python learning rate, and accounted for an average of 2% of the variance across outcome variables. These results are consistent with previous research reporting higher or unique predictive utility of verbal aptitude tests when compared to mathematical ones  2, 12, 13  . It is also important to note that some Codecademy lessons focus on computing arithmetic operations, and others use mathematical equations to demonstrate concepts such as \"Booleans. \" The ability to execute such operations quickly may explain why numeracy predicted unique variance in learning rate, but not in programming accuracy or declarative knowledge.",
        "The regression analyses also showed that general cognitive abilities, including fluid reasoning ability and working memory factors (dark red), were the best average predictors of programming outcomes, explaining nearly 34% of the variance across outcome measures. These results are consistent with the findings of previous research on programming aptitude  4, 24  , suggesting that the results generalize to contemporary programming languages as well as to online learning environments. Our results are also consistent with classic information processing models of programming 3 , which include iterative roles for working memory and problem solving. Specifically, these models describe the processes by which programming goals, like language, must be divided into manageable chunks, and the subgoals of these chunks must be held in working memory and used to guide comprehension and production processes  3, 10  .",
        "Our findings also provide the first evidence that rsEEG features may be used as neuropsychometric indices of programming aptitude. Specifically, we found that power in beta and low-gamma frequency bands recorded over right fronto-temporal networks at rest predicted unique variance in rate of Python learning and programming accuracy, respectively. In fact, rsEEG measures (Fig.  3 : beige) explained an average of 10% of the variance in Python programming outcomes. Similar positive correlations between frontal beta power at rest and learning rate were found in both of our previous rsEEG investigations of natural language learning in adulthood  20, 25  . These findings add to the increasing body of literature suggesting that characterizations of resting-state brain networks can be used to understand individual differences in executive functioning  26  and complex skill learning more broadly  [19] [20] [21]  .",
        "Though a considerable body of research has investigated the relation between individual differences in alpha power at rest and online cognitive processes  27  , the implications of differences in resting-state beta power for cognitive abilities are more mysterious. Beta oscillations, however, have become increasingly associated with online language processes 28 . They have also been related to the top-down gating of information into working memory  29  . Similarly, according to the Predictive Coding Framework  30  , beta oscillations function both to maintain dynamic representations of meaning during sentence comprehension, and to deploy top-down mechanisms that facilitate comprehension of predicted completions. Linking these theories to the current data, one recent study showed that individual differences in the ability to learn syntactic structures in an artificial grammar task were related to differences in synchronization over beta frequencies during learning  31  . A proposed connection between beta computations and resting-state measures is offered by Raichle and Schneider, who suggest that resting-state network activity reflects \"… the maintenance of information for interpreting, responding to, and even predicting environmental demands\"  32  . Thus, the data reported herein contribute to a growing body of research suggesting that individual differences in beta power and coherence at rest may reflect differences in the ability to acquire and apply statistical knowledge based on sequentially presented information. This proposition is purely speculative and requires further investigations relating differences in resting-state and task-based EEG to learning parameters.",
        "In the current experiment, the various Python learning outcomes were predicted to differing degrees by the components of neurocognitive battery employed. Specifically, programming accuracy was most strongly predicted by fluid cognitive abilities; whereas, learning rate was most strongly predicted by the MLAT, a test that is largely composed of declarative and associative memory-based tasks. These data are consistent with an early cognitive model proposed by Shniederman and Mayer  4  . Specifically, they converge to suggest that learning to program and writing programs depend upon somewhat different cognitive abilities. In particular, both their model and our data highlight the relative importance of analogical reasoning (or problem solving) and working memory capacity for program generation.",
        "Taken together, the results reported herein provide foundational information about the neurocognitive characteristics of \"high aptitude\" learners of Python, and by virtue, about who may struggle given equal access to learning environments. We argue, as have others before us  2, 6  , that both educational and engineering practices have proceeded without this critical knowledge about why, and for whom, learning to program is difficult. Contrary to widely held stereotypes, the \"computer whisperers\" investigated herein were facile problem solvers with a high aptitude for natural languages. Although numeracy was a reliable predictor of programming aptitude, it was far from the most significant predictor. Importantly, this research also begins the process of identifying the neural characteristics of individual differences in Python learning aptitude, which can be used as targets for technologies such as neurofeedback and neurostimulation that modify patterns of connectivity and alter corresponding behaviors  33, 34  . Important future work is needed to determine the extent to which our results will translate to classroom learning environments, to less \"user friendly\" languages such as Java that are more widely employed in software engineering spaces, and to higher programming proficiency levels. Still, the research reported herein begins to paint a picture of what a good programmer actually looks like, and that picture is different in important ways from many previously held beliefs."
      ],
      "Methods": [
        "Participants. Forty-two healthy adults aged 18-35 years were recruited for participation in this study. Five participants were excluded from these analyses due to attrition (not completing the training sessions), and one participant was excluded because he was an extreme outlier in learning rate (>3 sd away from the mean). The remaining 36 participants (21 female) were included in our analyses. All participants were right-handed native English speakers with no exposure to a second natural language before the age of 6 years (mean = 13.9 years, range = 6-22 years), and with moderate self-rated second language proficiency (mean = 3.2/10, range = 0-8/10). All experimental protocols and paradigms were approved by the University of Washington Institutional Review Board, and all participants provided informed consent in accordance with the standards set forth by the same review board. All individuals were compensated for their participation.",
        "Rasch-based numeracy scale. Numeracy was assessed using a Rasch-Based Numeracy Scale which was created by evaluating 18 numeracy questions across multiple measures and determining the 8 most predictive items  23  . The test was computerized and untimed."
      ]
    }
  },
  {
    "paperId": "6433b460bd25bfb60f153d8700364335ba3cd29c",
    "title": "Physical validation of simulators in computer graphics",
    "sections": {}
  },
  {
    "paperId": "1f23bb14daf16a7441aa0102c91d2a4582f63529",
    "title": "Fundamentals of computer graphics",
    "sections": {
      "Introduction": [
        "Graphics Areas p.",
        "Major Applications p.",
        "Graphics APIs p.",
        "Graphics Pipeline p.",
        "Numerical Issues p."
      ]
    }
  },
  {
    "paperId": "85bfd320fd6aed25614d69d9cff71acf98d111be",
    "title": "Code replicability in computer graphics",
    "sections": {
      "Introduction": [
        "The ability to reproduce an experiment and validate its results is a cornerstone of scientific research, a key to our understanding of the world. Scientific advances often provide useful tools, and build upon a vast body of previous work published in the literature. As such, research that cannot be reproduced by peers despite best efforts often has limited value, and thus impact, as it does not benefit to others, cannot be used as a basis for further research, and casts doubt on published results. Reproducibility is also important for comparison purposes since new methods are often seen in the light of results obtained by published competing approaches. Recently serious concerns have emerged in various scientific communities from psychological sciences  [Open Science Collaboration et al. 2015]  to artificial intelligence  [Hutson 2018 ] over the lack of reproducibility, and one could wonder about the state of computer graphics research in this matter.",
        "In the recent trend of open science and reproducible research, this paper aims at assessing the state of replicability of papers published at ACM Transactions on Graphics as part of SIGGRAPH conferences. Contrary to reproducibility which assesses how results can be obtained by independently reimplementing published papers -an overwhelming task 1:2 Nicolas Bonneel, David Coeurjolly, Julie Digne, and Nicolas Mellado given the hundred papers accepted yearly to this event -replicability ensures the authors' own codes run and produce the published results. While sharing code is not the only available option to guarantee that published results can be duplicated by a practitioner -after all, many contributions can be reimplemented from published equations or algorithm descriptions with more or less effort -it remains an important tool that reduces the time spent in reimplementation, in particular as computer graphics algorithms get more sophisticated.",
        "Our contributions are twofold. First, we analyze code sharing practices and replicability in computer graphics. We hypothesize strong influence of topics, an increase of replicability over time similar to the trend observed in artificial intelligence  [Hutson 2018] , and an increased impact of replicable papers, as observed in image processing  [Vandewalle 2019 ].",
        "To evaluate these hypotheses, we manually collected source codes of SIGGRAPH 2014, 2016 and 2018 papers and ran them, and when possible, assessed how they could replicate results shown in the paper or produce reasonably similar results on different inputs. Second, we provide detailed stepby-step instructions to make these software packages run (in practice, in many cases, code adaptations had to be done due to dependencies having evolved) through a website, thus becoming a large code review covering 151 codes obtained from 374 SIGGRAPH papers. We hope this platform can be used collaboratively in the future to help researchers having difficulties reproducing published results.",
        "Our study shows that:",
        "• Code sharing is correlated with paper citation count, and has improved over time. • Code sharing practices largely vary with sub-fields of computer graphics. • It is often not enough to share code for a paper to be replicable. Build instructions with precise dependencies version numbers as well as example command lines and data are important."
      ],
      "Methods": [
        "Our goal is to assess trends in replicability in computer graphics. We chose to focus on the conference in the field with highest exposure, ACM SIGGRAPH, as an upper bound proxy for replicability. Although this hypothesis remains to be verified, this conference more often publishes completed research projects as opposed to preliminary exploratory ideas that are more often seen in smaller venues which could explain lower code dissemination. To estimate a trend over time, we focus on three SIGGRAPH conferences: SIGGRAPH 2014 (Vancouver, 127 accepted papers), 2016 (Anaheim, 119 accepted papers), and 2018 (Vancouver, 128 accepted papers). We did not include SIGGRAPH 2019 (Los Angeles) since authors sometimes need time to clean up and publish their code after publication. We did not include SIGGRAPH Asia nor papers published in ACM Transactions on Graphics outside of the conference main track to reduce variability in results and keep a more focused scope. We chose a two-year interval between conferences in the hope to get clearer trends, and to keep a tractable number of papers to evaluate.",
        "We searched for source codes as well as closed-source binaries for all papers. We restricted our search to original implementations and reimplementations authored and released by the original authors of the paper, excluding reimplementations by others, as we aim at assessing replicability and not reproducibility (see Sec. 2). For each paper, we report the objective and subjective information described below.",
        "Identifying and factual information. This includes the paper name and DOI, ACM keywords, pdf, project and code or binaries URLs if they have been found, as well as information indicating if authors are from the industry, academia, or unaffiliated, for further analysis. For papers, we include information as whether they can be found on arXiv or other Open Archive Initiative providers we may have found, in open access on the ACM Digital Library, or by other means such as institutional web pages. Aside from ACM keywords, we further categorize papers into 6 broad topics related to computer graphics, and we also keep track of whether they relate to neural networks. We defined these topics as:",
        "• Rendering. This includes simulating light transport, real-time rendering, sampling, reflectance capture, datastructures for intersections, and non-photorealistic rendering.",
        "• Animation and simulation. This includes character animation, motion capture and rigging/skinning, cinematography/camera path planning, deformable models as well as fluid, cloth, hair or sound simulation, including geometric or topology problems related to these subjects. • Geometry. This includes geometry processing and modeling, for point-based, voxel-based and mesh-based geometries, as well as topology, mapping, vector fields and shape collection analysis. We also include image-based modeling.",
        "• Images. This includes image and video processing, as well as texture synthesis and editing, image segmentation, drawing, sketching and illustration, intrinsic decomposition or computational photography. We also included here image-based rendering, which relies more on image techniques than rendering. • Virtual Reality. This category includes virtual and augmented reality, 3d displays, and interactions. • Fabrication. This includes 3d printing, knitting or caustic design.",
        "We strive to classify each paper into a single category to simplify analyses. Both these categories and paper assignments to these categories can be largely debated. While they may be prone to errors at the individual level, they still provide meaningful insight when seen as statistical aggregates. These categories were used in our analysis instead of ACM keywords for several reasons: first, we counted more than 127 different ACM keywords which would make overspecialized categories.",
        "The hierarchical nature of this taxonomy also makes the analysis more complicated. In Fig.  2  we show the distribution of ACM keywords of papers involved in each of our categories. Interestingly, this visualization exacerbates the lack of ACM keywords dedicated to fabrication despite the increasing popularity of this topic. Information about code includes code license, presence of documentation, readme files and explicit mention of the code authors (who usually are a subset of the paper authors), as well as build mechanism (Makefile, CMakeLists, SCons, IDE projects, or other types of scripts), and lists of dependencies. We notably indicate whether library or software dependencies are open source (e.g., Eigen, OpenCV), closed source but free at least for research purpose (e.g., mosek, CUDA or Intel MKL), or closed source and paying even for research purpose (e.g., Matlab). Similarly, we ask whether the code depends on data other than examples or input data (e.g., training data or neural network description files) and their license.",
        "One of our key contributions is that we report the undocumented steps required to make the code run -from bug fixes to dependency installation procedures. We believe this information is valuable to the community as these steps are often independently found by students relying on these codes sometimes after significant effort.",
        "Subjective judgments on replicability. For papers without published code, this includes information as to whether the paper contains explicit algorithms and how much effort is deemed required to implement them (on a scale of 1 to 5). For algorithms requiring little reimplementation effort (with a score of 5) -typically for short shaders or short self-contained algorithms -this can give an indication as to why releasing the code was judged unnecessary. For papers containing code, we evaluate how difficult it was to replicate results through a number of questions on a scale of 1 to 5. This includes the difficulty to find and install dependencies, to configure and build the project, to fix bugs, to adapt the code to other contexts, and how much we could replicate the results shown in the paper. We strived to remain flexible in the replicability score: often, the exact input data were not provided but the algorithms produced satisfactory results that are qualitatively close to those published on different data, or algorithms relied on random generators (e.g., for neural network initializations) that do not produce repeatable number sequences and results. Contrary to existing replicability initiatives, we did not penalize these issues, and this did not prevent high replicability scores.",
        "We shared the task of evaluating these 374 submissions across 4 full-time tenured researchers (authors of the paper), largely experienced in programming and running complex computer graphics systems. Reasonable efforts were made to find and compile the provided code, including retrieving outdated links from the WayBack Machine  [Tofel 2007 ], recreating missing Makefiles, debugging, trying on multiple OS (compiling was tested on Windows 10, Debian Buster, Ubuntu 18.04 and 19.10 and MacOS 10.15 1  ), or adapting the code to match libraries having evolved. Efforts to adapt the code to evolved libraries, compilers or languages are due to practical reasons: it is sometimes impractical to rely on old Visual Studio 2010 precompiled libraries when only having access to a newer version, or to rely on TensorFlow 1.4.0 requiring downgrading CUDA drivers to version 8 for the sole purpose of having a single code run. We chose to avoid contacting authors for clarifications, instructions or to report bug fixes to protect anonymity. We also added the GitHub projects to Software Heritage  [Di Cosmo and Zacchiroli 2017]  when they were not already archived and gave the link to the Software Heritage entry in our online tool."
      ],
      "Results": [
        "This section analyzes both objective and subjective metrics. All reported p-values were adjusted for multiple comparisons using the false discovery rate control procedure proposed by  Benjamini and Hochberg [1995] ."
      ],
      "Discussion": [
        "Availability of papers. Papers are overall available. Over all 374 papers, only two are available only on the ACM Digital Library. Notably, ACM provides free access to all SIGGRAPH and SIGGRAPH Asia proceedings though it Fig.  3 . We designed a web interface to explore our collected data allowing to see individual paper replicability and build instructions, available at https://replicability.graphics.",
        "is little advertised  [?] . Also, 27 are available as preprints on arXiv (9 only on arXiv), 17 on HAL (7 only on HAL) 3  , 44 benefit from the ACM Open Access policy -the other papers being available as preprints at least on the authors website or other paper repositories.",
        "Availability of code. Software packages were available for 151 papers, which consist of 133 papers for which source code was provided plus 18 papers for which no source code was provided but instead compiled software was provided. For the rest of the analysis, we considered both compiled and open source software combined. While open source research codes allow for adaptation, making it easier to build upon them, and are thus ideal, binary software at least allows for effortless method comparisons. Nevertheless, among these software packages, we could not run 19 of them due to technical issues preventing the codes to compile or run, and 5 of them due to lack of dedicated hardware (see Sec. 6). Among these 133 codes, 60 do not have license information, which could notably prevent code dissemination in the industry, and 11 do not come with any documentation nor build instructions.",
        "We perform 𝜒 2 tests to analyze trends in code sharing. Overall, codes or binaries could be found for 37 papers out of 127 (29.1%) in 2014, 47 out of 119 (39.5%) in 2016, 67 papers out of 128 (52.3%) in 2018. This increase is statistically significant between 2014 and 2018 (𝑝 = 1.3 10 -3 ), though not between 2014 and 2016 (𝑝 = 0.13) nor between 2016 and 2018 (𝑝 = 0.086). This trend is similar to that observed in artificial intelligence  [Gundersen and Kjensmo 2018; Hutson 2018] . Results can be seen in Fig.  4 . In two cases, we had to retrieve the code from the WayBack Machine  [Tofel 2007 ] due to expired URLs. Further analysis per topic shows vastly different practices, with 17.1% of papers sharing code for Fabrication, 26.9% for Animation, 31.8% for Virtual Reality, 47.9% for Rendering, 51.9% for Geometry and 57.9% for Images (Fig.  4 ).",
        "We also analyzed the involvement of at least one author from the industry on the release of codes or binaries. We found that overall, papers involving the industry provided code or binaries 31.3% of the times, while this was the case for 45.4% of purely academic papers -a difference that is significant (𝑝 = 0.031). This could be explained by strict rules imposed by employers, understandably worried about industrial consequences of sharing a code.",
        "Given the sheer amount of deep learning codes available online, we hypothesized that deep learning-related papers were more likely to share code. We tested this hypothesis on our dataset, but we found that they provided code only 44.6% of the times (25 out of 56), while this was the case 39.6% of the times for non-deep papers (126 out of 318) -a non-significant difference (𝑝 = 0.48).",
        "We finally found that, in the long term, sharing code results in higher citations, with a median citation count of up to 67 in 2014 for papers sharing code compared to 43 for papers not sharing code (see Fig.  5 ). A Mann-Whitney U-test gives this difference significant (𝑝 = 0.045). This observation is similar to that observed in image processing  [Vandewalle 2019 ] though the effect is less pronounced (they observed a doubling of citation rates). Few additional information is given in Table  1 .",
        "As replicability scores are subjective, we first perform an analysis of variance (ANOVA), despite some limitations here (see  Norman [2010]  for a full discussion), aimed at determining two things: is there a dependence on the reviewer of the code on replicability scores? And, does the year influence replicability (as it would seem that older non-maintained codes are harder to replicate)? The ANOVA is performed on the replicability score, taking only papers with codes for which compiling was successful, and with two factors: reviewer and year. The answer to both questions seems negative (resp. 𝑝 = 0.13 and 𝑝 = 0.27).",
        "To make the codes run, we had to modify source codes in 68 out of 151 codes. These code alterations were deemed Fig.  5 . We compute the median number of citations and its 95% confidence intervals for papers sharing code (or executable) and for papers not sharing code nor executable. difficult (\"easy to fix bugs\" score ≤ 2 out of 5) for 20 codes.",
        "The time spent to make codes run, including time to debug and compile dependencies was longer than 100 minutes for 27 codes.",
        "In the years covered by this study, we found a total of 5 papers with a Replicability Stamp from the Graphics Replicability Stamp Initiative  [Panozzo 2016] . While this number is too low to derive meaningful statistics, one can note that out of these 5 papers, 4 get the maximum score for results replication. This could be expected because this initiative ensures that a script for each single result shown in the paper is provided. A limitation is that these scripts are only guaranteed to work at the time when the stamp is granted -a limitation shared by the present study."
      ],
      "Conclusion": [
        "Our analysis has a number of limitations. First, the data we collected may only be partly reliable. While we spent reasonable efforts to find, run and compile codes, it is possible that we missed codes, or that additional efforts or contacting the authors for clarifications or to report bugs would result in different outcome for a few papers. Similarly, we could not fully evaluate codes that depend on specific hardware (such as spatial light modulators, microcontrollers, Hall effect sensors etc.) for 4 papers. Our analysis focused on assessing the codes provided by the authors which only assesses replicability but not reproducibility: there are instances for which papers were successfully reimplemented by other teams, which falls out of our analysis scope. It could also be expected that certain codes could be available upon request ; in fact, in a few cases, the provided code relied on data only available upon request, which we did not assess.",
        "Second, the codes we found and assessed may have evolved after the paper has been published, which we cannot control. Similarly, the published code could be a cleaned-up version of the original code, or even a full reimplementation.",
        "Third, our focus on SIGGRAPH could hide a more negative picture of the entire field. We believe that the exposure SIG-GRAPH probably gives biases our results, with a tendency to find more codes here than in smaller venues. It would be an interesting future work to compare replicability across computer graphics venues.",
        "Our in-depth study of three years of ACM SIGGRAPH conference papers showed a clear increase in replicability as authors are increasingly sharing their codes and data. Our study also showed that sharing code was correlated with a higher impact, measured in terms of citation numbers. We developed a website which aims at helping practitioners run existing codes on current hardware and software generations, with build instructions for 151 codes we found online when we could run them. Contrary to existing replicability stamps, our replicability scores are non-binary but on a 1-to-5 integer scale, less strict in the sense that results sufficiently close those shown in the paper on different data were deemed appropriate, but sometimes inconsistent with these stamps when software could not be run anymore on current hardware and software generations. In the future, we hope to see interactions with these replicability stamp initiatives for which we share the common goal of spreading open research."
      ]
    }
  },
  {
    "paperId": "4405d420721847d2adf2e0fc36f1cdfe7a286f20",
    "title": "3D Modeling and Computer Graphics in Virtual Reality",
    "sections": {
      "Introduction": [
        "In the past few decades, virtual reality (VR) has been widely used in many different areas including entertainment, education and training, manufacturing, medical and rehabilitation. The compound annual growth rate for VR revenue is expected to grow more than fifty percent from 2018 to 2023. It is expected that education and training is one of the leading sectors in the coming 5 years  [1] . VR not only provides immersive stereoscopic visualization of virtual environments and sound effects, but participants can also interact with virtual objects and environment with haptic feedback. No matter what kind of application to be applied by the VR, the visualization effect and computer graphics are critical to enhance the engagement of participants and thus increases the education and training effectiveness  [2] . Nevertheless, increasing the visual realism in VR is not an easy task because it is not only due to artist's sense of the design engineers but also due to the drawback between the realistic VR environment and the demanding computation requirement of real-time interaction in VR.",
        "3D modeling and computer graphics techniques have been developed for several decades  [3] . Due to the era of digital information technologies, 3D modeling and computer graphics techniques drive the explosive growth and becoming crucially important in the recent years. The techniques not only apply to the development of virtual models for computer simulation, virtual reality (VR), augmented reality (AR), mixed reality (MR), etc., but also it can be applied to many various application such as artificial intelligence (AI), big data analytics, etc.  [4] . Despite VR technologies have been developed for many years, the development of computer hardware and the 5th generation (5G) mobile network bloomed the 5Vs of the data flow including volume, velocity, value, veracity and variety  [5] . As a consequence, the computation requirements and the flow of big data in VR is very demanding not only due to the need for real-time interaction, wireless connection, data interexchange, but also due to the greater expectations in computer graphical effects, realistic 3D models and infectant of virtual environments.",
        "We would like to organize this book chapter as following sections. In Section 2, we aim to review the major software in 3D modeling and rendering in computer graphics. We will present the key computer modeling, computer graphics and VR programming software and tools. The techniques in computer modeling and graphics are particularly important for real-time and realistic interaction in VR. Therefore, in Section 3, we will describe some of the key modeling techniques used in VR. These techniques include shading and mesh editing modifiers. We will compare the difference of these techniques and their visual effects."
      ],
      "Conclusion": [
        "In this book chapter, we have reviewed the recent exiting 3D modeling and texture painting software packages and the difficulties in handling the software.",
        "The key techniques used in the creation of 3D models for VR are also described. The techniques including the shading and mesh editing modifiers not only help reducing the mesh size of the 3D models but also maintaining the visual realism of the models. It is particularly important to meet the demanding computation requirement of real-time interaction in VR program. Results have also shown that bevel modifiers with a few segments can enhance the visual effects compare with the loop cut modifier. However, this feature will change the mesh size of the model. The smooth shading modifiers not only maintain the complexity of the models but also enhanced the visual realism significantly. The mesh editing and shading modifiers can also be applied based on the requirement of the models in VR program."
      ]
    }
  },
  {
    "paperId": "908dad62c0e43d80e3e3cb3c0402f7c71c70499c",
    "title": "MemGPT: Towards LLMs as Operating Systems",
    "sections": {
      "Introduction": [
        "In recent years, large language models (LLMs) and their underlying transformer architecture  (Vaswani et al., 2017; Devlin et al., 2018; Brown et al., 2020; Ouyang et al., 2022)  have become the cornerstone of conversational AI and have led to a wide array of consumer and enterprise applications. Despite these advances, the limited fixed-length context windows used by LLMs significantly hinders their applicability to long conversations or reasoning about long documents. For example, the most widely used open-source 1 University of California, Berkeley. Correspondence to: Charles Packer <cpacker@berkeley.edu>.",
        "LLMs can only support a few dozen back-and-forth messages or reason about a short document before exceeding their maximum input length  (Touvron et al., 2023) .",
        "Directly extending the context length of transformers incurs a quadratic increase in computational time and memory cost due to the transformer architecture's self-attention mechanism, making the design of new long-context architectures a pressing research challenge  (Dai et al., 2019; Kitaev et al., 2020; Beltagy et al., 2020) . While developing longer models is an active area of research  (Dong et al., 2023) , even if we could overcome the computational challenges of context scaling, recent research shows that longcontext models struggle to utilize additional context effectively  (Liu et al., 2023a) . As consequence, given the considerable resources needed to train state-of-the-art LLMs and diminishing returns of context scaling, there is a critical need for alternative techniques to support long context.",
        "In this paper, we study how to provide the illusion of an infinite context while continuing to use fixed-context models. Our approach borrows from the idea of virtual memory paging that was developed to enable applications to work on datasets that far exceed the available memory by paging data between main memory and disk. We leverage the recent progress in function calling abilities of LLM agents  (Schick et al., 2023; Liu et al., 2023b)  to design MemGPT, an OS-inspired LLM system for virtual context management. Using function calls, LLM agents can read and write to external data sources, modify their own context, and choose when to return responses to the user. These capabilities allow LLMs to effective \"page\" in and out information between context windows (analogous to \"main memory\" in operating systems) and external storage, similar to hierarchical memory in traditional OSes. In addition, function calls can be leveraged to manage control flow between context management, response generation, and user interactions. This allows for an agent to choose to iteratively modify what is in its context for a single task, thereby more effectively utilizing its limited context.",
        "In MemGPT, we treat context windows as a constrained memory resource, and design a memory hiearchy for LLMs analogous to memory tiers used in traditional OSes  (Patterson et al., 1988) . Applications in traditional OSes interact Did you go with James? It's so cute how both met there! recall_storage.search(\"six flags\")",
        "Figure  1 .  MemGPT (left)  writes data to persistent memory after it receives a system alert about limited context space.",
        "with virtual memory, which provides an illusion of there being more memory resources than are actually available in physical (i.e., main) memory by the OS paging overflow data to disk and retrieving data (via a page fault) back into memory when accessed by applications. To provide a similar illusion of longer context length (analogous to virtual memory), we allow the LLM to manage what is placed in its own context (analogous to physical memory) via an 'LLM OS', which we call MemGPT. MemGPT enables the LLM to retrieve relevant historical data missing from what is placed in-context, and also evict less relevant data from context and into external storage systems. Figure  3  illustrates the components of MemGPT.",
        "The combined use of a memory-hierarchy, OS functions and event-based control flow allow MemGPT to handle unbounded context using LLMs that have finite context windows. To demonstrate the utility of our new OSinspired LLM system, we evaluate MemGPT on two domains where the performance of existing LLMs is severely limited by finite context: document analysis, where the length of standard text files can quickly exceed the input capacity of modern LLMs, and conversational agents, where LLMs bound by limited conversation windows lack context awareness, persona consistency, and long-term memory during extended conversations. In both settings, MemGPT is able to overcome the limitations of finite context to outperform existing LLM-based approaches."
      ],
      "Results": [
        "We assess MemGPT in two long-context domains: conversational agents and document analysis. For conversational agents, we expand the existing Multi-Session Chat dataset  (Xu et al., 2021)  and introduce two new dialogue tasks that evaluate an agent's ability to retain knowledge working_context.replace( \"Boyfriend named James\", \"Ex-boyfriend named James\" ) Sorry to hear that -hope you're OK 💔 actually james and i broke up How's James doing? Any special plans today? across long conversations. For document analysis, we benchmark MemGPT on existing tasks from  (Liu et al., 2023a)  for question answering and key-value retrieval over lengthy documents. We also propose a new nested keyvalue retrieval task requiring collating information across multiple data sources, which tests the ability of an agent to collate information from multiple data sources (multihop retrieval). We publicly release our augmented MSC dataset, nested KV retrieval dataset, and a dataset of embeddings for 20M Wikipedia articles to facilitate future research. Our code for the benchmarks is available at https://research.memgpt.ai.",
        "Implementation details. When discussing OpenAI models, unless otherwise specified 'GPT-4 Turbo' refers to the specific gpt-4-1106-preview model endpoint (context window of 128, 000), 'GPT-4' refers to gpt-4-0613 (context window of 8, 192), and 'GPT-3.5 Turbo' refers to gpt-3.5-turbo-1106 (context window of 16, 385).",
        "In experiments, we run MemGPT with all baseline models (GPT-4, GPT-4 Turbo, and GPT 3.5) to show how the underlying model performance affects MemGPT's."
      ],
      "Discussion": [
        "Document analysis also faces challenges due to the limited context windows of today's transformer models. As shown in Table  1 , both open and closed source models suffer from constrained context length (up to 128k tokens for OpenAI's models). However many documents easily surpass these lengths; for example, legal or financial documents such as Annual Reports (SEC Form 10-K) can easily pass the million token mark. Moreover, many real document analysis tasks require drawing connections across multiple such lengthy documents. Anticipating these scenarios, it becomes difficult to envision blindly scaling up context as a solution to the fixed-context problem. Recent research  (Liu et al., 2023a)  also raises doubts about the utility of simply scaling contexts, since they find uneven attention distributions in large context models (the model is more capable of recalling information at the beginning or end of its context window, vs tokens in the middle). To enable reasoning across documents, more flexible memory architectures like MemGPT are needed."
      ],
      "Related Work": [
        "Long-context LLMs. Several lines of work have improved the context length of LLMs. For instance, more efficient transformer architectures via sparsifying the attention  (Child et al., 2019; Beltagy et al., 2020) , low-rank approximations  (Wang et al., 2020) , and neural memory  (Lee et al., 2019) . Another line of work aims to extend context windows beyond the length they were original trained for, their training size, such as  Press et al. (2021) ;  Chen et al. (2023) . MemGPT builds upon these improvements in context length as they improve the size of the main memory in MemGPT. Our main contribution is a hierarchical tiered memory that uses a long-context LLM as the implementation of main memory.",
        "Retrieval-Augmented Models. The design of the external memory of MemGPT builds upon much prior work augmenting LLMs with relevant inputs from external retrievers  (Ram et al., 2023; Borgeaud et al., 2022; Karpukhin et al., 2020; Lewis et al., 2020; Guu et al., 2020; Lin et al., 2023)"
      ],
      "Conclusion": [
        "In this paper, we introduced MemGPT, a novel LLM system inspired by operating systems to manage the limited context windows of large language models. By designing a memory hierarchy and control flow analogous to traditional OSes, MemGPT provides the illusion of larger context resources for LLMs. This OS-inspired approach was evaluated in two domains where existing LLM performance is constrained by finite context lengths: document analysis and conversational agents. For document analysis, MemGPT could process lengthy texts well beyond the context limits of current LLMs by effectively paging relevant context in and out of memory. For conversational agents, MemGPT enabled maintaining long-term memory, consistency, and evolvability over extended dialogues. Overall, MemGPT demonstrates that operating system techniques like hierarchical memory management and interrupts can unlock the potential of LLMs even when constrained by fixed context lengths. This work opens numerous avenues for future exploration, including applying MemGPT to other domains with massive or unbounded contexts, integrating different memory tier technologies like databases or caches, and further improving control flow and memory management policies. By bridging concepts from OS architecture into AI systems, MemGPT represents a promising new direction for maximizing the capabilities of LLMs within their fundamental limits.",
        "6. Appendix"
      ]
    }
  },
  {
    "paperId": "530a059cb48477ad1e3d4f8f4b153274c8997332",
    "title": "Explainable Artificial Intelligence (XAI): Concepts, Taxonomies, Opportunities and Challenges toward Responsible AI",
    "sections": {
      "Introduction": [
        "Artificial Intelligence (AI) lies at the core of many activity sectors that have embraced new information technologies  [1] . While the roots of AI trace back to several decades ago, there is a clear consensus on the paramount importance featured nowadays by intelligent machines endowed with learning, reasoning and adaptation capabilities. It is by virtue of these capabilities that AI methods are achieving unprecedented levels of performance when learning to solve increasingly complex computational tasks, making them pivotal for the future development of the human society  [2] . The sophistication of AI-powered systems has lately increased to such an extent that almost no human intervention is required for their design and deployment. When decisions derived from such systems ultimately affect humans' lives (as in e.g. medicine, law or defense), there is an emerging need for understanding how such decisions are furnished by AI methods  [3] .",
        "While the very first AI systems were easily interpretable, the last years have witnessed the rise of opaque decision systems such as Deep Neural Networks (DNNs). The empirical success of Deep Learning (DL) models such as DNNs stems from a combination of efficient learning algorithms and their huge parametric space. The latter space comprises hundreds of layers and millions of parameters, which makes DNNs be considered as complex black-box models  [4] . The opposite of black-box-ness is transparency, i.e., the search for a direct understanding of the mechanism by which a model works  [5] .",
        "As black-box Machine Learning (ML) models are increasingly being employed to make important predictions in critical contexts, the demand for transparency is increasing from the various stakeholders in AI  [6] . The danger is on creating and using decisions that are not justifiable, legitimate, or that simply do not allow obtaining detailed explanations of their behaviour  [7] . Explanations supporting the output of a model are crucial, e.g., in precision medicine, where experts require far more information from the model than a simple binary prediction for supporting their diagnosis  [8] . Other examples include autonomous vehicles in transportation, security, and finance, among others.",
        "In general, humans are reticent to adopt techniques that are not directly interpretable, tractable and trustworthy  [9] , given the increasing demand for ethical AI  [3] . It is customary to think that by focusing solely on performance, the systems will be increasingly opaque. This is true in the sense that there is a trade-off between the performance of a model and its transparency  [10] . However, an improvement in the understanding of a system can lead to the correction of its deficiencies. When developing a ML model, the consideration of interpretability as an additional design driver can improve its implementability for 3 reasons:",
        "• Interpretability helps ensure impartiality in decision-making, i.e. to detect, and consequently, correct from bias in the training dataset.",
        "• Interpretability facilitates the provision of robustness by highlighting potential adversarial perturbations that could change the prediction.",
        "• Interpretability can act as an insurance that only meaningful variables infer the output, i.e., guaranteeing that an underlying truthful causality exists in the model reasoning.",
        "All these means that the interpretation of the system should, in order to be considered practical, provide either an understanding of the model mechanisms and predictions, a visualization of the model's discrimination rules, or hints on what could perturb the model  [11] .",
        "In order to avoid limiting the effectiveness of the current generation of AI systems, eXplainable AI (XAI)  [7]  proposes creating a suite of ML techniques that 1) produce more explainable models while maintaining a high level of learning performance (e.g., prediction accuracy), and 2) enable humans to understand, appropriately trust, and effectively manage the emerging generation of artificially intelligent partners. XAI draws as well insights from the Social Sciences  [12]  and considers the psychology of explanation.    1 : Evolution of the number of total publications whose title, abstract and/or keywords refer to the field of XAI during the last years. Data retrieved from Scopus R (December 10th, 2019) by using the search terms indicated in the legend when querying this database. It is interesting to note the latent need for interpretable AI models over time (which conforms to intuition, as interpretability is a requirement in many scenarios), yet it has not been until 2017 when the interest in techniques to explain AI models has permeated throughout the research community.",
        "Figure  1  displays the rising trend of contributions on XAI and related concepts. This literature outbreak shares its rationale with the research agendas of national governments and agencies. Although some recent surveys  [8, 13, 10, 14, 15, 16, 17]  summarize the upsurge of activity in XAI across sectors and disciplines, this overview aims to cover the creation of a complete unified framework of categories and concepts that allow for scrutiny and understanding of the field of XAI methods. Furthermore, we pose intriguing thoughts around the explainability of AI models in data fusion contexts with regards to data privacy and model confidentiality. This, along with other research opportunities and challenges identified throughout our study, serve as the pull factor toward Responsible Artificial Intelligence, term by which we refer to a series of AI principles to be necessarily met when deploying AI in real applications. As we will later show in detail, model explainability is among the most crucial aspects to be ensured within this methodological framework. All in all, the novel contributions of this overview can be summarized as follows:",
        "1. Grounded on a first elaboration of concepts and terms used in XAI-related research, we propose a novel definition of explainability that places audience (Figure  2 ) as a key aspect to be considered when explaining a ML model. We also elaborate on the diverse purposes sought when using XAI techniques, from trustworthiness to privacy awareness, which round up the claimed importance of purpose and targeted audience in model explainability.",
        "2. We define and examine the different levels of transparency that a ML model can feature by itself, as well as the diverse approaches to post-hoc explainability, namely, the explanation of ML models that are not transparent by design."
      ],
      "Methods": [
        "The use of background knowledge in the form of logical statements or constraints in Knowledge Bases (KBs) has shown to not only improve explainability but also performance with respect to purely data-driven approaches  [297, 298, 299] . A positive side effect shown is that this hybrid approach provides robustness to the learning system when errors are present in the training data labels. Other approaches have shown to be able to jointly learn and reason with both symbolic and sub-symbolic representations and inference. The interesting aspect is that this blend allows for expressive probabilistic-logical reasoning in an end-to-end fashion  [300] . A successful use case is on dietary recommendations, where explanations are extracted from the reasoning behind (non-deep but KB-based) models  [301] .",
        "Future data fusion approaches may thus consider endowing DL models with explainability by externalizing other domain information sources. Deep formulation of classical ML models has been done, e.g. in Deep Kalman filters (DKFs)  [302] , Deep Variational Bayes Filters (DVBFs)  [303] , Structural Variational Autoencoders (SVAE)  [304] , or conditional random fields as RNNs  [305] . These approaches provide deep models with the interpretability inherent to probabilistic graphical models. For instance, SVAE combines probabilistic graphical models in the embedding space with neural networks to enhance the interpretability of DKFs. A particular example of classical ML model enhanced with its DL counterpart is Deep Nearest Neighbors DkNN  [264] , where the neighbors constitute human-interpretable explanations of predictions. The intuition is based on the rationalization of a DNN prediction based on evidence. This evidence consists of a characterization of confidence termed credibility that spans the hierarchy of representations within a DNN, that must be supported by the training data  [264] .",
        "x y",
        "Transparent design methods"
      ],
      "Conclusion": [
        "This overview has revolved around eXplainable Artificial Intelligence (XAI), which has been identified in recent times as an utmost need for the adoption of ML methods in real-life applications. Our study Fundación BBVA a Equipos de Investigación Científica 2018 call (DeepSCOP project). This work was also funded in part by the European Union's Horizon 2020 research and innovation programme AI4EU under grant agreement 825619. We also thank Chris Olah, Alexander Mordvintsev and Ludwig Schubert for borrowing images for illustration purposes. Part of this overview is inspired by a preliminary work of the concept of Responsible AI: R. Benjamins, A. Barbado, D. Sierra, \"Responsible AI by Design\", to appear in the Proceedings of the Human-Centered AI: Trustworthiness of AI Models & Data (HAI) track at AAAI Fall Symposium, DC, November 7-9, 2019  [386] ."
      ]
    }
  },
  {
    "paperId": "5cde474869cb230a29b3ba0f6f685f5162b1a1a1",
    "title": "Revolutionizing healthcare: the role of artificial intelligence in clinical practice",
    "sections": {
      "Introduction": [
        "Artificial Intelligence (AI) is a rapidly evolving field of computer science that aims to create machines that can perform tasks that typically require human intelligence. AI includes various techniques such as machine learning (ML), deep learning (DL), and natural language processing (NLP). Large Language Models (LLMs) are a type of AI algorithm that uses deep learning techniques and massively large data sets to understand, summarize, generate, and predict new text-based content  [1] [2] [3] . LLMs have been architected to generate text-based content and possess broad applicability for various NLP tasks, including text generation, translation, content summary, rewriting, classification, categorization, and sentiment analysis. NLP is a subfield of AI that focuses on the interaction between computers and humans through natural language, including understanding, interpreting, and generating human language. NLP involves various techniques such as text mining, sentiment analysis, speech recognition, and machine translation. Over the years, AI has undergone significant transformations, from the early days of rule-based systems to the current era of ML and deep learning algorithms  [1] [2] [3] .",
        "AI has evolved since the first AI program was developed in 1951 by Christopher Strachey. At that time, AI was in its infancy and was primarily an academic research topic. In 1956, John McCarthy organized the Dartmouth Conference, where he coined the term \"Artificial Intelligence. \" This event marked the beginning of the modern AI era. In the 1960 and 1970 s, AI research focused on rule-based and expert systems. However, this approach was limited by the need for more computing power and data  [4] .",
        "In the 1980 and 1990 s, AI research shifted to ML and neural networks, which allowed machines to learn from data and improve their performance over time. This period saw the development of systems such as IBM's Deep Blue, which defeated world chess champion Garry Kasparov in 1997. In the 2000s, AI research continued to evolve, focusing on NLP and computer vision, which led to the development of virtual assistants, such as Apple's Siri and Amazon's Alexa, which could understand natural language and respond to user requests (Fig.  1 )  [3, 4] .",
        "Today, AI is transforming healthcare, finance, and transportation, among other fields, and its impact is only set to grow. In academia, AI has been used to develop intelligent tutoring systems, which are computer programs that can adapt to the needs of individual students. These systems have improved student learning outcomes in various subjects, including math and science. In research, AI has been used to analyze large datasets and identify patterns that would be difficult for humans Keywords AI, Healthcare, Patient care, Quality of life, Clinicians, Decision-making, Personalized treatment plans Fig.  1  Tracing the Evolution of AI with a Better Understanding of the Relationship Between AI, ML, DL, and NLP to detect; this has led to breakthroughs in fields such as genomics and drug discovery. AI has been used in healthcare settings to develop diagnostic tools and personalized treatment plans. As AI continues to evolve, it is crucial to ensure that it is developed responsibly and for the benefit of all  [5] [6] [7] [8] .",
        "The rapid progression of AI technology presents an opportunity for its application in clinical practice, potentially revolutionizing healthcare services. It is imperative to document and disseminate information regarding AI's role in clinical practice, to equip healthcare providers with the knowledge and tools necessary for effective implementation in patient care. This review article aims to explore the current state of AI in healthcare, its potential benefits, limitations, and challenges, and to provide insights into its future development. By doing so, this review aims to contribute to a better understanding of AI's role in healthcare and facilitate its integration into clinical practice."
      ],
      "Conclusion": [
        "The integration of AI in healthcare has immense potential to revolutionize patient care and outcomes. AI-driven predictive analytics can enhance the accuracy, efficiency, and cost-effectiveness of disease diagnosis and clinical laboratory testing. Additionally, AI can aid in population health management and guideline establishment, providing real-time, accurate information and optimizing medication choices. Integrating AI in virtual health and mental health support has shown promise in improving patient care. However, it is important to address limitations such as bias and lack of personalization to ensure equitable and effective use of AI.",
        "Several measures must be taken to ensure responsible and effective implementation of AI in healthcare.",
        "Firstly, comprehensive cybersecurity strategies and robust security measures should be developed and implemented to protect patient data and critical healthcare operations. Collaboration between healthcare organizations, AI researchers, and regulatory bodies is crucial to establishing guidelines and standards for AI algorithms and their use in clinical decision-making. Investment in research and development is also necessary to advance AI technologies tailored to address healthcare challenges.",
        "AI algorithms can continuously examine factors such as population demographics, disease prevalence, and geographical distribution. This can identify patients at a higher risk of certain conditions, aiding in prevention or treatment. Edge analytics can also detect irregularities and predict potential healthcare events, ensuring that resources like vaccines are available where most needed.",
        "Public perception of AI in healthcare varies, with individuals expressing willingness to use AI for health purposes while still preferring human practitioners in complex issues. Trust-building and patient education are crucial for the successful integration of AI in healthcare practice. Overcoming challenges like data quality, privacy, bias, and the need for human expertise is essential for responsible and effective AI integration.",
        "Collaboration among stakeholders is vital for robust AI systems, ethical guidelines, and patient and provider trust. Continued research, innovation, and interdisciplinary collaboration are important to unlock the full potential of AI in healthcare. With successful integration, AI is anticipated to revolutionize healthcare, leading to improved patient outcomes, enhanced efficiency, and better access to personalized treatment and quality care."
      ]
    }
  },
  {
    "paperId": "7b72711ac2ea7bd7f519cac162a4a6578bbb7d0d",
    "title": "ARTIFICIAL INTELLIGENCE FOR THE REAL WORLD",
    "sections": {
      "Introduction": [
        "Artificial intelligence (AI) is defined as the ability of an artificial entity to solve complicated problems using its own intelligence. Computer science and physiology are combined in Artificial Intelligence. In layman's terms, intelligence is the computational component of one's capacity to attain goals in the real world. Intelligence is defined as the capacity to think, envision, memorize, and comprehend, see patterns, make decisions, adapt to change, and learn from experience. Artificial intelligence is focused with making computers behave more humanlike and in a fraction of the time it takes a person to do it. As a result, it is known as Artificial Intelligence. Artificial intelligence is also concerned with pushing the boundaries of practical computer science in the direction of systems that are adaptable, flexible, and capable of forming their own analyses and solution techniques by applying general knowledge to specific situations."
      ]
    }
  },
  {
    "paperId": "9e8b7b0d4c628c12b6a65ab56ac5f33a35eff2e6",
    "title": "Unifying Large Language Models and Knowledge Graphs: A Roadmap",
    "sections": {
      "Introduction": [
        "Large language models (LLMs) 1 (e.g., BERT  [1] , RoBERTA  [2] , and T5  [3] ), pre-trained on the large-scale corpus, have shown great performance in various natural language processing (NLP) tasks, such as question answering  [4] , machine translation  [5] , and text generation  [6] . Recently, the dramatically increasing model size further enables the LLMs with the emergent ability  [7] , paving the road for applying LLMs as Artificial General Intelligence (AGI). Advanced LLMs like ChatGPT 2 and PaLM2 3 , with billions of parameters, exhibit great potential in many complex practical tasks, such as education  [8] , code generation  [9]  and recommendation  [10] .",
        "• Shirui Pan is with the School of Information and Communication Technology and Institute for Integrated and Intelligent Systems (IIIS), Griffith University, Queensland, Australia. Email: s.pan@griffith.edu.au; • Linhao Luo and Yufei Wang are with the Department of Data Science and AI, Monash University, Melbourne, Australia. E-mail: linhao.luo@monash.edu, garyyufei@gmail.com. • Chen Chen is with the Nanyang Technological University, Singapore. Email: s190009@ntu.edu.sg. • Jiapu Wang is with the Faculty of Information Technology, Beijing University of Technology, Beijing, China. E-mail: jpwang@emails.bjut.edu.cn. • Xindong Wu is with the Key Laboratory of Knowledge Engineering with Big Data (the Ministry of Education of China), Hefei University of Technology, Hefei, China, and also with the Research Center for Knowledge Engineering, Zhejiang Lab, Hangzhou, China. Email: xwu@hfut.edu.cn. • Shirui Pan and Linhao Luo contributed equally to this work.",
        "• Corresponding Author: Xindong Wu.",
        "1. LLMs are also known as pre-trained language models (PLMs). 2. https://openai.com/blog/chatgpt 3. https://ai.google/discover/palm2 Fig.  1 . Summarization of the pros and cons for LLMs and KGs. LLM pros: General Knowledge  [11] , Language Processing  [12] , Generalizability  [13] ; LLM cons: Implicit Knowledge  [14] , Hallucination  [15] , Indecisiveness  [16] , Black-box  [17] , Lacking Domain-specific/New Knowledge  [18] . KG pros: Structural Knowledge  [19] , Accuracy  [20] , Decisiveness  [21] , Interpretability  [22] , Domain-specific Knowledge  [23] , Evolving Knowledge  [24] ; KG cons: Incompleteness  [25] , Lacking Language Understanding  [26] , Unseen Facts  [27] . Pros. and Cons. are selected based on their representativeness. Detailed discussion can be found in Appendix A.",
        "Despite their success in many applications, LLMs have been criticized for their lack of factual knowledge. Specifically, LLMs memorize facts and knowledge contained in the training corpus  [14] . However, further studies reveal that LLMs are not able to recall facts and often experience hallucinations by generating statements that are factually incorrect  [15] ,  [28] . For example, LLMs might say \"Einstein discovered gravity in 1687\" when asked, \"When did Einstein discover gravity?\", which contradicts the fact that Isaac Newton formulated the gravitational theory. This issue severely impairs the trustworthiness of LLMs.",
        "As black-box models, LLMs are also criticized for their lack of interpretability. LLMs represent knowledge implicitly in their parameters. It is difficult to interpret or validate the knowledge obtained by LLMs. Moreover, LLMs perform reasoning by a probability model, which is an indecisive process  [16] . The specific patterns and functions LLMs used to arrive at predictions or decisions are not directly accessible or explainable to humans  [17] . Even though some LLMs are equipped to explain their predictions by applying chain-of-thought  [29] , their reasoning explanations also suffer from the hallucination issue  [30] . This severely impairs the application of LLMs in high-stakes scenarios, such as medical diagnosis and legal judgment. For instance, in a medical diagnosis scenario, LLMs may incorrectly diagnose a disease and provide explanations that contradict medical commonsense. This raises another issue that LLMs trained on general corpus might not be able to generalize well to specific domains or new knowledge due to the lack of domain-specific knowledge or new training data  [18] .",
        "To address the above issues, a potential solution is to incorporate knowledge graphs (KGs) into LLMs. Knowledge graphs (KGs), storing enormous facts in the way of triples, i.e., (head entity, relation, tail entity), are a structured and decisive manner of knowledge representation (e.g., Wikidata  [20] , YAGO  [31] , and NELL  [32] ). KGs are crucial for various applications as they offer accurate explicit knowledge  [19] . Besides, they are renowned for their symbolic reasoning ability  [22] , which generates interpretable results. KGs can also actively evolve with new knowledge continuously added in  [24] . Additionally, experts can construct domain-specific KGs to provide precise and dependable domain-specific knowledge  [23] .",
        "Nevertheless, KGs are difficult to construct  [25] , and current approaches in KGs  [27] ,  [33] ,  [34]  are inadequate in handling the incomplete and dynamically changing nature of real-world KGs. These approaches fail to effectively model unseen entities and represent new facts. In addition, they often ignore the abundant textual information in KGs. Moreover, existing methods in KGs are often customized for specific KGs or tasks, which are not generalizable enough. Therefore, it is also necessary to utilize LLMs to address the challenges faced in KGs. We summarize the pros and cons of LLMs and KGs in Fig.  1 , respectively.",
        "Recently, the possibility of unifying LLMs with KGs has attracted increasing attention from researchers and practitioners. LLMs and KGs are inherently interconnected and can mutually enhance each other. In KG-enhanced LLMs, KGs can not only be incorporated into the pre-training and inference stages of LLMs to provide external knowledge  [35] -  [37] , but also used for analyzing LLMs and providing interpretability  [14] ,  [38] ,  [39] . In LLM-augmented KGs, LLMs have been used in various KG-related tasks, e.g., KG embedding  [40] , KG completion  [26] , KG construction  [41] , KG-to-text generation  [42] , and KGQA  [43] , to improve the performance and facilitate the application of KGs. In Synergized LLM + KG, researchers marries the merits of LLMs and KGs to mutually enhance performance in knowledge representation  [44]  and reasoning  [45] ,  [46] . Although there are some surveys on knowledge-enhanced LLMs  [47] -  [49] , which mainly focus on using KGs as an external knowledge to enhance LLMs, they ignore other possibilities of integrating KGs for LLMs and the potential role of LLMs in KG applications.",
        "In this article, we present a forward-looking roadmap for unifying both LLMs and KGs, to leverage their respective strengths and overcome the limitations of each approach, for various downstream tasks. We propose detailed categorization, conduct comprehensive reviews, and pinpoint emerging directions in these fast-growing fields. Our main contributions are summarized as follows:",
        "1) Roadmap. We present a forward-looking roadmap for integrating LLMs and KGs. Our roadmap, consisting of three general frameworks to unify LLMs and KGs, namely, KG-enhanced LLMs, LLMaugmented KGs, and Synergized LLMs + KGs, provides guidelines for the unification of these two distinct but complementary technologies. 2) Categorization and review. For each integration framework of our roadmap, we present a detailed categorization and novel taxonomies of research on unifying LLMs and KGs. In each category, we review the research from the perspectives of different integration strategies and tasks, which provides more insights into each framework. 3) Coverage of emerging advances. We cover the advanced techniques in both LLMs and KGs. We include the discussion of state-of-the-art LLMs like ChatGPT and GPT-4 as well as the novel KGs e.g., multi-modal knowledge graphs. 4) Summary of challenges and future directions. We highlight the challenges in existing research and present several promising future research directions.",
        "The rest of this article is organized as follows. Section 2 first explains the background of LLMs and KGs. Section 3 introduces the roadmap and the overall categorization of this article. Section 4 presents the different KGs-enhanced LLM approaches. Section 5 describes the possible LLMaugmented KG methods. Section 6 shows the approaches of synergizing LLMs and KGs. Section 7 discusses the challenges and future research directions. Finally, Section 8 concludes this paper.",
        "In this section, we will first briefly introduce a few representative large language models (LLMs) and discuss the prompt engineering that efficiently uses LLMs for varieties of applications. Then, we illustrate the concept of knowledge graphs (KGs) and present different categories of KGs."
      ],
      "Discussion": [
        "Knowledge graphs (KGs) for pre-train language models (LLMs) analysis aims to answer the following questions such as \"how do LLMs generate the results?\", and \"how do the function and structure work in LLMs?\". To analyze the inference process of LLMs, as shown in Fig.  13 , KagNet  [38]  and QA-GNN  [131]  make the results generated by LLMs at each reasoning step grounded by knowledge graphs. In this way, the reasoning process of LLMs can be explained by extracting the graph structure from KGs. Shaobo et al.  [123]  investigate how LLMs generate the results correctly. They adopt the causal-inspired analysis from facts extracted from KGs. This analysis quantitatively measures the word patterns that LLMs depend on to generate the results. The results show that LLMs generate the missing factual more by the positionally closed words rather than the knowledgedependent words. Thus, they claim that LLMs are inadequate to memorize factual knowledge because of the inaccurate dependence. To interpret the training of LLMs, Swamy   [40]  2020 E LLMs as Text Encoders Nayyeri et al.  [132]  2022 E LLMs as Text Encoders Huang et al.  [133]  2022 E LLMs as Text Encoders CoDEx  [134]  2022 E LLMs as Text Encoders LMKE  [135]  2022 E LLMs for Joint Text and KG Embedding kNN-KGE  [136]  2022 E LLMs for Joint Text and KG Embedding LambdaKG  [137]  2023 E + D + ED LLMs for Joint Text and KG Embedding LLM-augmented KG completion KG-BERT  [26]  2019 E Joint Encoding MTL-KGC  [138]  2020 E Joint Encoding PKGC  [139]  2022 E Joint Encoding LASS  [140]  2022 E Joint Encoding MEM-KGC  [141]  2021 E MLM Encoding OpenWorld KGC  [142]  2023 E MLM Encoding",
        "StAR  [143]  2021 E Separated Encoding SimKGC  [144]  2022 E Separated Encoding LP-BERT  [145]  2022 E Separated Encoding",
        "GenKGC  [96]  2022 ED LLM as decoders KGT5  [146]  2022 ED LLM as decoders KG-S2S  [147]  2022 ED LLM as decoders AutoKG  [93]  2023 D LLM as decoders LLM-augmented KG construction ELMO  [148]  2018 E Named Entity Recognition GenerativeNER  [149]  2021 ED Named Entity Recognition LDET  [150]  2019 E Entity Typing BOX4Types  [151]  2021 E Entity Typing ELQ  [152]  2020 E Entity Linking ReFinED  [153]  2022 E Entity Linking",
        "BertCR  [154]  2019 E CR (Within-document) Spanbert  [155]  2020 E CR (Within-document) CDLM  [156]  2021 E CR (Cross-document) CrossCR  [157]  2021 E CR (Cross-document) CR-RL  [158]  2021 E CR (Cross-document)",
        "SentRE  [159]  2019 E RE (Sentence-level) Curriculum-RE  [160]  2021 E RE (Sentence-level) DREEAM  [161]  2023 E RE (Document-level) Kumar et al.  [95]  2020 E End-to-End Construction Guo et al.  [162]  2021 E End-to-End Construction Grapher  [41]  2021 ED End-to-End Construction PiVE  [163]  2023 D + ED End-to-End Construction COMET  [164]  2019 D Distilling KGs from LLMs BertNet  [165]  2022 E Distilling KGs from LLMs West et al.  [166]  2022 D Distilling KGs from LLMs LLM-augmented KG-to-text Generation Ribeiro et al  [167]  2021 ED Leveraging Knowledge from LLMs JointGT  [42]  2021 ED Leveraging Knowledge from LLMs FSKG2Text  [168]  2021 D + ED Leveraging Knowledge from LLMs GAP  [169]  2022 ED Leveraging Knowledge from LLMs",
        "GenWiki  [170]  2020 -Constructing KG-text aligned Corpus KGPT  [171]  2020 ED Constructing KG-text aligned Corpus LLM-augmented KGQA Lukovnikov et al.  [172]  2019 E Entity/Relation Extractor Luo et al.  [173]  2020 E Entity/Relation Extractor QA-GNN  [131]  2021 E Entity/Relation Extractor Nan et al.  [174]  2023 E + D + ED Entity/Relation Extractor DEKCOR  [175]  2021 E Answer Reasoner DRLK  [176]  2022 E Answer Reasoner OreoLM  [177]  2022 E Answer Reasoner GreaseLM  [178]  2022 E Answer Reasoner ReLMKG  [179]  2022 E Answer Reasoner UniKGQA  [43]  2023 E Answer Reasoner E: Encoder-only LLMs, D: Decoder-only LLMs, ED: Encoder-decoder LLMs.",
        "et al.  [122]  adopt the language model during pre-training to generate knowledge graphs. The knowledge acquired by LLMs during training can be unveiled by the facts in KGs explicitly. To explore how implicit knowledge is stored in parameters of LLMs, Dai et al.  [39]  propose the concept of knowledge neurons. Specifically, activation of the identified knowledge neurons is highly correlated with knowledge expression. Thus, they explore the knowledge and facts represented by each neuron by suppressing and amplifying knowledge neurons.",
        "Justin et al.  [187]  provide a comprehensive analysis of KGC methods integrated with LLMs. Their research investigates the quality of LLM embeddings and finds that they are suboptimal for effective entity ranking. In response, they propose several techniques for processing embeddings to improve their suitability for candidate retrieval. The study also compares different model selection dimensions, such as Embedding Extraction, Query Entity Extraction, and Language Model Selection. Lastly, the authors propose a framework that effectively adapts LLM for knowledge graph completion."
      ],
      "Conclusion": [
        "Unifying large language models (LLMs) and knowledge graphs (KGs) is an active research direction that has attracted increasing attention from both academia and industry. In this article, we provide a thorough overview of the recent research in this field. We first introduce different manners that integrate KGs to enhance LLMs. Then, we introduce existing methods that apply LLMs for KGs and establish taxonomy based on varieties of KG tasks. Finally, we discuss the challenges and future directions in this field.",
        "We envision that there will be multiple stages (milestones) in the roadmap of unifying KGs and LLMs, as shown in Fig.  26 . In particular, we will anticipate increasing research on three stages: Stage 1: KG-enhanced LLMs, LLM-augmented KGs, Stage 2: Synergized LLMs + KGs, and Stage 3: Graph Structure Understanding, Multi-modality, Knowledge Updating. We hope that this article will provide a guideline to advance future research."
      ]
    }
  },
  {
    "paperId": "3950df97ea527009a32569cb7016bc3df1383dca",
    "title": "QA-GNN: Reasoning with Language Models and Knowledge Graphs for Question Answering",
    "sections": {
      "Introduction": [
        "Question answering systems must be able to access relevant knowledge and reason over it. Typically, knowledge can be implicitly encoded in large language models (LMs) pre-trained on unstructured text  (Petroni et al., 2019; Bosselut et al., 2019) , or explicitly represented in structured knowledge graphs (KGs), such as Freebase  (Bollacker et al., 2008)  and ConceptNet  (Speer et al., 2017) , where entities are represented as nodes and relations between them as edges. Recently, pre-trained LMs have demonstrated remarkable success in many question answering tasks  (Liu et al., 2019; Raffel et al., 2020) . However, while LMs have a broad coverage of knowledge, they do not empirically perform well on structured reasoning (e.g., handling negation)  (Kassner and Schütze, 2020) . On the other hand, KGs are more suited for structured reasoning  (Ren et al., 2020; Ren and Leskovec, 2020)  and enable explainable predictions e.g., by providing reasoning paths  (Lin et al., 2019)   be noisy  (Bordes et al., 2013; Guu et al., 2015) . How to reason effectively with both sources of knowledge remains an important open problem.",
        "Combining LMs and KGs for reasoning (henceforth, LM+KG) presents two challenges: given a QA context (e.g., question and answer choices; Figure  1  purple box), methods need to (i) identify informative knowledge from a large KG (green box); and (ii) capture the nuance of the QA context and the structure of the KGs to perform joint reasoning over these two sources of information. Previous works  (Bao et al., 2016; Sun et al., 2018; Lin et al., 2019)  retrieve a subgraph from the KG by taking topic entities (KG entities mentioned in the given QA context) and their few-hop neighbors. However, this introduces many entity nodes that are semantically irrelevant to the QA context, especially when the number of topic entities or hops increases. Additionally, existing LM+KG methods for reasoning  (Lin et al., 2019; Wang et al., 2019a; Feng et al., 2020; Lv et al., 2020)   Figure  2 : Overview of our approach. Given a QA context (z), we connect it with the retrieved KG to form a joint graph (working graph; §3.1), compute the relevance of each KG node conditioned on z ( §3.2; node shading indicates the relevance score), and perform reasoning on the working graph ( §3.3).",
        "neural networks (GNNs) to the KG, and do not mutually update or unify their representations. This separation might limit their capability to perform structured reasoning, e.g., handling negation.",
        "Here we propose QA-GNN, an end-to-end LM+KG model for question answering that addresses the above two challenges. We first encode the QA context using an LM, and retrieve a KG subgraph following prior works  (Feng et al., 2020) . Our QA-GNN has two key insights: (i) Relevance scoring: Since the KG subgraph consists of all few-hop neighbors of the topic entities, some entity nodes are more relevant than others with respect to the given QA context. We hence propose KG node relevance scoring: we score each entity on the KG subgraph by concatenating the entity with the QA context and calculating the likelihood using a pretrained LM. This presents a general framework to weight information on the KG; (ii) Joint reasoning: We design a joint graph representation of the QA context and KG, where we explicitly view the QA context as an additional node (QA context node) and connect it to the topic entities in the KG subgraph as shown in Figure  1 . This joint graph, which we term the working graph, unifies the two modalities into one graph. We then augment the feature of each node with the relevance score, and design a new attention-based GNN module for reasoning. Our joint reasoning algorithm on the working graph simultaneously updates the representation of both the KG entities and the QA context node, bridging the gap between the two sources of information.",
        "We evaluate QA-GNN on two question answering datasets that require reasoning with knowledge: CommonsenseQA  (Talmor et al., 2019)  and OpenBookQA  (Mihaylov et al., 2018) , using the ConceptNet KG  (Speer et al., 2017) . QA-GNN outperforms strong fine-tuned LM baselines as well as the existing best LM+KG model (with the same LM) by up to 5.7% and 3.7% respectively. In particular, QA-GNN exhibits improved performance on some forms of structured reasoning (e.g., correctly han-dling negation and entity substitution in questions): it achieves 4.6% improvement over fine-tuned LMs on questions with negation, while existing LM+KG models are +0.6% over fine-tuned LMs. We also show that one can extract reasoning processes from QA-GNN in the form of general KG subgraphs, not just paths  (Lin et al., 2019) , suggesting a general method for explaining model predictions."
      ],
      "Methods": [
        "Careful Selection  (Banerjee et al., 2019)  72.0 AristoRoBERTa 77.8 KF + SIR  (Banerjee and Baral, 2020)  80.0 AristoRoBERTa + PG  (Wang et al., 2020b)  80.2 AristoRoBERTa + MHGRN  (Feng et al., 2020)  80.6 Albert + KB 81.0 T5 *  (Raffel et al., 2020)  83.2 UnifiedQA *  (Khashabi et al., 2020)  87.2",
        "AristoRoBERTa + QA-GNN (Ours) 82.8  MHGRN. The boost over MHGRN suggests that QA-GNN makes a better use of KGs to perform joint reasoning than existing LM+KG methods.",
        "We also achieve competitive results to other systems on the official leaderboards (Table  3  and 5 ). Notably, the top two systems, T5  (Raffel et al., 2020)  and UnifiedQA  (Khashabi et al., 2020) , are trained with more data and use 8x to 30x more parameters than our model (ours has ∼360M parameters). Excluding these and ensemble systems, our model is comparable in size and amount of data to other systems, and achieves the top performance on the two datasets."
      ],
      "Related Work": [
        "Knowledge-aware methods for NLP. Various works have studied methods to augment NLP systems with knowledge. Existing works  (Pan et al., 2019; Ye et al., 2019; Petroni et al., 2019; Bosselut et al., 2019 ) study pre-trained LMs' potential as latent knowledge bases. To provide more explicit and interpretable knowledge, several works integrate structured knowledge (KGs) into LMs  (Mihaylov and Frank, 2018; Lin et al., 2019; Wang et al., 2019a; Yang et al., 2019; Wang et al., 2020b; Bosselut et al., 2021) . Other works on scoring or pruning KG nodes/paths rely on graph-based metrics such as PageRank, centrality, and off-the-shelf KG embeddings  (Paul and Frank, 2019; Fadnis et al., 2019; Bauer et al., 2018; Lin et al., 2019) , without reflecting the QA context.",
        "Other QA tasks. Several works study other forms of question answering tasks, e.g., passagebased QA, where systems identify answers using given or retrieved documents  (Rajpurkar et al., 2016; Joshi et al., 2017; Yang et al., 2018) , and KBQA, where systems perform semantic parsing of a given question and execute the parsed queries on knowledge bases  (Berant et al., 2013; Yih et al., 2016; Yu et al., 2018) . Different from these tasks, we approach question answering using knowledge available in LMs and KGs.",
        "Knowledge representations. Several works study joint representations of external textual knowledge (e.g., Wikipedia articles) and structured knowledge (e.g., KGs)  (Riedel et al., 2013; Toutanova et al., 2015; Xiong et al., 2019; Sun et al., 2019; Wang et al., 2019b) . The primary distinction of our joint graph representation is that we construct a graph connecting each question and KG rather than textual and structural knowledge, approaching a complementary problem to the above works.",
        "Graph neural networks (GNNs). GNNs have been shown to be effective for modeling graphbased data. Several works use GNNs to model the structure of text  (Yasunaga et al., 2017; Zhang et al., 2018; Yasunaga and Liang, 2020)  or KGs  (Wang et al., 2020a) . In contrast to these works, QA-GNN jointly models the language and KG. Graph Attention Networks (GATs)  (Veličković et al., 2018)  perform attention-based message passing to induce graph representations. We build on this framework, and further condition the GNN on the language input by introducing a QA context node ( §3.1), KG node relevance scoring ( §3.2), and joint update of the KG and language representations ( §3.3)."
      ],
      "Conclusion": [
        "We presented QA-GNN, an end-to-end question answering model that leverages LMs and KGs.",
        "Our key innovations include (i) Relevance scoring, where we compute the relevance of KG nodes conditioned on the given QA context, and (ii) Joint reasoning over the QA context and KGs, where we connect the two sources of information via the working graph, and jointly update their representations through GNN message passing. Through both quantitative and qualitative analyses, we showed QA-GNN's improvements over existing LM and LM+KG models on question answering tasks, as well as its capability to perform interpretable and structured reasoning, e.g., correctly handling negation in questions."
      ]
    }
  },
  {
    "paperId": "42b323b6df79e49c9bf5cee2a91398a7fa3d594d",
    "title": "Knowledge Graphs: Opportunities and Challenges",
    "sections": {
      "Introduction": [
        "Knowledge plays a vital role in human existence and development. Learning and representing human knowledge are crucial tasks in artificial intelligence (AI) research. While humans are able to understand and analyze their surroundings, AI systems require additional knowledge to obtain the same abilities and solve complex tasks in realistic scenarios  (Ji et al. 2021 ). To support these systems, we have seen the emergence of many approaches for representing human knowledge according to different conceptual models. In the last decade, knowledge graphs have become a standard solution in this space, as well as a research trend in academia and industry  (Kong et al. 2022) .",
        "Knowledge graphs are defined as graphs of data that accumulate and convey knowledge of the real world. The nodes in knowledge graphs represent the entities of interest, and the edges represent the relations between the entities  (Hogan et al. 2021; Cheng et al. 2022a ). These representations utilize formal semantics, which allows computers to process them efficiently and unambiguously. For example, the entity \"Bill Gates\" can be linked to the entity \"Microsoft\" because Bill Gates is the founder of Microsoft; thus, they have relationships in the real world.",
        "Due to the great significance of knowledge graphs in processing heterogeneous information within a machine-readable context, a considerable amount of research has been conducted continuously on these solutions in recent years  (Dai et al. 2020a ). The proposed knowledge graphs are widely employed in various AI systems recently  (Ko et al. 2021; Mohamed et al. 2021) , such as recommender systems, question answering, and information retrieval. They are also widely applied in many fields (e.g., education and medical care) to benefit human life and society  (Sun et al. 2020; Bounhas et al. 2020) .",
        "Therefore, knowledge graphs have seized great opportunities by improving the quality of AI systems and being applied to various areas. However, the research on knowledge graphs still faces significant technical challenges. For example, there are major limitations in the current technologies for acquiring knowledge from multiple sources and integrating them into a typical knowledge graph. Thus, knowledge graphs provide great opportunities in modern society. However, there are technical challenges in their development. Consequently, it is necessary to analyze knowledge graphs with respect to their opportunities and challenges to develop a better understanding of knowledge graphs.",
        "To deeply understand the development of knowledge graphs, this survey extensively analyzes knowledge graphs in terms of their opportunities and challenges. Firstly, we discuss the opportunities of knowledge graphs in terms of two aspects: AI systems whose performance is significantly improved by knowledge graphs and application fields that benefit from knowledge graphs. Then, we analyze the challenges of knowledge graphs by considering the limitations of knowledge graph technologies. The main contributions of this paper are as follows:",
        "• Survey on knowledge graphs: We conduct a comprehensive survey of existing knowledge graph studies. In particular, this work thoroughly analyzes the advancements in knowledge graphs in terms of state-of-the-art technologies and applications. • Knowledge graph opportunities: We investigate potential opportunities for knowledge graphs in terms of knowledge graph-based AI systems and application fields that utilize knowledge graphs. Firstly, we examine the benefits of knowledge graphs for AI systems, including recommender systems, question-answering systems, and information retrieval. Then, we discuss the far-reaching impacts of knowledge graphs on human society by describing current and potential knowledge graph applications in various fields (e.g., education, scientific research, social media, and medical care). • Knowledge graph challenges: We provide deep insights into significant technical challenges facing knowledge graphs. In particular, we elaborate on limitations concerning five representative knowledge graph technologies, including knowledge graph embeddings, knowledge acquisition, knowledge graph completion, knowledge fusion, and knowledge reasoning.",
        "The rest of the paper is organized as follows. Section 2 provides an overview of knowledge graphs, including the definitions and the categorization of existing research on knowledge graphs. To examine the opportunities of knowledge graphs, Section 3 and Section 4 introduce relevant AI systems and application fields, respectively. Section 5 details the challenges of knowledge graphs based on the technologies. Finally, we conclude this paper in Section 6."
      ],
      "Methods": [
        "The core idea of tensor factorization-based methods is transforming the triplets in the knowledge graph into a 3D tensor  (Balažević et al. 2019) . As Fig 5 presents, the tensor X ∈ R m×m×n , where m and n indicate the number of entity and relation, respectively, contains n slices, and each slice corresponds to one relation type. If the condition X ijk = 1 is met, the triplet (e i , r k , e j ) , where e and r denote entity and relation, respectively, exists in the knowledge graph. Otherwise, if X ijk = 0 , there is no such a triplet in the knowledge graph. Then, the tensor is represented by the embedding matrices that consist of the vectors of entities and relations.",
        "Translation-based methods exploit the scoring function, which is based on translation invariance. Translation invariance interprets the distance between the vectors of the two words, which is represented by the vector of their semantic relationships  (Mikolov et al. 2013) .  Bordes et al. (2013)  firstly utilized the translation invariance-based scoring functions to measure the embedding results. They creatively proposed the TransE model, which translates all the entities and relations of a knowledge graph into a continuous and low vector space. Specifically, the vectors of the head and tail entities in a triplet are connected by the vector of their relation. Consequently, in the vector space, the semantic meaning of every triplet is preserved. Formally, given a triplet (head, relation, tail), the embedding vectors of the head entity, relation, and tail entity are h , r , and t , respectively. In the vector space, the plausibility of the triplet (h, r, t) is computed by the translation invariance-based scoring function to ensure it follows the geometric principle: h + r ≈ t.",
        "After TransE, a lot of related extensions, such as TransH  (Wang et al. 2014)  and TransR  (Lin et al. 2015) , are continually proposed to improve the performance of the Translationbased knowledge graph embeddings.",
        "Nowadays, deep learning has become a popular tool that is utilized for knowledge graph embeddings, and a considerable amount of research proposes to employ neural networks to represent the triplets of knowledge graphs  (Dai et al. 2020a) . In this section, we discuss three representative works, including SME, ConvKB, and R-GCN, to briefly introduce neural network-based knowledge graph embeddings. SME  (Bordes et al. 2014 ) designs an energy function to conduct semantic matching, which utilizes neural networks to measure the confidence of each triplet (h, r, t) in knowledge graphs. The scoring function of SME is defined as follows:",
        "The scoring function of SME (bilinear) is:",
        "Here W ∈ ℝ d×d denotes the weight matrix, b indicates the bias vector. h , r , and t are the embedding vectors of head entity, relation, and tail entity, respectively.",
        "ConvKB  (Nguyen et al. 2017 ) utilizes a convolutional neural network (CNN) to conduct knowledge graph embeddings. ConvKB represents each triplet (h, r, t) as a three-row matrix A , which is input to a convolution layer to obtain feature maps. Afterward, the feature maps are concatenated as a vector, and then a score is calculated to estimate the confidence of the triplet. The scoring function is as follows:",
        "where O signifies the concatenation operator, g(⋅) is the ReLU activation function, A * Ω indicates the convolution operation of matrix A by using the filters in the set Ω , w ∈ ℝ 3d is a weight vector. R-GCN  (Schlichtkrull et al. 2018 ) is an improvement of graph neural networks (GNNs). R-GCN represents knowledge graphs by providing relation-specific transformation. Its forward propagation is calculated as follows:",
        "where h (l+1) k is the hidden state of the entity k in l-th layer, N r k denotes a neighbor collection of entity k and relation r ∈ R , n k,r is the normalization process, W (l)  i and W (l)  k are the weight matrices.",
        "The existing methods for generating knowledge graph embeddings still suffer several severe limitations. Many established methods only consider surface facts (triplets) of knowledge graphs. However, additional information, such as entity types and relation paths, are ignored, which can further improve the embedding accuracy. The performance of most traditional methods that do not consider the additional information is unsatisfactory. Table  3  lists the embedding methods, which do not consider the additional information. In Table  3 , the performance evaluation is based on the link prediction and triplet classification tasks. The metrics that are for evaluation results are hit rate at 10 (Hits@10) and accuracy. As Table  3  presents, only a few models have impressive results, including the results of QuatE (90%), RMNN (89.9%), and KBGAN (89.2%). Recently, some researchers have started to combine additional information with a knowledge graph to improve the efficiency of embedding models. For example,  Guo et al. (2015)  take advantage of additional entity type information, which is the semantic category of each entity, to obtain the correlation between the entities and to tackle the data sparsity issue. Therefore, knowledge graphs",
        "are represented more accurately. Not only entity types, some other information, including relation paths  (Li et al. 2021) , time information of dynamic graphs  (Messner et al. 2022) , and textual descriptions of entities  (An et al. 2018) , are getting the researchers' attention in recent years. However, it is still a daunting challenge to effectively utilize rich additional information to improve the accuracy of knowledge graph embeddings. General additional information can not adequately represent the semantic meaning of the triplets. For instance, the entity types are not related to the semantic information of triplets. Furthermore, the types of additional information that can be incorporated into the features of the triplets are now severely limited. Therefore, to improve the performance of existing knowledge graph embedding methods, multivariate information (such as the hierarchical descriptions of relations and the combination of entity types and textual descriptions) needs to be incorporated into the features of the triplets.",
        "To the best of our knowledge, complex relation path remains an open research problem  (Peng et al. 2021) . For example, the inherent relations, referring to the indirect relationships between two unconnected entities, are not represented effectively. Although the inherent relations between the entities can be explored based on the chain of relationships in knowledge graphs, the inherent relations are complex and multiple. Therefore, it is not straightforward to represent these relations effectively."
      ],
      "Conclusion": [
        "Knowledge graphs have played an instrumental role in creating many intelligent services and applications for various fields. In this survey, we provided an overview of knowledge graphs in terms of opportunities and challenges. We first introduced the definitions and existing research directions regarding knowledge graphs to provide an introductory analysis of knowledge graphs. Afterward, we discussed AI systems that take advantage of knowledge graphs. Then, we presented some representative knowledge graph applications in several fields. Furthermore, we analyzed the limitations of current knowledge graph technologies, which lead to severe technical challenges. We expect this survey to spark new ideas and insightful perspectives for future research and development activities involving knowledge graphs."
      ]
    }
  },
  {
    "paperId": "fb00016c1e048b9373803add001c1ec7e877cb23",
    "title": "Head-to-Tail: How Knowledgeable are Large Language Models (LLMs)? A.K.A. Will LLMs Replace Knowledge Graphs?",
    "sections": {
      "Introduction": [
        "Pre-trained large language models (LLMs), such as ChatGPT 1 ,  GPT-4 (OpenAI, 2023) , and Llama 2  (Touvron et al., 2023b) , have demonstrated impressive capabilities in internalizing knowledge and responding to common inquiries  (Ouyang et al., 2022; OpenAI, 2023) . Nevertheless, these models often lack knowledge of nuanced, domain-specific details and are susceptible to hallucinations  (Bang et al., 2023) , underscoring the significant challenges of increasing the factuality of LLMs and minimizing hallucinations from LLM responses. Conversely, the rise of LLMs has sparked debates on whether Knowledge Graphs (KGs), which store real-world factual knowledge in triplet form (subject, predicate, object), will be replaced with LLMs. This paper tries to answer these questions from a new angle: How knowledgeable are LLMs?",
        "Figure  1 : The question-answering accuracy of GPT-4 decreases in the order of head, torso, and tail entities on the Head-to-Tail benchmark, and is only 31% on average.",
        "Finding answers to these questions is not easy. First, it is hard to directly \"query\" the knowledge embedded in an LLM-hallucination can be due to lack of knowledge but can also be caused by dysfunction of the generative model even if the knowledge is already parameterized in the model. We approximate the amount of knowledge in an LLM by its accuracy in answering simple-formed questions, such as \"where was the basketball player Michael Jordan born?\"; in addition, we ask the LLM to generate brief answers and admit \"unsure\" when its confidence is low. We chose this proxy because we found LLMs are normally very good at understand-ing simple-formed questions and produce consistent answers when regenerating answers, especially if asked to be brief (Section 3.5).",
        "Second, there is no ready-to-use benchmark that either well represents distributions of user's interest (the query logs for major LLMs or search engines are not publicly available) or well represents the uniform distribution of the world knowledge (even the largest knowledge graphs admit sparsity of knowledge, especially towards non-popular facts). To address this challenge, we construct a benchmark of 18K QA pairs that cover various domains and various relationships in these domains. We bucket entities and relationships to head, torso, and tail according to how popular they are (details in Section 2) and randomly sample from each bucket; as such, we call our benchmark Head-to-Tail. This benchmark facilitates us to achieve a comprehensive view of how knowledgeable LLMs are regarding each bucket.",
        "Through the Head-to-Tail benchmark and the experimental methodology, we answer the following three research questions (RQs):",
        "RQ1: How reliable are LLMs in answering factual questions? (Section 3.2) RQ2: Do LLMs perform equally well on head, torso, and tail facts? (Section 3.3) RQ3: Do normal methods that improve LLMs, such as model size increase and instruction tuning, help LLMs to be more knowledgeable? (Section 3.4)",
        "As shown in Figure  1 , our analysis demonstrates a consistent decline in the performance of LLMs, following the order of head, torso, and tail entities, confirming our hypothesis that LLMs contain more head knowledge where training data abound. Surprisingly, even for the top-0.5% popular entities in popular domains such as Movie, the evaluated LLMs, at best, provide accurate answers for only ∼60% of the questions in the benchmark. Normal methods that enhance LLMs do not necessarily make them more knowledgeable, highlighting the need for more effective approaches to increase LLMs' factuality.",
        "Our main contributions are as follows:",
        "(i) We introduce Head-to-Tail, the first benchmark focused on comprehensively assessing the effectiveness of LLMs in incorporating factual knowledge encompassing the head, torso, and tail portions of knowledge graphs (Section 2.1). Head-to-Tail will be available at https://github.com/ facebookresearch/head-to-tail.",
        "(ii) We present an evaluation methodology accompanied by metrics designed to assess the factuality of LLMs. Our metrics allow us to distinguish hallucination and missing answers, and our evaluation method, whereas entirely automated, proves to be reliable and robust (Section 2.2-2.3).",
        "(iii) We conducted a comprehensive evaluation and quantified the factuality of 16 LLMs regarding head, torso, and tail facts to answer the research questions (RQ1-RQ3) (Section 3). In light of these findings, we envision the future of knowledge graphs and outline a research landscape aimed at improving the overall factual reliability of LLMs (Section 4).",
        "2 The Head-to-Tail Benchmark",
        "We now describe the Head-to-Tail benchmark, the metrics, and our evaluation methodology."
      ],
      "Methods": [
        "We prompted the LLM as shown in Prompt 3 in Appendix A.1. First, we asked LLMs to give as concise answers as possible. Second, we prompted LLMs to respond \"unsure\" when the LLM is not confident in the answer. We applied few-shot learning and included in the prompt two examples that are not in Head-to-Tail: one is a simple, answerable question with the corresponding answer as the response; the other is an unanswerable question with \"unsure\" as the response.",
        "With this prompt, rule-based metrics are more likely to reflect the factual correctness of the answers, and we can simply compute the missing rate (i.e., M) by counting \"unsure\" or empty answers. We observed that explicitly asking for \"unsure\" as an answer could significantly reduce hallucination rate (Section 3.5).",
        "To summarize, the following three setups in the benchmark and evaluation methodology help us best approximate the existence of (confident) knowledge in the LLMs: (i) focusing on simple questions in easy-to-understand forms, (ii) asking for concise answers to ease evaluation, and (iii) hinting the LLMs to answer \"unsure\" to suppress unnecessary hallucinations.",
        "LLMs increase the factuality? Second, compared with LLaMA and Falcon, the instruction-tuned counterparts (i.e., Vicuna and Falcon-Instruct) have lower accuracy, as they learned to be more conservative in providing factual answers and thus generate \"unsure\" more often (e.g., Vicuna-13B is 26.9% higher in M than LLaMA-13B). Despite so, they still have high hallucination rate.",
        "Finally, we evaluate the robustness of our evaluation methodology.",
        "Correlations between rule-and LLM-based metrics. For each combination of popularity (head, torso, tail) and domain (movie, book, academics, open), we calculate Spearman's rank and Pearson correlation coefficients between rule-and LLMbased metrics over all LLMs. We report the aggregated results (minimum, mean) in Effect of brief and \"unsure\". We randomly sampled 1.2K questions and tested the stability of answers if we call ChatGPT to regenerate answers. When not requiring brief or \"unsure\" answers, for 18% of questions, ChatGPT regenerated different answers. Adding the requirement for brief answers (Prompt 6 in Appendix A.1) reduced the percentage to 4%, and further asking \"unsure\" answers with few-shot examples (Prompt 3) reduced the percentage to 1%. In addition, according to manual evaluation on 150 randomly sampled questions, removing \"unsure\" as an option increases ChatGPT's hallucination rate by 13 percentage points."
      ],
      "Conclusion": [
        "Taxonomy. Our work does not discuss the effectiveness of LLMs in capturing taxonomy or type hierarchies, which could be an extension of this study. Specifically, we hypothesize that LLMs can effectively incorporate type relationships (e.g., hypernyms and synonyms), even for the fine-granularity sub-types. Hence, it may no longer be worth manually constructing a very deep and complex hierarchy in the future. Robustness to question formulation. This paper primarily aims to evaluate how much an LLM \"knows\" a fact with high confidence; we thus tested various ways of formulating factual questions and selected the least ambiguous form for this study. However, this approach does not assess the model's robustness to paraphrasing or consider the diverse ways models can be queried, such as entailment or cloze-style prompts. Our supplementary experiment in Appendix A.5 suggests that varying the form of questions does not significantly impact the evaluation results. A more thorough evaluation of robustness is beyond the scope of this paper and left for future research.",
        "We introduce Head-to-Tail, the first benchmark designed to assess the ability of LLMs to internalize head, torso, and tail facts. Alongside the dataset, we present a new evaluation methodology with appropriate metrics for automatically evaluating LLMs' factuality. Our evaluation shows that even the most advanced LLMs have notable limitations in representing factual knowledge, particularly for the torso and tail entities. Accordingly, we suggest new research areas to seamlessly blend knowledge in the symbolic form and neural form. instead of \"unsure\" when the confidence is low)."
      ],
      "Related Work": [
        "Benchmarks. Most works studied the factuality of LLMs using existing QA benchmarks such as We-bQuestions  (Berant et al., 2013) , TriviaQA  (Joshi et al., 2017) , LC-QuAD  (Trivedi et al., 2017; Dubey et al., 2019) , QALD-9  (Usbeck et al., 2018) , Natural Questions  (Kwiatkowski et al., 2019) , and Enti-tyQuestions  (Sciavolino et al., 2021) . A recent line of work has been constructing new QA benchmarks to assess LLMs' factuality, especially for long-tail knowledge  (Mallen et al., 2023; Kim et al., 2023) . Compared with these benchmarks, Head-to-Tail is the first to specifically assess how well LLMs incorporate head, torso, and tail factual information. LLM Evaluation. Recent years have seen a proliferation of research on assessing the factuality of LLMs  (Roberts et al., 2020; Petroni et al., 2021; Shuster et al., 2021; Mielke et al., 2022; Tan et al., 2023; Hu et al., 2023; Peng et al., 2023a; Omar et al., 2023; Kandpal et al., 2023; Mallen et al., 2023; Chen et al., 2023) . Most of these works focus on a single knowledge source, such as Freebase or Wikipedia, and they have yet to systematically perform the evaluation explicitly regarding head/torso/tail entities or attributes. One work close to ours is  Omar et al. (2023) , which evaluated ChatGPT using facts collected from diverse knowledge sources; however, their evaluation was carried out manually on only 450 QA instances.",
        "There are three works that also showed the correlation between the QA accuracy of language models and fact popularity  (Mallen et al., 2023; Kandpal et al., 2023; Kim et al., 2023) . Our work, conducted in parallel, focuses on a different anglehow knowledgeable are LLMs? For this purpose, we systematically designed experimental methodology, including the definition of head, torso, and tail entities, the design of metrics, and the evaluation method. Our benchmark is comprehensive in containing different knowledge sources, different domains, and rich relations. Compared with these three works, we gave more quantified answers for research questions RQ1-RQ3."
      ]
    }
  },
  {
    "paperId": "02033e83ff310f35e4623bd339982c52d926f2d5",
    "title": "Give us the Facts: Enhancing Large Language Models With Knowledge Graphs for Fact-Aware Language Modeling",
    "sections": {
      "Introduction": [
        "I N recent years, the rapid development of big data [1]- [3]   and high-speed computing has led to the emergence of pretrained language models (PLMs). Plenty of PLMs, such as BERT  [4] , GPT  [5] , and T5  [6] , have been proposed, which greatly improve the performance of various natural language processing (NLP) tasks. Recently, researchers have found that scaling model size or data size can improve model capacities on downstream tasks. Moreover, they found that when the parameter size exceeds a certain scale  [7] , these PLMs exhibit some surprising emergent abilities. Emergent abilities refer to the abilities that are not present in small models but arise in large models  [7] , which are utilized to distinguish large language models (LLMs) from PLMs.",
        "On November 30, 2022, a chatbot program named ChatGPT was released by OpenAI, which is developed based on the LLM GPT-3.5. By fine-tuning GPT with supervised learning and further optimizing the model using reinforcement learning from human feedback (RLHF), ChatGPT is capable of engaging in continuous conversation with humans based on chat context. It can even complete complex tasks such as coding and paper writing, showcasing its powerful emergent abilities  [7] . Consequently, some researchers  [8] -  [11]  explored whether LLMs can serve as parameterized knowledge bases to replace structured knowledge bases like knowledge graphs (KGs), as they also store a substantial amount of facts.",
        "However, existing studies  [12] -  [15]  have found that LLMs' ability to generate factually correct text is still limited. They are capable of remembering facts only during training. Consequently, these models often face challenges when attempting to recall relevant knowledge and apply the correct knowledge to generate knowledge grounded contents. On the other hand, as artificially constructed structured knowledge bases, KGs store a vast amount of knowledge closely related to real-world facts in a readable format. They explicitly express relationships between entities and intuitively display the overall structure of knowledge and reasoning chains, making them an ideal choice for knowledge modeling. As a result, there exists not only a competitive but also a complementary relationship between LLMs and KGs. LLMs have the ability to enhance knowledge extraction accuracy and improve the quality of KGs  [16] , while KGs can utilize explicit knowledge to guide the training of LLMs, improving their ability to recall and apply knowledge.",
        "So far, numerous methods have been proposed for strengthening PLMs with KGs, which can be categorized into three types: before-training enhancement, during-training enhancement, and post-training enhancement. Although there exist a few surveys  [17] -  [19]  of knowledge-enhanced PLMs, they focus on various forms of knowledge, lacking a systematic review of knowledge graph enhanced pre-trained language model (KGPLM) methods. For instance, Wei et al.  [17]  conducted a review of knowledge enhanced PLMs based on diverse knowledge sources but only covered a small set of KGPLMs. Similarly, Yang et al.  [18]  covered various forms of knowledge enhanced PLMs but provided only a partial review of KGPLMs without technical categorization. In another study, Zhen et al.  [19]  categorized knowledge enhanced PLMs into implicit incorporation and explicit incorporation methods, yet their review encompassed only a small subset of KGPLMs. Moreover, this field is rapidly evolving with numerous new technologies consistently being introduced. Therefore, to address questions of whether constructing KGs is still necessary and how to improve the knowledge modeling ability of LLMs, we present a systematic review of relevant studies. We conducted a thorough search for papers related to the keywords \"language model\" and \"knowledge graph\". Subsequently, the papers that were most relevant to KGPLM were carefully refined and categorized. In comparison with existing surveys, this paper specifically concentrates on KGPLM and covers a broader range of up-to-date papers. Furthermore, we suggest the development of knowledge graph enhanced large language models (KGLLMs) to tackle the knowledge modeling challenge in LLMs. The main contributions of this paper are summarized as follows:",
        "• We provide a comprehensive review for KGPLMs, which helps researchers to gain a deep insight of this field.",
        "• We overview research on the evaluation of LLMs and draw comparisons between LLMs and KGs. • We propose to enhance LLMs with KGs and suggest some possible future research directions, which may benefit researchers in the field of LLM. The remainder of this paper is organized as follows. Section II overviews the background of LLMs. Section III categorizes the existing methods for KGPLMs and introduces representatives from each group. Section IV introduces the applications of KGPLMs. Section V discusses whether LLMs can replace KGs with the evidence from existing studies. Section VI proposes to enhance LLMs' ability to learn factual knowledge by developing KGLLMs and presents some future research directions. Section VII draws the conclusions. II. BACKGROUND PLMs learn dense and continuous representations for words, addressing the issue of feature sparsity encountered in traditional encoding methods and significantly improving performance across various NLP tasks. Consequently, PLM-based methods have gained prominence, leading to the development of various types of PLMs. Recently, PLMs have been scaled to LLMs in order to achieve even better performance. In this section, we provide a comprehensive background of PLMs and offer an overview of their historical development.",
        "PLMs are a type of language model obtained through unsupervised learning  [20]  on a large corpus. They are capable of capturing the structure and characteristics of a language and generating universal representations for words. Following pretraining, PLMs can be fine-tuned for specific downstream tasks like text summarization, text classification, and text generation.",
        "The model frameworks used by existing PLMs can be classified into three categories, as illustrated in Fig.  1 : encoderonly, decoder-only, and encoder-decoder  [21] . The encoderonly framework utilizes a bidirectional transformer to recover masked tokens based on the input sentences, which effectively utilizes contextual information to learn better text representations. More specifically, given an input token sequence ̸ C = (x 1 , ..., x T ) with a few masked tokens M, it models the likelihood of the masked tokens as p(x) = xt∈M p(x t |x ̸ C ). However, due to the lack of a decoder, it cannot be directly applied to text generation tasks. BERT and its improved models mostly adopt the encoder-only framework. The decoder-only framework leverages a unidirectional transformer to predict tokens in an autoregressive fashion, making it suitable for text generation tasks. That is, given the text sequence C = (x 1 , ..., x T ), this framework models the likelihood of the input token sequence as p(x) = T t=1 p(x t |x <t ). GPT series and their improved models mostly adopt this framework. Nevertheless, compared with the other two frameworks, the decoderonly framework cannot make use of contextual information and cannot generalize well to other tasks. The encoder-decoder framework constructs a sequence-to-sequence model to predict the current token based on historical context with masked tokens. Its objective can be described as",
        "This framework excels at tasks that require generating output based on given inputs, yet its encoding and decoding speed is slow compared to the other two frameworks.",
        "Multiple pre-training tasks for PLMs have been designed, which can be categorized into word-level, phrase-level, and sentence-level tasks. Typical word-level pre-training tasks include masked language modeling (MLM)  [4]  and replaced token detection (RTD)  [22] . MLM randomly masks some tokens in the input sequence and trains PLMs to reconstruct the masked tokens based on context, whose loss function is:",
        "It can promote the learning of contextual information, thereby achieving better results in language understanding and language modeling tasks. RTD operates similarly to MLM but introduces greater randomness by substituting some tokens with alternative ones and training the model to predict the original tokens, whose loss function is defined as:",
        "Here, x is the corrupted token of x, while y t is 1 if xt = x t and 0 otherwise. Compared with MLM, RTD can reflect changes in vocabulary in real texts more realistically and enable PLMs to handle unknown and misspelled words. The representative of phrase-level pre-training tasks is span boundary objective (SBO)  [23] ,  [24] , which forces PLMs to predict each token of a masked span solely relying on the representations of the visible tokens at the boundaries, enhancing the syntactic structure analysis ability of PLMs and improving their performance in named entity recognition and sentiment analysis. The training objective of the SBO task can be expressed as:",
        "where y i is token x i 's representation in the span. Representatives of sentence-level pre-training tasks include next sentence prediction (NSP)  [4]  and sentence order prediction (SOP)  [25] . NSP trains PLMs to distinguish whether two given sentences are continuous, thereby improving PLMs' performance in context-based tasks such as natural language inference and text classification. Similarly, SOP trains PLMs to determine the order of two randomly sampled and disrupted sentences, which improves their ability to capture sentence order information.",
        "The training objective of NSP and SOP is as follows:",
        "where y = 1 if s 1 and s 2 are two consecutive segments extracted from the corpus. Other tasks like deleted token detection (DTD), text infilling, sentence reordering (SR), and document reordering (DR) are also utilized by some PLMs  [26] , which improve their performance in some special tasks."
      ],
      "Discussion": [
        "In addition to knowledge graph enhancement methods, there are also other enhancement methods that can be used to improve LLMs' factual language modeling ability. Typically, these methods include data augmentation and retrieval augmentation. Data augmentation involves refining the training data during pretraining and emphasizing informative words, emphasizing the importance of the training corpus in equipping the model with factual knowledge. Compared with knowledge graph enhancement methods, these approaches utilize implicit knowledge to model factual knowledge in text and ignore the relationships between entities. Retrieval augmentation has emerged as a widely adopted approach, allowing LLMs to retrieve external data from databases  [149]  or tools and pass it to LLMs in the form of prompts or embeddings to improve LLMs' generations. These methods can address some challenges faced by plain LLMs, such as outdated information and the inability to memorize. However, they cannot fundamentally improve LLMs' knowledge modeling ability since they do not change LLMs' parameters.",
        "Besides, some plugins have been developed to enhance the capabilities of LLMs in the context of a knowledge base. For example, the Browsing plugin can call search engines to access real-time information on the website; the Retrieval plugin 7  uses OpenAI embeddings to index and search documents in vector databases; the Wolfram 8 plugin enables ChatGPT to provide more comprehensive and accurate answers by giving it access to the Wolfram Alpha knowledge base; the Expedia plugin 9 enables ChatGPT to provide personalized travel recommendations with the help of Expedia's entity graph.",
        "Although KGLLMs have achieved some success, there are still many unresolved challenges. Here, we outline and discuss a few promising research directions for KGLLMs.",
        "Improving the efficiency of KGLLMs. Due to the need for preprocessing and encoding knowledge from KGs, developing KGLLMs typically requires more computational resources and time compared to plain LLMs. However, the scaling law of KGLLMs may differ from that of plain LLMs. Previous studies on KGPLMs have demonstrated that smaller KGPLMs can even outperform larger PLMs. Therefore, a comprehensive investigation of the scaling law of KGLLMs is necessary to determine the optimal parameter size for their development.",
        "Based on this, we can potentially achieve a smaller model that satisfies performance requirements, resulting in reduced computational resources and time.",
        "Merging different knowledge in different ways. Some common and well-defined knowledge could be stored within KGs for ease of access, while rarely used or implicit knowledge that cannot be expressed through triples should be incorporated into the parameters of LLMs. In particular, domainspecific knowledge, although infrequently accessed, may still require a significant amount of human effort to construct an associated KG due to the sparse nature of its related corpus.",
        "Incorporating more types of knowledge. As introduced in Section III, the majority of existing KGPLMs only utilize a single modality and static KGs. However, there exist multimodal and temporal KGs that contain multimodal and temporal knowledge. These types of knowledge can complement textual and structural knowledge, enabling LLMs to learn the relationships between entities over time. Moreover, multimodal pre-trained models have gained popularity as they have been proven to improve the performance of pre-trained models on multimodal tasks  [150]  and enhance their cognitive ability. Therefore, incorporating multimodal and temporal KGs into LLMs has the potential to improve their performance, which is worth investigating. To achieve this goal, we need to align multimodal entities, design encoders capable of processing and fusing multimodal temporal data, and establish multimodal temporal learning tasks to extract useful information.",
        "Improving the effectiveness of knowledge incorporation. By modifying inputs, model architecture, and the fine-tuning process, diverse methods have been proposed to incorporate relational triplets into PLMs. However, each method has its own set of advantages and disadvantages, with some performing well on particular tasks but underperforming on others. For example, LUKE  [76]  exhibits superior performance over 7 https://github.com/openai/chatgpt-retrieval-plugin 8 https://www.wolfram.com/wolfram-plugin-chatgpt/ 9 https://chatonai.org/expedia-chatgpt-plugin KEPLER  [85]  in most entity typing and relation classification tasks but performs worse in a few other tasks  [89] . Besides, recent experimental analysis  [151]  reveals that existing KG-PLMs integrate only a small fraction of factual knowledge. Therefore, there is still a lot of room for research on effective knowledge integration methods. Further research is required on the selection of valuable knowledge and avoiding catastrophic forgetting when faced with vast and clashing knowledge.",
        "Enhancing the interpretability of KGLLMs. Although it is widely believed that KGs can enhance the interpretability of LLMs, corresponding methods have not yet been thoroughly studied. Schuff et al.  [152]  investigated whether integrating external knowledge can improve natural language inference models' explainability by evaluating the scores of generated explanations on in-domain data and special transfer datasets. However, they found that the most commonly used metrics do not consistently align with human evaluations concerning the accuracy of explanations, incorporation of common knowledge, and grammatical and labeling correctness. To provide human-understandable explanations for LLMs, Chen et al.  [153]  proposed a knowledge-enhanced interpretation module that utilizes a KG and a GNN to extract key decision signals of LLMs. Despite a few studies attempting to improve the interpretability of PLMs, it remains unclear how to leverage KGs to improve the interpretability of KGPLMs. A feasible approach may involve searching for the relevant reasoning path in KGs based on the generated content and then generating an explanatory text based on the reasoning path.",
        "Exploring domain-specific KGLLMs. Though there is already considerable research incorporating standard KGs with general PLMs, limited work has focused on domain-specific KGLLMs. However, the rise of artificial intelligence for science will lead to an increasing demand for domain-specific KGLLMs. In comparison to general LLMs, domain-specific LLMs require greater precision and specificity in incorporating domain knowledge. As a result, constructing accurate domainspecific KGs and integrating them with LLMs warrant further exploration. In order to develop domain-specific KGLLMs, it is essential to first construct a domain KG and gather relevant corpus data with the help of domain experts. Considering the generality of language patterns, it is advisable to blend common KGs with the domain-specific KG for enhancement."
      ],
      "Conclusion": [
        "The phenomenal success of ChatGPT has spurred the rapid advancement of LLMs. Given the impressive performance of LLMs on a variety of NLP tasks, some researchers wonder if they can be viewed as a type of parameterized knowledge base and replace KGs. However, LLMs still fall short in recalling and correctly using factual knowledge while generating knowledge-grounded text. In order to clarify the value of KGs in the era of LLMs, a comprehensive survey on KGPLMs was conducted in this paper. We began by examining the background of PLMs and the motivation for incorporating KGs into PLMs. Next, we categorized existing KGPLMs into three categories and provided details about each category. We also reviewed the applications of KGPLMs. After that, we analyzed whether PLMs and recent LLMs can replace KGs based on existing studies. In the end, we proposed enhancing LLMs with KGs to conduct fact-aware language modeling for improving their learning of factual knowledge. This paper addresses three questions: (1) What is the value of KGs in the era of LLMs? (2) How to incorporate KGs into LLMs to improve their performance? (3) What do we need to do for the future development of KGLLM? We hope this work will stimulate additional research advancements in LLM and KG."
      ]
    }
  },
  {
    "paperId": "d25f8c388677d287d00ca67d44ef02da2b45f2d9",
    "title": "Large Language Models and Knowledge Graphs: Opportunities and Challenges",
    "sections": {
      "Methods": [
        "There are a few of reasons that retrieval augmented methods are necessary for LLMs to obtain external knowledge. One reason is to address the problem of knowledge cutoff, i.e., LLMs are not aware of the events that happened after their training. Also, although parametric knowledge would increase when the size of parameters increases, training LLMs is expensive; e.g., GPT-3 (175B parameters) costs $4.6 million to train, and PaLM (540B parameters) costs $17 million. In fact, research suggests that the obtained knowledge from such training is mainly about popular entities  [107] . Furthermore, for domain specific applications, there might be some significant knowledge that is not yet in LLMs, including private and business critical knowledge that cannot be put into LLMs.",
        "One idea to deal with the above lack of (updated) knowledge is to edit the knowledge in LLMs. A obvious strategy is to retrain and fine-tune the model based on the modified data. However, apart from being costly, retraining cannot guarantee that erroneous data will be corrected. Another Strategy is to develop hyper-network to learn a parameter shift for the base model. De  Cao et al. [37]  trained a hyper-network, KnowledgeEditor, to modify a fact and used Kullback-Leibler (KL) divergence-constrained optimization to alleviate the side effect on other data/knowledge that should not be changed. However, this method does not perform well when editing multiple edits, as it uses the same strategy to process multiple edits and ignore the relation between different edit gradients, resulting in a \"zero-sum\" phenomenon, where the inter-gradient conflict will inevitably cause some data modifications to fail.  Han et al. [57]  design explicit and implicit multi-editor models to learn diverse editing strategies in terms of dynamic structure and dynamic parameters respectively, allowing to deal with the conflict data in an efficient end-to-end manner.",
        "However, the above Knowledge Editing methods are not yet scalable, people started to introduce retrieve-generate architectures for building retrieval augmented generation models. These methods are mainly using unstructured passages as external knowledge. RAG [92] outperforms DPR [82] by marginalizing the retrieval step to train the generator and retriever jointly with the supervision of the label answer. FiD [71] encodes the concatenation of the passages retrieved by pre-trained DPR and the original question separately, and then fuses them with concatenation to the decoder. It is expected that structured knowledge will be the main source of external knowledge, as passages often contain noise. Knowledge Graphs can be used directly as external knowledge. They can also be used to enhance passage-based methods  [189] .",
        "Retrieval augmentation is a very promising direction. There are a few pressing challenges:",
        "Unifying Knowledge Editing and Retrieval Augmentation: KGs can be used for editing knowledge in LLMs, while at the same time, KGs can also be used as external knowledge to assist LLMs in retrieval augmented methods. In fact, knowledge editing and retrieval augmentation is getting very close. For example, Mitchel et al.  [111]  proposed a Retrieval-Augmented Counterfactual Model (SERAC), which stores edits in an explicit memory for knowledge editing over LLMs.",
        "Semi-parametric LLMs: This direction is highly related to the topic of this position paper.",
        "The idea is to make use of explicit knowledge to augment LLMs. One of the key issue is to integrate different explicit knowledge [158], including unstructured ones, such as passages, and structured ones, such as KGs and databases, for augmenting LLMs. Support of Complex Reasoning: Can we go beyond simply retrieving explicit knowledge by enabling reasoning through retrieval augmented methods? BehnamGhader et al.  [11]  demonstrated with their experimental results that the similarity metric used by the retrievers is generally insufficient for reasoning tasks. Furthermore, LLMs do not take the complicated relations between statements into account, thus leading to to poor reasoning performance."
      ]
    }
  }
]